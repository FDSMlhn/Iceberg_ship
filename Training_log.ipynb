{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st December:\n",
    "\n",
    "trying resnet18:\n",
    "1. setting batchsize 32. best parameter set (lr, wd): (0.1,0.0001), **(0.1,0.001)**, (0.01,0.0481). First two are more stable\n",
    "2. for (0.1,0.001), lr decay (50, 70, 80), approach acc 90.8% around 74 epoch. 0.245 loss val. 0.195 loss LB.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2nd December:\n",
    "1. try (0.1,0.0001) for much longer train, perform not good\n",
    "2. decrease the size of last avg_pooling kernel, perform not good. global pooling can lead to better regularize\n",
    "3. try dropout: (0.5 ), (0.3, ). After adding dropout, seems have a hard time to converge even.\n",
    "4. try (0.1,0.001), lr decay (55, 75) for 80 epoch on all training data. LB 0.205. Althoug LB score is not accurate, the progress is not significant\n",
    "5. try (0.1,0.001), lr decay (55, 75) for 80 epoch on 90% training data, see how the optimal point moves when we have more training data: In this case the optimal point is not stable. **Risk of overfitting rises. So at least from now on, we'd better stick to 5 fold split**\n",
    "6. Evidence back up the exp above: I train (0.1,0.0001) for (50,100,140,170) for 200 epoch with 10 fold split. The best score achieves at 50 epochs. Score is attractive like this, but the LB work out poorly.(0.2581 and 0.27** respectively). I can see that the score bounces around staggeringly around 50 epochs. So I has to be careful over this.     \n",
    "[=================== 46/46 ==================>]  Step: 51ms | Tot: 10s437ms | Loss: 0.224 | Acc: 90.859% (1312/1444))\n",
    "[=================== 3/3 ======>..............]  Step: 66ms | Tot: 176ms | Loss: 0.227 | Acc: 91.250% (146/160))   \n",
    "\n",
    "trying resnet34:\n",
    "1. try (0.1,0.0001) (50,80, 110,170), 'resnet34.pth' is the one stop at epoch52, then I choose two for test, epoch(83, 99), both get LB 0.195. However, I can tell this is much more stable, because you get the same score as val. Worth mentioning, **this network with 0.001 wd seems have a hard time to overfit the model**:  \n",
    "[=================== 41/41 =================>.]  Step: 504ms | Tot: 17s440ms | Loss: 0.195 | Acc: 91.846% (1194/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 830ms | Loss: 0.198 | Acc: 93.092% (283/304)  \n",
    "[=================== 41/41 =================>.]  Step: 502ms | Tot: 17s428ms | Loss: 0.175 | Acc: 93.231% (1212/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 832ms | Loss: 0.203 | Acc: 93.092% (283/304)\n",
    "2. try (0.1,0.0001) (100, 150,200). I change the lr decay become this need more time to fit together. And obviously, this network has less problem to overfit the model. So I choose one at 118 epoch to test my test data. And the LB score is 0.1836. **The score is better because model has strong representational power which are powerful enough to overfit the model.** It indicates this is more appropriate to fit it. The correlation of this to resnet34 above is about 0.88    \n",
    "[=================== 41/41 =================>.]  Step: 510ms | Tot: 17s538ms | Loss: 0.124 | Acc: 95.231% (1238/1300)\n",
    "[=================== 5/5 ============>........]  Step: 188ms | Tot: 838ms | Loss: 0.193 | Acc: 93.421% (284/304)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3rd December:\n",
    "\n",
    "trying resnet 50:\n",
    "1. try (0.1, 0.0001), (0.1, 0.00005), (0.1,0.00001), the net have a very hard time to overfit the data, even if it overfit to 95% acc, the val still around 91 and 90. So I try (0.15,0.0001) again trying to fit it faster. \n",
    "\n",
    "#### 4th December:\n",
    "trying densenet 169:\n",
    "1. Well this network has no troblem to overfit. But it overfit fast and score for val do not good. I will try to use densenetBC later\n",
    "\n",
    "trying resnet34\n",
    "1. These time I add more elements like shearing, rotation(from 0 to 360), tranlation and scaling, but it can't converge at all with the training setting before, with which I achiveve very good result. so now I only reserve,  rotation(from 0 to 360) and scaling. However, this time with (0.1,0.0001), it overfits faster than before? I don't know why is that for now.\n",
    "\n",
    "trying densenetBC 100:\n",
    "1. don't figure out where went wrong. The val score indicates it just doesn't work.\n",
    "\n",
    "\n",
    "LOG: Revise the data augmentation in validation set, do not flip the val data which is not valid before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
