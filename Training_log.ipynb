{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st December:\n",
    "\n",
    "trying resnet18:\n",
    "1. setting batchsize 32. best parameter set (lr, wd): (0.1,0.0001), **(0.1,0.001)**, (0.01,0.0481). First two are more stable\n",
    "2. for (0.1,0.001), lr decay (50, 70, 80), approach acc 90.8% around 74 epoch. 0.245 loss val. 0.195 loss LB.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2nd December:\n",
    "1. try (0.1,0.0001) for much longer train, perform not good\n",
    "2. decrease the size of last avg_pooling kernel, perform not good. global pooling can lead to better regularize\n",
    "3. try dropout: (0.5 ), (0.3, ). After adding dropout, seems have a hard time to converge even.\n",
    "4. try (0.1,0.001), lr decay (55, 75) for 80 epoch on all training data. LB 0.205. Althoug LB score is not accurate, the progress is not significant\n",
    "5. try (0.1,0.001), lr decay (55, 75) for 80 epoch on 90% training data, see how the optimal point moves when we have more training data: In this case the optimal point is not stable. **Risk of overfitting rises. So at least from now on, we'd better stick to 5 fold split**\n",
    "6. Evidence back up the exp above: I train (0.1,0.0001) for (50,100,140,170) for 200 epoch with 10 fold split. The best score achieves at 50 epochs. Score is attractive like this, but the LB work out poorly.(0.2581 and 0.27** respectively). I can see that the score bounces around staggeringly around 50 epochs. So I has to be careful over this.     \n",
    "[=================== 46/46 ==================>]  Step: 51ms | Tot: 10s437ms | Loss: 0.224 | Acc: 90.859% (1312/1444))\n",
    "[=================== 3/3 ======>..............]  Step: 66ms | Tot: 176ms | Loss: 0.227 | Acc: 91.250% (146/160))   \n",
    "\n",
    "trying resnet34:\n",
    "1. try (0.1,0.0001) (50,80, 110,170), 'resnet34.pth' is the one stop at epoch52, then I choose two for test, epoch(83, 99), both get LB 0.195. However, I can tell this is much more stable, because you get the same score as val. Worth mentioning, **this network with 0.001 wd seems have a hard time to overfit the model**:  \n",
    "[=================== 41/41 =================>.]  Step: 504ms | Tot: 17s440ms | Loss: 0.195 | Acc: 91.846% (1194/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 830ms | Loss: 0.198 | Acc: 93.092% (283/304)  \n",
    "[=================== 41/41 =================>.]  Step: 502ms | Tot: 17s428ms | Loss: 0.175 | Acc: 93.231% (1212/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 832ms | Loss: 0.203 | Acc: 93.092% (283/304)\n",
    "2. try (0.1,0.0001) (100, 150,200). I change the lr decay become this need more time to fit together. And obviously, this network has less problem to overfit the model. So I choose one at 118 epoch to test my test data. And the LB score is 0.1836. **The score is better because model has strong representational power which are powerful enough to overfit the model.** It indicates this is more appropriate to fit it. The correlation of this to resnet34 above is about 0.88    \n",
    "[=================== 41/41 =================>.]  Step: 510ms | Tot: 17s538ms | Loss: 0.124 | Acc: 95.231% (1238/1300)\n",
    "[=================== 5/5 ============>........]  Step: 188ms | Tot: 838ms | Loss: 0.193 | Acc: 93.421% (284/304)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3rd December:\n",
    "\n",
    "trying resnet 50:\n",
    "1. try (0.1, 0.0001), (0.1, 0.00005), (0.1,0.00001), the net have a very hard time to overfit the data, even if it overfit to 95% acc, the val still around 91 and 90. So I try (0.15,0.0001) again trying to fit it faster. \n",
    "\n",
    "#### 4th December:\n",
    "trying densenet 169:\n",
    "1. Well this network has no troblem to overfit. But it overfit fast and score for val do not good. I will try to use densenetBC later\n",
    "\n",
    "trying resnet34\n",
    "1. These time I add more elements like shearing, rotation(from 0 to 360), tranlation and scaling, but it can't converge at all with the training setting before, with which I achiveve very good result. so now I only reserve,  rotation(from 0 to 360) and scaling. However, this time with (0.1,0.0001), it overfits faster than before? I don't know why is that for now.\n",
    "\n",
    "trying densenetBC 100:\n",
    "1. don't figure out where went wrong. The val score indicates it just doesn't work.\n",
    "\n",
    "\n",
    "LOG: Revise the data augmentation in validation set, do not flip the val data which is not valid before.\n",
    "\n",
    "#### 6th December:\n",
    "\n",
    "trying resnet101\n",
    "1. try (0.1, 0.0001), and **batch size=16**, this time network is actually have better generalization power only because I use smaller batch size. So it is worth trying to use even more small batch next time. (Or just the learning rate I need to change??) The LB score is around 0.2415.\n",
    "\n",
    "2. Test-time augmentation improve result, 40 time augmentation improve 0.2415 to 0.2311. \n",
    "\n",
    "trying resnet34:\n",
    "1. I try again for 4-feature image one, and no improvement or degradation. LB 0.1907. Worth mentioning, the 66 times augmentation don't improve my result again. I receive result 0.1921. 40 times also 0.1921\n",
    "\n",
    "\n",
    "Some advices on the ensemble:\n",
    "1. Running average of parameters during training. exponential weight decay cs231n\n",
    "2. Take more prediction using data augmentation and take average\n",
    "\n",
    "\n",
    "#### 7th December:\n",
    "\n",
    "trying resnet50\n",
    "1. small batchsize 16 and small learning rate 0.005 is not wroking.\n",
    "\n",
    "#### 9th December:\n",
    "\n",
    "trying resnet34:\n",
    "1. use different seed to sift out all model with val > 92, mean LB 0.2, minmax basemodel 0.22.\n",
    "\n",
    "#### 14th December:\n",
    "\n",
    "try resnet34:\n",
    "1. I try to rotate like 30, 60 degree, and the performance\n",
    "\n",
    "tips:\n",
    "1. There are 133 of na angle in the training data, almost 3/4 are in the last 300. I think it explain why the performance of the specific selection of dataset are better. lr wd (0.01, 0.0002) patience 10 early stopping 25. LB 0.186\n",
    "\n",
    "This is epoch:37\n",
    "[============ 41/41 ==========>]  Step: 65ms | Tot: 3s285ms | Loss: 0.122 | Acc: 95.077% (1236/1300)\n",
    "[============ 5/5 =======>.....]  Step: 33ms | Tot: 168ms | Loss: 0.180 | Acc: 94.079% (286/304)\n",
    "\n",
    "another training also with val 94.079%, 0.00021, but LB  0.201\n",
    "Can't train it with same setting!\n",
    "\n",
    "#### 15th December:\n",
    "\n",
    "trying resnet34\n",
    "1. Training another time, with acc as flag to change the lr. With this run, push the LB further to 0.1801. For the same set of data, it's amazing how the running outcome will vary during the same set of data. Thus in the future experiment, for the same fold, we should run multiple time to sift out the best one.\n",
    "This is epoch:28\n",
    "[============ 41/41 ==========>]  Step: 65ms | Tot: 3s280ms | Loss: 0.162 | Acc: 93.538% (1216/1300)\n",
    "[============ 5/5 =======>.....]  Step: 33ms | Tot: 170ms | Loss: 0.180 | Acc: 94.408% (287/304)\n",
    "\n",
    "2. train resnet34 multiple times to get a bunch of 0.17,0.18 val loss models. The best one with val loss 0.156, only achieve 0.191 on LB. Take the average of all model, get 0.179 on lB\n",
    "\n",
    "\n",
    "\n",
    "1. At round 0, fold 3, seed [4987], we find a good value with acc: 92.83489096573209, loss: 0.18720525540295418 **LB .1911**\n",
    "2. At round 0, fold 3, seed [4987], we find a good value with acc: 93.45794392523365, loss: 0.18422258188048626     \n",
    "3. At round 0, fold 3, seed [4987], we find a good value with acc: 93.76947040498442, loss: 0.18761429014235642     \n",
    "    \n",
    "4. At round 1, fold 3, seed [7386], we find a good value with acc: 93.14641744548287, loss: 0.18572754503410555 **LB 0.2208** only     \n",
    "This is epoch:6     \n",
    "[============ 41/41 ==========>]  Step: 28ms | Tot: 3s242ms | Loss: 0.165 | Acc: 93.453% (1199/1283)      \n",
    "[============ 6/6 ========>....]  Step: 13ms | Tot: 193ms | Loss: 0.123 | Acc: 95.950% (308/321)\n",
    "\n",
    "5. At round 1, fold 3, seed [7386], we find a good value with acc: 92.5233644859813, loss: 0.17600850301368215    \n",
    "6. At round 1, fold 3, seed [7386], we find a good value with acc: 93.76947040498442, loss: 0.17959555435774854    \n",
    "7. At round 1, fold 3, seed [7386], we find a good value with acc: 92.83489096573209, loss: 0.18345521060848533     \n",
    "\n",
    "8. At round 1, fold 4, seed [7386], we find a good value with acc: 93.125, loss: 0.19450552463531495  \n",
    "9. At round 1, fold 4, seed [7386], we find a good value with acc: 92.5, loss: 0.19440045058727265  \n",
    "\n",
    "10. At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.17605417614042573  \n",
    "11. At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.19229212802518567  \n",
    "12. At round 2, fold 1, seed [8868], we find a good value with acc: 93.45794392523365, loss: 0.17976307720410117  \n",
    "13. At round 2, fold 4, seed [8868], we find a good value with acc: 93.125, loss: 0.18329112231731415\n",
    "14. At round 2, fold 4, seed [8868], we find a good value with acc: 92.8125, loss: 0.18368732035160065  **LB0.1870** after tunning.    \n",
    "This is epoch:25   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s256ms | Loss: 0.172 | Acc: 92.991% (1194/1284)  \n",
    "[============ 5/5 =======>.....]  Step: 42ms | Tot: 177ms | Loss: 0.111 | Acc: 95.625% (306/320)  \n",
    "\n",
    "15. At round 2, fold 4, seed [8868], we find a good value with acc: 93.4375, loss: 0.19045546948909758  \n",
    "16. At round 3, fold 4, seed [9915], we find a good value with acc: 94.6875, loss: 0.17815355360507965  \n",
    "17. At round 3, fold 4, seed [9915], we find a good value with acc: 93.125, loss: 0.16909184455871581  \n",
    "18. At round 3, fold 4, seed [9915], we find a good value with acc: 92.5, loss: 0.1774096041917801   \n",
    "19. At round 3, fold 4, seed [9915], we find a good value with acc: 95.0, loss: 0.15670025497674941  \n",
    "This is epoch:8   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s229ms | Loss: 0.148 | Acc: 94.548% (1214/1284)  \n",
    "[============ 5/5 =======>.....]  Step: 44ms | Tot: 179ms | Loss: 0.129 | Acc: 95.625% (306/320)   \n",
    "\n",
    "20. At round 5, fold 4, seed [7433], we find a good value with acc: 93.4375, loss: 0.17777724266052247   \n",
    "This is epoch:12   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s252ms | Loss: 0.169 | Acc: 93.302% (1198/1284)   \n",
    "[============ 5/5 =======>.....]  Step: 43ms | Tot: 178ms | Loss: 0.160 | Acc: 93.438% (299/320)   \n",
    "\n",
    "What I have tried:\n",
    "1. All retrain with mean ensemble: LB0.1787\n",
    "2. All except my best 0.180 with mean ensemble: LB0.1788\n",
    "\n",
    "#### 17th December:\n",
    "\n",
    "0. the best one   0.136 train loss, 0.136 val loss  - LB0.2077\n",
    "1. All retrain and my best 0.180 with mean ensemble. 0.1791\n",
    "2. All models 0.05, 0.95, mean ensemble 0.1771\n",
    "3. All models,0.05, 0.95, best base model 0.1783\n",
    "4. All retrain models and my best 0.180(except the folds with one that achieves 0.22 LB), 0.05, 0.95, mean ensemble. LB 0.1762\n",
    "\n",
    "At round 1, fold 0, seed [7791], we find a good value with acc: 92.5233644859813, loss: 0.1843862771245178\n",
    "retrain two last:\n",
    "This is epoch:19\n",
    "[=================== 41/41 =================>.]  Step: 30ms | Tot: 3s254ms | Loss: 0.175 | Acc: 92.985% (1193/1283)\n",
    "[=================== 6/6 =============>.......]  Step: 12ms | Tot: 193ms | Loss: 0.155 | Acc: 94.081% (302/321)\n",
    "\n",
    "At round 1, fold 0, seed [7791], we find a good value with acc: 93.14641744548287, loss: 0.1838890655761196\n",
    "This is epoch:4\n",
    "[=================== 41/41 =================>.]  Step: 30ms | Tot: 3s236ms | Loss: 0.153 | Acc: 93.920% (1205/1283)\n",
    "[=================== 6/6 =============>.......]  Step: 14ms | Tot: 193ms | Loss: 0.158 | Acc: 94.393% (303/321)\n",
    "\n",
    "\n",
    "#### 18th December:\n",
    "\n",
    "\n",
    "lr 0.01, wd 0.002,dropout[0.2,0.2, 0.3, 0.3, 0.4,0.4]\n",
    "This is epoch:53  \n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.195 | Acc: 91.871% (5402/5880)  \n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 94.576% (279/295)  \n",
    "  \n",
    "This is epoch:59()  \n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.178 | Acc: 92.313% (5428/5880)  \n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 92.881% (274/295)  \n",
    "loss: Save it!  \n",
    "  \n",
    "0.002 -> 0.0015\n",
    "\n",
    "lr 0.01, wd 0.00175  **LB0.1717**   \n",
    "This is epoch:64\n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.166 | Acc: 93.333% (5488/5880)\n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 94.237% (278/295)\n",
    "\n",
    "1. All retrain models and my best 0.180, 0.05, 0.95, mean ensemble. **LB 0.1762 still!**\n",
    "2. All retrain models and my best 0.180, 0.1, 0.9, mean ensemble. **LB 0.1783**\n",
    "3. All retrain models and my best 0.180, 0.025, 0.975, mean ensemble. **LB 0.1767**\n",
    "4. All retrain models so far, 0.05, 0.95, mean ensemble. **LB 0.1758**. So actually more retrain models give me better results,\n",
    "\n",
    "lr 0.01, wd 0.002,dropout[0.3,0.3, 0.4, 0.4, 0.5,0.5]\n",
    "This is epoch:197\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 2s8ms|Loss: 0.175 | Acc: 93.027% (5470/5880)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.180 | Acc: 92.542% (273/295)\n",
    "loss: Save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At round 0, fold 3, seed [4987], we find a good value with acc: 92.83489096573209, loss: 0.18720525540295418\r\n",
      "At round 0, fold 3, seed [4987], we find a good value with acc: 93.45794392523365, loss: 0.18422258188048626\r\n",
      "At round 0, fold 3, seed [4987], we find a good value with acc: 93.76947040498442, loss: 0.18761429014235642\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 93.14641744548287, loss: 0.18572754503410555\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 92.5233644859813, loss: 0.17600850301368215\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 93.76947040498442, loss: 0.17959555435774854\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 92.83489096573209, loss: 0.18345521060848533\r\n",
      "At round 1, fold 4, seed [7386], we find a good value with acc: 93.125, loss: 0.19450552463531495\r\n",
      "At round 1, fold 4, seed [7386], we find a good value with acc: 92.5, loss: 0.19440045058727265\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.17605417614042573\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.19229212802518567\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 93.45794392523365, loss: 0.17976307720410117\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 93.125, loss: 0.18329112231731415\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 92.8125, loss: 0.18368732035160065\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 93.4375, loss: 0.19045546948909758\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 94.6875, loss: 0.17815355360507965\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 93.125, loss: 0.16909184455871581\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 92.5, loss: 0.1774096041917801\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 95.0, loss: 0.15670025497674941\r\n",
      "At round 5, fold 4, seed [7433], we find a good value with acc: 93.4375, loss: 0.17777724266052247\r\n",
      "At round 1, fold 0, seed [7791], we find a good value with acc: 92.5233644859813, loss: 0.1843862771245178\r\n",
      "At round 1, fold 0, seed [7791], we find a good value with acc: 93.14641744548287, loss: 0.1838890655761196\r\n"
     ]
    }
   ],
   "source": [
    "! cat log.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
