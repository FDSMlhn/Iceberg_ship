{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st December:\n",
    "\n",
    "trying resnet18:\n",
    "1. setting batchsize 32. best parameter set (lr, wd): (0.1,0.0001), **(0.1,0.001)**, (0.01,0.0481). First two are more stable\n",
    "2. for (0.1,0.001), lr decay (50, 70, 80), approach acc 90.8% around 74 epoch. 0.245 loss val. 0.195 loss LB.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2nd December:\n",
    "1. try (0.1,0.0001) for much longer train, perform not good\n",
    "2. decrease the size of last avg_pooling kernel, perform not good. global pooling can lead to better regularize\n",
    "3. try dropout: (0.5 ), (0.3, ). After adding dropout, seems have a hard time to converge even.\n",
    "4. try (0.1,0.001), lr decay (55, 75) for 80 epoch on all training data. LB 0.205. Althoug LB score is not accurate, the progress is not significant\n",
    "5. try (0.1,0.001), lr decay (55, 75) for 80 epoch on 90% training data, see how the optimal point moves when we have more training data: In this case the optimal point is not stable. **Risk of overfitting rises. So at least from now on, we'd better stick to 5 fold split**\n",
    "6. Evidence back up the exp above: I train (0.1,0.0001) for (50,100,140,170) for 200 epoch with 10 fold split. The best score achieves at 50 epochs. Score is attractive like this, but the LB work out poorly.(0.2581 and 0.27** respectively). I can see that the score bounces around staggeringly around 50 epochs. So I has to be careful over this.     \n",
    "[=================== 46/46 ==================>]  Step: 51ms | Tot: 10s437ms | Loss: 0.224 | Acc: 90.859% (1312/1444))\n",
    "[=================== 3/3 ======>..............]  Step: 66ms | Tot: 176ms | Loss: 0.227 | Acc: 91.250% (146/160))   \n",
    "\n",
    "trying resnet34:\n",
    "1. try (0.1,0.0001) (50,80, 110,170), 'resnet34.pth' is the one stop at epoch52, then I choose two for test, epoch(83, 99), both get LB 0.195. However, I can tell this is much more stable, because you get the same score as val. Worth mentioning, **this network with 0.001 wd seems have a hard time to overfit the model**:  \n",
    "[=================== 41/41 =================>.]  Step: 504ms | Tot: 17s440ms | Loss: 0.195 | Acc: 91.846% (1194/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 830ms | Loss: 0.198 | Acc: 93.092% (283/304)  \n",
    "[=================== 41/41 =================>.]  Step: 502ms | Tot: 17s428ms | Loss: 0.175 | Acc: 93.231% (1212/1300)\n",
    "[=================== 5/5 ============>........]  Step: 189ms | Tot: 832ms | Loss: 0.203 | Acc: 93.092% (283/304)\n",
    "2. try (0.1,0.0001) (100, 150,200). I change the lr decay become this need more time to fit together. And obviously, this network has less problem to overfit the model. So I choose one at 118 epoch to test my test data. And the LB score is 0.1836. **The score is better because model has strong representational power which are powerful enough to overfit the model.** It indicates this is more appropriate to fit it. The correlation of this to resnet34 above is about 0.88    \n",
    "[=================== 41/41 =================>.]  Step: 510ms | Tot: 17s538ms | Loss: 0.124 | Acc: 95.231% (1238/1300)\n",
    "[=================== 5/5 ============>........]  Step: 188ms | Tot: 838ms | Loss: 0.193 | Acc: 93.421% (284/304)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3rd December:\n",
    "\n",
    "trying resnet 50:\n",
    "1. try (0.1, 0.0001), (0.1, 0.00005), (0.1,0.00001), the net have a very hard time to overfit the data, even if it overfit to 95% acc, the val still around 91 and 90. So I try (0.15,0.0001) again trying to fit it faster. \n",
    "\n",
    "#### 4th December:\n",
    "trying densenet 169:\n",
    "1. Well this network has no troblem to overfit. But it overfit fast and score for val do not good. I will try to use densenetBC later\n",
    "\n",
    "trying resnet34\n",
    "1. These time I add more elements like shearing, rotation(from 0 to 360), tranlation and scaling, but it can't converge at all with the training setting before, with which I achiveve very good result. so now I only reserve,  rotation(from 0 to 360) and scaling. However, this time with (0.1,0.0001), it overfits faster than before? I don't know why is that for now.\n",
    "\n",
    "trying densenetBC 100:\n",
    "1. don't figure out where went wrong. The val score indicates it just doesn't work.\n",
    "\n",
    "\n",
    "LOG: Revise the data augmentation in validation set, do not flip the val data which is not valid before.\n",
    "\n",
    "#### 6th December:\n",
    "\n",
    "trying resnet101\n",
    "1. try (0.1, 0.0001), and **batch size=16**, this time network is actually have better generalization power only because I use smaller batch size. So it is worth trying to use even more small batch next time. (Or just the learning rate I need to change??) The LB score is around 0.2415.\n",
    "\n",
    "2. Test-time augmentation improve result, 40 time augmentation improve 0.2415 to 0.2311. \n",
    "\n",
    "trying resnet34:\n",
    "1. I try again for 4-feature image one, and no improvement or degradation. LB 0.1907. Worth mentioning, the 66 times augmentation don't improve my result again. I receive result 0.1921. 40 times also 0.1921\n",
    "\n",
    "\n",
    "Some advices on the ensemble:\n",
    "1. Running average of parameters during training. exponential weight decay cs231n\n",
    "2. Take more prediction using data augmentation and take average\n",
    "\n",
    "\n",
    "#### 7th December:\n",
    "\n",
    "trying resnet50\n",
    "1. small batchsize 16 and small learning rate 0.005 is not wroking.\n",
    "\n",
    "#### 9th December:\n",
    "\n",
    "trying resnet34:\n",
    "1. use different seed to sift out all model with val > 92, mean LB 0.2, minmax basemodel 0.22.\n",
    "\n",
    "#### 14th December:\n",
    "\n",
    "try resnet34:\n",
    "1. I try to rotate like 30, 60 degree, and the performance\n",
    "\n",
    "tips:\n",
    "1. There are 133 of na angle in the training data, almost 3/4 are in the last 300. I think it explain why the performance of the specific selection of dataset are better. lr wd (0.01, 0.0002) patience 10 early stopping 25. LB 0.186\n",
    "\n",
    "This is epoch:37\n",
    "[============ 41/41 ==========>]  Step: 65ms | Tot: 3s285ms | Loss: 0.122 | Acc: 95.077% (1236/1300)\n",
    "[============ 5/5 =======>.....]  Step: 33ms | Tot: 168ms | Loss: 0.180 | Acc: 94.079% (286/304)\n",
    "\n",
    "another training also with val 94.079%, 0.00021, but LB  0.201\n",
    "Can't train it with same setting!\n",
    "\n",
    "#### 15th December:\n",
    "\n",
    "trying resnet34\n",
    "1. Training another time, with acc as flag to change the lr. With this run, push the LB further to **LB 0.1801**. For the same set of data, it's amazing how the running outcome will vary during the same set of data. Thus in the future experiment, for the same fold, we should run multiple time to sift out the best one.\n",
    "This is epoch:28\n",
    "[============ 41/41 ==========>]  Step: 65ms | Tot: 3s280ms | Loss: 0.162 | Acc: 93.538% (1216/1300)\n",
    "[============ 5/5 =======>.....]  Step: 33ms | Tot: 170ms | Loss: 0.180 | Acc: 94.408% (287/304)\n",
    "\n",
    "2. train resnet34 multiple times to get a bunch of 0.17,0.18 val loss models. The best one with val loss 0.156, only achieve **0.191 on LB**. Take the average of all model, get **0.179 on lB**\n",
    "\n",
    "1. At round 0, fold 3, seed [4987], we find a good value with acc: 92.83489096573209, loss: 0.18720525540295418 **LB .1911**\n",
    "2. At round 0, fold 3, seed [4987], we find a good value with acc: 93.45794392523365, loss: 0.18422258188048626     \n",
    "3. At round 0, fold 3, seed [4987], we find a good value with acc: 93.76947040498442, loss: 0.18761429014235642     \n",
    "    \n",
    "4. At round 1, fold 3, seed [7386], we find a good value with acc: 93.14641744548287, loss: 0.18572754503410555 **LB 0.2208** only     \n",
    "This is epoch:6     \n",
    "[============ 41/41 ==========>]  Step: 28ms | Tot: 3s242ms | Loss: 0.165 | Acc: 93.453% (1199/1283)      \n",
    "[============ 6/6 ========>....]  Step: 13ms | Tot: 193ms | Loss: 0.123 | Acc: 95.950% (308/321)\n",
    "\n",
    "5. At round 1, fold 3, seed [7386], we find a good value with acc: 92.5233644859813, loss: 0.17600850301368215    \n",
    "6. At round 1, fold 3, seed [7386], we find a good value with acc: 93.76947040498442, loss: 0.17959555435774854    \n",
    "7. At round 1, fold 3, seed [7386], we find a good value with acc: 92.83489096573209, loss: 0.18345521060848533     \n",
    "\n",
    "8. At round 1, fold 4, seed [7386], we find a good value with acc: 93.125, loss: 0.19450552463531495  \n",
    "9. At round 1, fold 4, seed [7386], we find a good value with acc: 92.5, loss: 0.19440045058727265  \n",
    "\n",
    "10. At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.17605417614042573  \n",
    "11. At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.19229212802518567  \n",
    "12. At round 2, fold 1, seed [8868], we find a good value with acc: 93.45794392523365, loss: 0.17976307720410117  \n",
    "13. At round 2, fold 4, seed [8868], we find a good value with acc: 93.125, loss: 0.18329112231731415\n",
    "14. At round 2, fold 4, seed [8868], we find a good value with acc: 92.8125, loss: 0.18368732035160065  **LB0.1870** after tunning.    \n",
    "This is epoch:25   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s256ms | Loss: 0.172 | Acc: 92.991% (1194/1284)  \n",
    "[============ 5/5 =======>.....]  Step: 42ms | Tot: 177ms | Loss: 0.111 | Acc: 95.625% (306/320)  \n",
    "\n",
    "15. At round 2, fold 4, seed [8868], we find a good value with acc: 93.4375, loss: 0.19045546948909758  \n",
    "16. At round 3, fold 4, seed [9915], we find a good value with acc: 94.6875, loss: 0.17815355360507965  \n",
    "17. At round 3, fold 4, seed [9915], we find a good value with acc: 93.125, loss: 0.16909184455871581  \n",
    "18. At round 3, fold 4, seed [9915], we find a good value with acc: 92.5, loss: 0.1774096041917801   \n",
    "19. At round 3, fold 4, seed [9915], we find a good value with acc: 95.0, loss: 0.15670025497674941  \n",
    "This is epoch:8   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s229ms | Loss: 0.148 | Acc: 94.548% (1214/1284)  \n",
    "[============ 5/5 =======>.....]  Step: 44ms | Tot: 179ms | Loss: 0.129 | Acc: 95.625% (306/320)   \n",
    "\n",
    "20. At round 5, fold 4, seed [7433], we find a good value with acc: 93.4375, loss: 0.17777724266052247   \n",
    "This is epoch:12   \n",
    "[============ 41/41 ==========>]  Step: 30ms | Tot: 3s252ms | Loss: 0.169 | Acc: 93.302% (1198/1284)   \n",
    "[============ 5/5 =======>.....]  Step: 43ms | Tot: 178ms | Loss: 0.160 | Acc: 93.438% (299/320)   \n",
    "\n",
    "What I have tried:\n",
    "1. All retrain with mean ensemble: **LB 0.1787**\n",
    "2. All except my best 0.180 with mean ensemble: **LB 0.1788**\n",
    "\n",
    "#### 17th December:\n",
    "\n",
    "trying resnet34:\n",
    "0. the best one   0.136 train loss, 0.136 val loss  - **LB 0.2077**\n",
    "1. All retrain and my best 0.180 with mean ensemble. **LB 0.1791**\n",
    "2. All models 0.05, 0.95, mean ensemble **LB 0.1771**\n",
    "3. All models,0.05, 0.95, best base model **LB 0.1783**\n",
    "4. All retrain models and my best 0.180(except the folds with one that achieves 0.22 LB), 0.05, 0.95, mean ensemble. **LB 0.1762**\n",
    "\n",
    "At round 1, fold 0, seed [7791], we find a good value with acc: 92.5233644859813, loss: 0.1843862771245178\n",
    "retrain two last:\n",
    "This is epoch:19\n",
    "[=================== 41/41 =================>.]  Step: 30ms | Tot: 3s254ms | Loss: 0.175 | Acc: 92.985% (1193/1283)\n",
    "[=================== 6/6 =============>.......]  Step: 12ms | Tot: 193ms | Loss: 0.155 | Acc: 94.081% (302/321)\n",
    "\n",
    "At round 1, fold 0, seed [7791], we find a good value with acc: 93.14641744548287, loss: 0.1838890655761196\n",
    "This is epoch:4\n",
    "[=================== 41/41 =================>.]  Step: 30ms | Tot: 3s236ms | Loss: 0.153 | Acc: 93.920% (1205/1283)\n",
    "[=================== 6/6 =============>.......]  Step: 14ms | Tot: 193ms | Loss: 0.158 | Acc: 94.393% (303/321)\n",
    "\n",
    "\n",
    "#### 18th December:\n",
    "\n",
    "\n",
    "lr 0.01, wd 0.002,dropout[0.2,0.2, 0.3, 0.3, 0.4,0.4]\n",
    "This is epoch:53  \n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.195 | Acc: 91.871% (5402/5880)  \n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 94.576% (279/295)  \n",
    "  \n",
    "This is epoch:59()  \n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.178 | Acc: 92.313% (5428/5880)  \n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 92.881% (274/295)  \n",
    "loss: Save it!  \n",
    "  \n",
    "0.002 -> 0.0015\n",
    "\n",
    "lr 0.01, wd 0.00175  **LB0.1717**     \n",
    "This is epoch:64  \n",
    "[============ 184/184 ========>] Step: 0ms| Tot: 2s8ms|Loss: 0.166 | Acc: 93.333% (5488/5880)  \n",
    "[============   5/  5 ===>.....] Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 94.237% (278/295)  \n",
    "\n",
    "trying resnet34:\n",
    "1. All retrain models and my best 0.180, 0.05, 0.95, mean ensemble. **LB 0.1762 still!**\n",
    "2. All retrain models and my best 0.180, 0.1, 0.9, mean ensemble. **LB 0.1783**\n",
    "3. All retrain models and my best 0.180, 0.025, 0.975, mean ensemble. **LB 0.1767**\n",
    "4. All retrain models so far, 0.05, 0.95, mean ensemble. **LB 0.1758**. So actually more retrain models give me better results.\n",
    "\n",
    "lr 0.01, wd 0.002,dropout[0.3,0.3, 0.4, 0.4, 0.5,0.5]\n",
    "This is epoch:197\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 2s8ms|Loss: 0.175 | Acc: 93.027% (5470/5880)  \n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.180 | Acc: 92.542% (273/295)  \n",
    "loss: Save it!\n",
    "\n",
    "\n",
    "#### 19th December \n",
    "\n",
    "trying plain cnn(finally got 36 models):\n",
    "1. The promising one with val loss 0.13 and train_loss even less, **LB 0.1926**\n",
    "2. take grand mean **LB 0.1699**\n",
    "3. minmax 0.05, 0.95 mean **LB 0.1691**\n",
    "4. minmax 0.05, 0.95 median **LB 0.1555**\n",
    "5. minmax 0.1, 0.9 median **LB 0.1551**\n",
    "\n",
    "trying resnet again here:\n",
    "6. minmax 0.05, 0.95 median **LB 0.1756**\n",
    "\n",
    "\n",
    "#### 20th December \n",
    "\n",
    "median is more like voting, especially when the number of model is large. \n",
    "\n",
    "trying plain cnn（36 models):   \n",
    "1. median ensemble to see if median has any advantage at all\n",
    "2. minmax 0.2, 0.8 median  **LB 0.1748**\n",
    "3. minmax 0.15, 0.85 median **LB 0.1546**\n",
    "4. Using the full data augmentation and get **LB:0.3885**\n",
    "\n",
    "trying resnet34:  \n",
    "1. all models using minmax 0.05, 0.95 median ensemble\n",
    "\n",
    "I want to add more models plain cnn here. I will see if the performance will improve dramatically or it has saturated already.\n",
    "\n",
    "This is epoch:75\n",
    "[========= 294/294 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.168 | Acc: 93.017% (8751/9408)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.178 | Acc: 92.542% (273/295)\n",
    "loss: Save it!\n",
    "\n",
    "\n",
    "This is epoch:78\n",
    "[========= 294/294 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.169 | Acc: 92.878% (8738/9408)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 92.542% (273/295)\n",
    "loss: Save it!\n",
    "\n",
    "And I train two more here\n",
    "(0.218, 91.86440677966101, 48)*I think this one ends too soon here.\n",
    "(0.188, 93.55932203389831, 97)\n",
    "\n",
    "I do observe better result by add more augmentation, what about random augmentation?\n",
    "not so good.\n",
    "\n",
    "\n",
    "\n",
    "trying fcn:\n",
    "I need to see what architecture works out good here!:\n",
    "1. layers = 64,128,256,512, dropout= [0.2,0.2,0.3,0.4]\n",
    "2. layers= [64,128,256,256], dropout= [0.2,0.2,0.3,0.4]\n",
    "0.2056680501517603, 92.88135593220339\n",
    "3. layers= [64,128,256,256], dropout= [0.2,0.2,0.3,0.5]\n",
    "This is epoch:103\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 3s3ms|Loss: 0.110 | Acc: 95.680% (5626/5880)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 93.898% (277/295)\n",
    "Whoa! SOOO GOOD, I will try again to see if it's somewhat consistent here. **LB:0.2112** It made me remember another result several days ago. It fits well doesn't mean it is good for a single model, it's actually overfitting in someway. But I think it's still worth trying!.\n",
    "(0.19807816213470394, 93.22033898305085, 114)\n",
    "(0.18171497209597443, 93.89830508474576, 117)\n",
    "[(0.195559827200437, 92.88135593220339, 88),\n",
    "0.18043368595131373, 92.88135593220339, 101),\n",
    "0.18044361972202688, 94.23728813559322, 124)]\n",
    "[(0.1838040376618757, 92.88135593220339, 98),\n",
    " (0.19152086276119037, 92.88135593220339, 90),\n",
    " (0.19953574477616004, 92.88135593220339, 95)]\n",
    "\n",
    "5. layers= [64,128,256,256], dropout= [0.2,0.2,0.3,0.7]\n",
    "[(0.16435863317574484, 93.89830508474576, 109),\n",
    " (0.17824883652945697, 93.55932203389831, 108),\n",
    " (0.1873901376784858, 93.89830508474576, 104)]\n",
    "[(0.18382113386513824, 93.55932203389831, 113),\n",
    " (0.1890310712790085, 94.23728813559322, 112),\n",
    " (0.19510147627127372, 92.88135593220339, 91)] \n",
    " \n",
    " \n",
    "6. layers= [64,128,256,256], dropout= [0.2,0.2,0.5,0.7]\n",
    "[(0.1898328377028643, 93.22033898305085, 138),\n",
    " (0.1851983806844485, 92.88135593220339, 135),\n",
    " (0.1942403570575229, 93.22033898305085, 89)]\n",
    "\n",
    "7. layers= [64,128,256,256], dropout= [0.2,0.2,0.3,0.7] add reg to 0.005\n",
    "[(0.18031210459895053, 93.55932203389831, 167),\n",
    " (0.17528720268253553, 93.55932203389831, 121)]\n",
    "[(0.20459563383611581, 92.20338983050847, 133),\n",
    " (0.19494897334252373, 92.54237288135593, 111),\n",
    " (0.18696752806841316, 93.55932203389831, 91)]\n",
    " \n",
    " \n",
    "I will choose this architecture. layers= [64,128,256,256], dropout= [0.2,0.2,0.3,0.7], reg=0.002\n",
    "\n",
    "try fcn 50 models:\n",
    "I observe strong correlation between these 50 models, and that's the reason why median is not working\n",
    "1. grand mean **LB 0.1732**\n",
    "2. min max 0.05, 0.95 median:  **LB 0.1862**\n",
    "3. r2-11 mean ensemble **LB 0.1729**\n",
    "\n",
    "\n",
    "trying plain cnn, still going to find the best way to use it:\n",
    "1. round2 training with one round, grand mean **0.1583**  \n",
    "2. use it as seperate, not so good.  \n",
    "2. using all 0.1,0.9 minmax median **0.1553**  \n",
    "2. using all 0.05,0.95 minmax median **0.1558**  \n",
    "use the round2 trained as based model, minmax 0.05, 0.95, we get **0.1573**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 22th December\n",
    "\n",
    "we do have a senet **LB 0.152** here\n",
    "Next goal:\n",
    "1. train a good vgg\n",
    "2. train a good resnet18\n",
    "3. (train a good resnet50)\n",
    "4. train gbm to increase the diversity of model\n",
    "\n",
    "#### 23th December\n",
    "\n",
    "### Get info that inc_ang is important!\n",
    "\n",
    "trying inception v3:\n",
    "on the na cancelled data, we achieve best with dropout 0.4, lr 0.01, wd,0.00005. First try this agenda\n",
    "\n",
    "0.1787\n",
    "\n",
    "minmax 0.9,0.1: 0.1819\n",
    "minmax 0.95 0.1835\n",
    "0.8 0.1791\n",
    "\n",
    "all models then 0.1828\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 23th December\n",
    "\n",
    "0.2007\n",
    "after tunning! 0.2000\n",
    "\n",
    "sift out the second model(lgb) and get **LB 0.2018** \n",
    "\n",
    "#### 25th December \n",
    "\n",
    "20 rounds full 5-fold models: **LB 0.2038**\n",
    "\n",
    "463 sifted good model (val_loss<0.195) mean **LB 0.2013**\n",
    "\n",
    "#### 26th December \n",
    "\n",
    "test my fcn(5 fold cv):\n",
    "\n",
    "dropout test:\n",
    "\n",
    "dropout=[0.2,0.2,0.3,0.5]    0.20885586443\n",
    "dropout=[0.2,0.2,0.3,0.7]    0.188997768215\n",
    "dropout=[0.2,0.2,0.3,0.4]    0.191862137737\n",
    "dropout=[0.2,0.2,0.5,0.7]    0.190536881425\n",
    "\n",
    "wd test:\n",
    "0.008,   0.196444644614\n",
    "0.005,   0.191571132873\n",
    "\n",
    "0.003    0.190497368597\n",
    "0.0025   0.195098413127\n",
    "0.002    0.193542474147\n",
    "\n",
    "0.001,   0.194501658298\n",
    "0.0005,  0.195932370073\n",
    "0.0001,  0.201556356661\n",
    "\n",
    "fancy augmentation:\n",
    "0.193472324949\n",
    "\n",
    "#### 27th December \n",
    "\n",
    "cnn sift out the potential 'bad' one\n",
    "\n",
    "TTA\n",
    "before: 5 chose r2 fcn models **LB 0.1766**\n",
    "after :                       **LB 0.2029**\n",
    "\n",
    "try cnn add angle:\n",
    "\n",
    "wd:0.003\n",
    "This is epoch:73\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 2s9ms|Loss: 0.157 | Acc: 93.384% (5491/5880)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 93.559% (276/295)\n",
    "acc: Save it!\n",
    "\n",
    "wd:0.002\n",
    "This is epoch:68\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 2s9ms|Loss: 0.164 | Acc: 93.231% (5482/5880)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 92.881% (274/295)\n",
    "loss: Save it!\n",
    "\n",
    "change angle /90\n",
    "wd:0.005: Loss to train loss 0.14* val 0.170. I will try again later\n",
    "\n",
    "This is epoch:111\n",
    "[========= 184/184 ======>]Step: 0ms| Tot: 2s8ms|Loss: 0.188 | Acc: 92.602% (5445/5880)\n",
    "[=========   5/  5 ==>....]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 92.203% (272/295)\n",
    "loss: Save it!\n",
    "\n",
    "The angle model don't give me very impressive result. So for now I will dispose this idea for a moment.\n",
    "\n",
    "Due to the unsure of cnn, I retrain 33 models I got before and see how they perform. Very disappointedly, they work out not so good.\n",
    "\n",
    "A single model of train_loss 0.06, val loss 0.06  **LB 0.2294** \n",
    "the mean of 33 retrain models:                    **LB 0.1703** \n",
    "minmax 0.05, 0.95 median of 33 retrain models:    **LB 0.1909** \n",
    "\n",
    "\n",
    "round two: 5 retrained model: **LB 0.1767**   \n",
    "round two models: choose the best performed in each fold. **LB 0.1561**. mean is 0.1909255.  \n",
    "They are: \n",
    "Phase2, At fold 0, seed [7135],round 2 we find one with acc: 93.55932203389831, loss: 0.2079596897303048\n",
    "Phase2, At fold 1, seed [7135],round 2 we find one with acc: 95.23809523809524, loss: 0.14296855346686174\n",
    "Phase2, At fold 2, seed [7135],round 1 we find one with acc: 92.17687074829932, loss: 0.24128405476103024\n",
    "Phase2, At fold 3, seed [7135],round 2 we find one with acc: 92.85714285714286, loss: 0.183856349235692\n",
    "Phase2, At fold 4, seed [7135],round 2 we find one with acc: 93.19727891156462, loss: 0.1785587087577703\n",
    "\n",
    "mean: 0.1796153 **LB 0.1636** So I choose to add three times to make it more stable.\n",
    "Phase4, At fold 0, seed 25364,round 2 we find one with acc: 91.1864406779661, loss: 0.23396236732349557   \n",
    "Phase4, At fold 1, seed 25364,round 2 we find one with acc: 95.23809523809524, loss: 0.1527542131794553    \n",
    "Phase4, At fold 2, seed 25364,round 3 we find one with acc: 93.5374149659864, loss: 0.20037819019385747   \n",
    "Phase4, At fold 3, seed 25364,round 2 we find one with acc: 95.23809523809524, loss: 0.15461719558149778   \n",
    "Phase4, At fold 4, seed 25364,round 2 we find one with acc: 94.89795918367346, loss: 0.15636469930613123   \n",
    "\n",
    "The final 3 models of cnn get **LB 0.1583**, I think I will use it.\n",
    "\n",
    "try fcn and sift out the good. get **LB 0.1705**   \n",
    "\n",
    "\n",
    "paramater count of different models:\n",
    "1. CNN: 64*5*5*64*5*5+64*5*5*32*5*5 + 32*5*5*2+ 3*3*3*64+64*3*3*128+128*3*3*128+128*3*3*64= 4138240\n",
    "2. FCN:                                         3*3*3*64+64*3*3*128+128*3*3*256+256*3*3*256 + 256*3*3*2= 964800\n",
    "3. Resnet34: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Jan\n",
    "\n",
    "10FOLD vgg_bn 0.1836\n",
    "5FOLD vgg_bn 0.1918\n",
    "0.1376 final+vgg_bn -gbm\n",
    "0.1371 final+vgg_bn\n",
    "0.1421 weighted ensemble: final+vgg_bn\n",
    "\n",
    "\n",
    "I am still training vgg today. The best val loss appears when I cancel data augmentation, I suspect it will also be helpful in the training of other models, so further experiment is needed.\n",
    "Several points I don't know how to explain but useful in training.\n",
    "1. I change vgg_bn to vgg and model improve signficantly. I don't know why that is \n",
    "\n",
    "10 fold cv with angle no aug,seed 9220. cv logloss mean 0.185038\n",
    "mean **LB 1710**\n",
    "median **LB 0.1650** (Use median because the correlation between them is low, that's really weird)\n",
    "10folds, Phase2, At fold 0, seed 9220,round 2 we find one with acc: 94.40993788819875, loss: 0.17998989833437878\n",
    "10folds, Phase2, At fold 1, seed 9220,round 1 we find one with acc: 95.03105590062111, loss: 0.16718394008482465\n",
    "10folds, Phase2, At fold 2, seed 9220,round 2 we find one with acc: 93.7888198757764, loss: 0.19904483242758683\n",
    "10folds, Phase2, At fold 3, seed 9220,round 2 we find one with acc: 94.40993788819875, loss: 0.16255430257098274\n",
    "10folds, Phase2, At fold 4, seed 9220,round 2 we find one with acc: 94.375, loss: 0.19608657360076903\n",
    "10folds, Phase2, At fold 5, seed 9220,round 2 we find one with acc: 92.5, loss: 0.1867382526397705\n",
    "10folds, Phase2, At fold 6, seed 9220,round 1 we find one with acc: 93.125, loss: 0.19803838133811952\n",
    "10folds, Phase2, At fold 7, seed 9220,round 1 we find one with acc: 93.125, loss: 0.1891440987586975\n",
    "10folds, Phase2, At fold 8, seed 9220,round 2 we find one with acc: 94.375, loss: 0.15092881619930268\n",
    "10folds, Phase2, At fold 9, seed 9220,round 1 we find one with acc: 91.875, loss: 0.22067069113254548\n",
    "\n",
    "I will try like 5 fold tmr and furthur test this result I got, maybe do another 10 fold?\n",
    "\n",
    "**LB 0.1347** with this training\n",
    "\n",
    "vgg change wd to 0.0001 , 5fold median. **LB 0.1729**   \n",
    "\n",
    "---\n",
    "\n",
    "resnet50 10 fold median **LB 0.1864**  (Maybe this is the first one, I may need to abandon this result, augmentation yields better result.)\n",
    "\n",
    "\n",
    "resnet50 with aug median: **LB 0.1783**  \n",
    "resnet50 with aug mean: **LB 0.1821**  \n",
    "resnet50 mean cv logloss$\\approx0.2$  \n",
    "10folds, Phase2,with augmentation, At fold 0, seed 9021,round 2 we find one with acc: 95.03105590062111, loss: 0.16581373789017986   \n",
    "10folds, Phase2,with augmentation, At fold 1, seed 9021,round 2 we find one with acc: 92.54658385093168, loss: 0.1733171411367677  \n",
    "10folds, Phase2,with augmentation, At fold 2, seed 9021,round 2 we find one with acc: 90.6832298136646, loss: 0.2190303859410819  \n",
    "10folds, Phase2,with augmentation, At fold 3, seed 9021,round 1 we find one with acc: 91.30434782608695, loss: 0.21518208502982714  \n",
    "10folds, Phase2,with augmentation, At fold 4, seed 9021,round 2 we find one with acc: 93.75, loss: 0.23215826451778412   \n",
    "10folds, Phase2,with augmentation, At fold 5, seed 9021,round 2 we find one with acc: 97.5, loss: 0.16  \n",
    "10folds, Phase2,with augmentation, At fold 6, seed 9021,round 2 we find one with acc: 93.125, loss: 0.220481339097023   \n",
    "10folds, Phase2,with augmentation, At fold 7, seed 9021,round 1 we find one with acc: 94.375, loss: 0.19899997115135193   \n",
    "10folds, Phase2,with augmentation, At fold 8, seed 9021,round 1 we find one with acc: 93.125, loss: 0.1750522032380104  \n",
    "10folds, Phase2,with augmentation, At fold 9, seed 9021,round 1 we find one with acc: 91.875, loss: 0.24764818847179412  \n",
    "\n",
    "##### change model choosing citerion to any logloss with the lowest, no need to be less than \n",
    "median **LB 0.1757**\n",
    "mean **LB 0.1800**\n",
    "resnet50 mean cv logloss$\\approx0.18$   \n",
    "10folds, Phase3,with augm, At fold 0, seed 2170,round 2 we find one with acc: 92.54658385093168, loss: 0.21921243837901525  \n",
    "10folds, Phase3,with augm, At fold 1, seed 2170,round 1 we find one with acc: 96.8944099378882, loss: 0.11521336750954575  \n",
    "10folds, Phase3,with augm, At fold 2, seed 2170,round 2 we find one with acc: 93.7888198757764, loss: 0.19353959565947515  \n",
    "10folds, Phase3,with augm, At fold 3, seed 2170,round 1 we find one with acc: 93.7888198757764, loss: 0.16900170034503345  \n",
    "10folds, Phase3,with augm, At fold 4, seed 2170,round 2 we find one with acc: 92.5, loss: 0.21836901754140853  \n",
    "10folds, Phase3,with augm, At fold 5, seed 2170,round 2 we find one with acc: 91.875, loss: 0.22409132719039918  \n",
    "10folds, Phase3,with augm, At fold 6, seed 2170,round 1 we find one with acc: 93.75, loss: 0.16519343554973603  \n",
    "10folds, Phase3,with augm, At fold 7, seed 2170,round 2 we find one with acc: 96.875, loss: 0.12502912282943726  \n",
    "10folds, Phase3,with augm, At fold 8, seed 2170,round 1 we find one with acc: 92.5, loss: 0.226381254196167  \n",
    "10folds, Phase3,with augm, At fold 9, seed 2170,round 1 we find one with acc: 94.375, loss: 0.15994667410850524  \n",
    "\n",
    "try fcn 20:  \n",
    "median directly: **LB 0.1727**  \n",
    "median every 5fold and have the mean: **LB 0.1720**  \n",
    "Maybe this strategy is not working for 5 fold? I don't know. I will try to make a 10 fold several days later\n",
    "\n",
    "\n",
    "#### 2nd Jan\n",
    "\n",
    "for vgg19, file: vgg19_10fold.csv median:        **LB 1577**  \n",
    "for vgg19, file: vgg19_10fold.csv(minmax0.2,0.8) **LB 0.1499**  \n",
    "for vgg19, file: vgg19_10fold2.csv median:        **LB 0.1652**  \n",
    "\n",
    "the first one has less correlation, the second one more highly correlated in each fold\n",
    "\n",
    "First one( mean:0.1804867 )vgg19_10fold.csv  \n",
    "10folds, Phase1, At fold 0, seed 6111,round 1 we find one with acc: 95.03105590062111, loss: 0.14730554310061175  \n",
    "10folds, Phase1, At fold 1, seed 6111,round 1 we find one with acc: 92.54658385093168, loss: 0.2261211675886782  \n",
    "10folds, Phase1, At fold 2, seed 6111,round 2 we find one with acc: 93.7888198757764, loss: 0.15770083848641525  \n",
    "10folds, Phase1, At fold 3, seed 6111,round 2 we find one with acc: 95.65217391304348, loss: 0.14488794876736885  \n",
    "10folds, Phase1, At fold 4, seed 6111,round 2 we find one with acc: 91.875, loss: 0.22704902291297913  \n",
    "10folds, Phase1, At fold 5, seed 6111,round 2 we find one with acc: 95.625, loss: 0.13715559244155884  \n",
    "10folds, Phase1, At fold 6, seed 6111,round 1 we find one with acc: 93.125, loss: 0.1816830426454544  \n",
    "10folds, Phase1, At fold 7, seed 6111,round 1 we find one with acc: 93.125, loss: 0.19585612118244172  \n",
    "10folds, Phase1, At fold 8, seed 6111,round 2 we find one with acc: 93.75, loss: 0.1799779638648033  \n",
    "10folds, Phase1, At fold 9, seed 6111,round 1 we find one with acc: 93.125, loss: 0.2071295917034149  \n",
    "\n",
    "Second one( mean:0.1946331 )vgg19_10fold2.csv  \n",
    "10folds, Phase2, At fold 0, seed 1290,round 1 we find one with acc: 92.54658385093168, loss: 0.17023844406101274  \n",
    "10folds, Phase2, At fold 1, seed 1290,round 1 we find one with acc: 93.7888198757764, loss: 0.17357573810941684  \n",
    "10folds, Phase2, At fold 2, seed 1290,round 2 we find one with acc: 93.7888198757764, loss: 0.23012021498650498  \n",
    "10folds, Phase2, At fold 3, seed 1290,round 1 we find one with acc: 91.92546583850931, loss: 0.22144015772002085  \n",
    "10folds, Phase2, At fold 4, seed 1290,round 2 we find one with acc: 91.875, loss: 0.2107028067111969   \n",
    "10folds, Phase2, At fold 5, seed 1290,round 1 we find one with acc: 91.875, loss: 0.24272730052471161    \n",
    "10folds, Phase2, At fold 6, seed 1290,round 2 we find one with acc: 94.375, loss: 0.161759914457798   \n",
    "10folds, Phase2, At fold 7, seed 1290,round 2 we find one with acc: 95.0, loss: 0.16489596143364907  \n",
    "10folds, Phase2, At fold 8, seed 1290,round 2 we find one with acc: 93.125, loss: 0.16848602294921874  \n",
    "10folds, Phase2, At fold 9, seed 1290,round 1 we find one with acc: 93.125, loss: 0.20238457024097442  \n",
    "\n",
    "#### 4th Jan\n",
    "\n",
    "1. Inception v-3\n",
    "minmax median 0.1,0.9. **LB: 0.1599**  \n",
    "simple median  **LB: 0.1625**  \n",
    "CV mean:0.163  \n",
    "10folds, Phase2,with aug, At fold 0, seed 768,round 2 we find one with acc: 93.7888198757764, loss: 0.17611497409225252  \n",
    "10folds, Phase2,with aug, At fold 2, seed 768,round 2 we find one with acc: 95.65217391304348, loss: 0.11043830852238287  \n",
    "10folds, Phase2,with aug, At fold 3, seed 768,round 1 we find one with acc: 94.40993788819875, loss: 0.15538229548042606  \n",
    "0.248  \n",
    "10folds, Phase2,with aug, At fold 5, seed 768,round 2 we find one with acc: 93.75, loss: 0.182021564245224  \n",
    "10folds, Phase2,with aug, At fold 6, seed 768,round 1 we find one with acc: 95.625, loss: 0.14549611508846283  \n",
    "10folds, Phase2,with aug, At fold 7, seed 768,round 2 we find one with acc: 95.0, loss: 0.1196599543094635  \n",
    "10folds, Phase2,with aug, At fold 8, seed 768,round 2 we find one with acc: 96.875, loss: 0.15567596033215522  \n",
    "10folds, Phase2,with aug, At fold 9, seed 768,round 1 we find one with acc: 95.625, loss: 0.16828406006097793  \n",
    "10folds, Phase2,with aug, At fold 1, seed 768,round 2 we find one with acc: 92.54658385093168, loss: 0.1788613901051305  \n",
    "\n",
    "\n",
    "resnet18:\n",
    "**LB 0.1960**\n",
    "\n",
    "#### 8th Jan\n",
    "\n",
    "final inception minmax 0.2,0.8: 0.1524   \n",
    "final inception median: 0.1570   \n",
    "\n",
    "\n",
    "#### 9th Jan\n",
    "final vgg16 minmax 0.2,0.8: 0.1472\n",
    "final vgg16 minmax 0.3,0.7: 0.1479 \n",
    "\n",
    "#### 11th Jan\n",
    "final resnet34 **LB 0.1663**\n",
    "But it seems like it will have large correlation with resnet50.\n",
    "\n",
    "\n",
    "## Final tryout\n",
    "The model I train or trust:\n",
    "1. Senet\n",
    "2. Densenet\n",
    "3. fcn\n",
    "4. resnet34\n",
    "5. cnn\n",
    "6. inception\n",
    "7. gbm\n",
    "\n",
    "The model I dont trust:\n",
    "1. original fcn\n",
    "2. tf-keras\n",
    "3. keras\n",
    "\n",
    "0.1, 0.9 minmax  T:1,2,3,4,5,6,7 N:1  **0.1323**   \n",
    "0.1, 0.9 minmax  T:1,2,3,4,5,6,7 N:   **0.1347** (delete that may not be bad, it might just work out on PB)  \n",
    "0.1, 0.9 minmax  T:1,2,3,4,5,6,7 N:1,2,3  **0.1289**   \n",
    "\n",
    "0.1, 0.9 mimmax  T:1,2,3,4,5,6,**my7**   **0.1335** (while I set max and min to 1-1e-10 and 1e-10, which means I got no wrongness here)\n",
    "0.2, 0.8 mimmax  T:1,2,3,4,5,6,**my7**   **0.1327** (while I set max and min to 1-1e-10 and 1e-10, which means I got no wrongness here)\n",
    "\n",
    "fcn to the mean change(just change it right)\n",
    "then **LB 0.1350**\n",
    "resnet34 to the grand mean: 0.1331\n",
    "change cnn to the grand mean: 0.1371\n",
    "\n",
    "\n",
    "\n",
    "changed to cnn_30 models and fcn_20 models.\n",
    "\n",
    "0.1, 0.9 minmax median  T:1,2,3,4,5,6,7 N:   **0.1382**\n",
    "0.3, 0.7 minmax median  T:1,2,3,4,5,6,7 N:   **0.1369**\n",
    "0.1, 0.9 minmax median  T:1,2,3,4,5,6 N:     **0.1386** \n",
    "0.1, 0.9 minmax mean    T:1,2,3,4,5,6,7 N:   **0.1401**   \n",
    "\n",
    "1,2,3,4,5,6,7, resnet50:resnet50_10fold.csv, vgg16:vgg_10fold.csv, vgg19:vgg19_10fold.csv(minmax)  **LB 0.1336**\n",
    "1,2,3,4,5, resnet50:resnet50_10fold.csv, vgg16:vgg_10fold.csv, vgg19:vgg19_10fold.csv(minmax)      **LB 0.1332**\n",
    "1,2,3,4,5, resnet50:resnet50_10fold.csv, vgg16:vgg_10fold.csv, vgg19:vgg19_10fold.csv(no minmax)   **LB 0.1364**\n",
    "\n",
    "\n",
    "1,2,3,4,5, resnet50:resnet50_10fold.csv(minmax0.10.9), vgg16:vgg_10fold.csv, vgg19:vgg19_10fold.csv(minmax0.2,0.8)\n",
    "incep_10fold.csv(minmax 0.1,0.9). 9 models  **LB 0.1350**\n",
    "\n",
    "1,2,5 , vgg16:vgg_10fold.csv, vgg19:vgg19_10fold.csv(minmax0.2,0.8), incep_10fold.csv(minmax 0.1,0.9). 6 models **LB 0.1355**\n",
    "\n",
    "all models(new ones use simple median(conservative)) **LB 0.1318** (0.3, 0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final resnet50, 10fold with aug and no dense\n",
    "\n",
    "5mean, for each 10cv median  **LB 0.1638**\n",
    "5mean, for each 10cv minmax0.2,0.8 median **LB 0.1621**\n",
    "5median, for each 10cv minmax0.2,0.8 median **LB 0.1650** When correlation is really high, mean is always good.\n",
    "\n",
    "\n",
    "cv mean- 0.1855228    **LB 0.1630**\n",
    "10folds, Phase4,with augm,no dense,At fold 0, seed 768,round 3 we find one with acc: 91.92546583850931, loss: 0.2469322307509665  \n",
    "10folds, Phase4,with augm,no dense,At fold 1, seed 768,round 2 we find one with acc: 92.54658385093168, loss: 0.1796084061172438  \n",
    "10folds, Phase4,with augm,no dense,At fold 2, seed 768,round 1 we find one with acc: 93.7888198757764, loss: 0.15580936285279554  \n",
    "10folds, Phase4,with augm,no dense,At fold 3, seed 768,round 1 we find one with acc: 92.54658385093168, loss: 0.19113417094723778  \n",
    "10folds, Phase4,with augm,no dense,At fold 4, seed 768,round 2 we find one with acc: 93.75, loss: 0.2535153150558472  \n",
    "10folds, Phase4,with augm,no dense,At fold 5, seed 768,round 1 we find one with acc: 95.0, loss: 0.178956863284111038  \n",
    "10folds, Phase4,with augm,no dense,At fold 6, seed 768,round 2 we find one with acc: 96.25, loss: 0.1482841342687607  \n",
    "10folds, Phase4,with augm,no dense,At fold 7, seed 768,round 2 we find one with acc: 96.875, loss: 0.12508011907339095  \n",
    "10folds, Phase4,with augm,no dense,At fold 8, seed 768,round 1 we find one with acc: 91.875, loss: 0.21355861574411392  \n",
    "10folds, Phase4,with augm,no dense,At fold 9, seed 768,round 1 we find one with acc: 95.625, loss: 0.16234854906797408  \n",
    "\n",
    "\n",
    "cv mean - 0.1816953  **LB 0.1721**  \n",
    "10folds, Phase4,with augm,no dense,At fold 0, seed 4096,round 2 we find one with acc: 96.8944099378882, loss: 0.09906100921379113  \n",
    "10folds, Phase4,with augm,no dense,At fold 1, seed 4096,round 1 we find one with acc: 94.40993788819875, loss: 0.16080622101977746  \n",
    "10folds, Phase4,with augm,no dense,At fold 2, seed 4096,round 1 we find one with acc: 93.16770186335404, loss: 0.18350936907418766  \n",
    "10folds, Phase4,with augm,no dense,At fold 3, seed 4096,round 1 we find one with acc: 92.54658385093168, loss: 0.19787818378543262  \n",
    "10folds, Phase4,with augm,no dense,At fold 4, seed 4096,round 2 we find one with acc: 91.25, loss: 0.22782183885574342  \n",
    "10folds, Phase4,with augm,no dense,At fold 5, seed 4096,round 1 we find one with acc: 92.5, loss: 0.21677926182746887  \n",
    "10folds, Phase4,with augm,no dense,At fold 6, seed 4096,round 2 we find one with acc: 92.5, loss: 0.1788931369781494  \n",
    "10folds, Phase4,with augm,no dense,At fold 7, seed 4096,round 1 we find one with acc: 89.375, loss: 0.29638972878456116  \n",
    "10folds, Phase4,with augm,no dense,At fold 8, seed 4096,round 1 we find one with acc: 98.125, loss: 0.09436087235808373  \n",
    "10folds, Phase4,with augm,no dense,At fold 9, seed 4096,round 1 we find one with acc: 95.0, loss: 0.16145298406481742  \n",
    "\n",
    "cv mean - 0.1818136  \n",
    "10folds, Phase4,with augm,no dense,At fold 0, seed 12987,round 1 we find one with acc: 94.40993788819875, loss: 0.19437752108766426  \n",
    "10folds, Phase4,with augm,no dense,At fold 1, seed 12987,round 1 we find one with acc: 92.54658385093168, loss: 0.21164310672638578  \n",
    "10folds, Phase4,with augm,no dense,At fold 2, seed 12987,round 2 we find one with acc: 91.30434782608695, loss: 0.22289319179072883  \n",
    "10folds, Phase4,with augm,no dense,At fold 3, seed 12987,round 1 we find one with acc: 95.03105590062111, loss: 0.15074475493675432  \n",
    "10folds, Phase4,with augm,no dense,At fold 4, seed 12987,round 1 we find one with acc: 92.5, loss: 0.17912872433662413  \n",
    "10folds, Phase4,with augm,no dense,At fold 5, seed 12987,round 1 we find one with acc: 95.0, loss: 0.13946886509656906  \n",
    "10folds, Phase4,with augm,no dense,At fold 6, seed 12987,round 2 we find one with acc: 95.0, loss: 0.17913521230220794  \n",
    "10folds, Phase4,with augm,no dense,At fold 7, seed 12987,round 1 we find one with acc: 93.125, loss: 0.17577919363975525  \n",
    "10folds, Phase4,with augm,no dense,At fold 8, seed 12987,round 1 we find one with acc: 95.625, loss: 0.16009887754917146  \n",
    "10folds, Phase4,with augm,no dense,At fold 9, seed 12987,round 2 we find one with acc: 94.375, loss: 0.20486608445644378  \n",
    "\n",
    "cv mean - 0.1742145\n",
    "10folds, Phase4,with augm,no dense,At fold 0, seed 19589,round 2 we find one with acc: 94.40993788819875, loss: 0.1421482977659806\n",
    "10folds, Phase4,with augm,no dense,At fold 1, seed 19589,round 1 we find one with acc: 94.40993788819875, loss: 0.20905315147145934\n",
    "10folds, Phase4,with augm,no dense,At fold 2, seed 19589,round 2 we find one with acc: 95.65217391304348, loss: 0.17344983255270846\n",
    "10folds, Phase4,with augm,no dense,At fold 3, seed 19589,round 1 we find one with acc: 93.7888198757764, loss: 0.1359396273304957\n",
    "10folds, Phase4,with augm,no dense,At fold 4, seed 19589,round 1 we find one with acc: 95.0, loss: 0.12574846148490906\n",
    "10folds, Phase4,with augm,no dense,At fold 5, seed 19589,round 2 we find one with acc: 94.375, loss: 0.1462269753217697\n",
    "10folds, Phase4,with augm,no dense,At fold 6, seed 19589,round 2 we find one with acc: 95.0, loss: 0.153817717730999\n",
    "10folds, Phase4,with augm,no dense,At fold 7, seed 19589,round 1 we find one with acc: 94.375, loss: 0.15000420063734055\n",
    "10folds, Phase4,with augm,no dense,At fold 8, seed 19589,round 1 we find one with acc: 92.5, loss: 0.2414792001247406\n",
    "10folds, Phase4,with augm,no dense,At fold 9, seed 19589,round 1 we find one with acc: 93.75, loss: 0.26427762657403947\n",
    "\n",
    "\n",
    "cv mean - 0.1888569\n",
    "10folds, Phase4,with augm,no dense,At fold 0, seed 18113,round 2 we find one with acc: 92.54658385093168, loss: 0.18240884862701345\n",
    "10folds, Phase4,with augm,no dense,At fold 1, seed 18113,round 1 we find one with acc: 93.16770186335404, loss: 0.21058275314591687\n",
    "10folds, Phase4,with augm,no dense,At fold 2, seed 18113,round 1 we find one with acc: 91.30434782608695, loss: 0.22363252798963038\n",
    "10folds, Phase4,with augm,no dense,At fold 3, seed 18113,round 1 we find one with acc: 96.27329192546584, loss: 0.12513534083777333\n",
    "10folds, Phase4,with augm,no dense,At fold 4, seed 18113,round 1 we find one with acc: 94.375, loss: 0.21258803009986876\n",
    "10folds, Phase4,with augm,no dense,At fold 5, seed 18113,round 2 we find one with acc: 93.75, loss: 0.14868703484535217\n",
    "10folds, Phase4,with augm,no dense,At fold 6, seed 18113,round 2 we find one with acc: 91.875, loss: 0.21845919489860535\n",
    "10folds, Phase4,with augm,no dense,At fold 7, seed 18113,round 1 we find one with acc: 91.875, loss: 0.1847989797592163\n",
    "10folds, Phase4,with augm,no dense,At fold 8, seed 18113,round 2 we find one with acc: 93.75, loss: 0.1728863000869751\n",
    "10folds, Phase4,with augm,no dense,At fold 9, seed 18113,round 1 we find one with acc: 91.25, loss: 0.20938956923782825\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final inception-v3, 10fold with aug and no dense\n",
    "\n",
    "CV mean:0.163  \n",
    "10folds, Phase2,with aug, At fold 0, seed 768,round 2 we find one with acc: 93.7888198757764, loss: 0.17611497409225252  \n",
    "10folds, Phase2,with aug, At fold 2, seed 768,round 2 we find one with acc: 95.65217391304348, loss: 0.11043830852238287  \n",
    "10folds, Phase2,with aug, At fold 3, seed 768,round 1 we find one with acc: 94.40993788819875, loss: 0.15538229548042606  \n",
    "0.248  \n",
    "10folds, Phase2,with aug, At fold 5, seed 768,round 2 we find one with acc: 93.75, loss: 0.182021564245224  \n",
    "10folds, Phase2,with aug, At fold 6, seed 768,round 1 we find one with acc: 95.625, loss: 0.14549611508846283  \n",
    "10folds, Phase2,with aug, At fold 7, seed 768,round 2 we find one with acc: 95.0, loss: 0.1196599543094635  \n",
    "10folds, Phase2,with aug, At fold 8, seed 768,round 2 we find one with acc: 96.875, loss: 0.15567596033215522  \n",
    "10folds, Phase2,with aug, At fold 9, seed 768,round 1 we find one with acc: 95.625, loss: 0.16828406006097793  \n",
    "10folds, Phase2,with aug, At fold 1, seed 768,round 2 we find one with acc: 92.54658385093168, loss: 0.1788613901051305  \n",
    "\n",
    "cv mean - 0.1649851\n",
    "10folds, Phase2,with aug, At fold 0, seed 23860,round 1 we find one with acc: 94.40993788819875, loss: 0.15873554395222517\n",
    "10folds, Phase2,with aug, At fold 1, seed 23860,round 1 we find one with acc: 97.51552795031056, loss: 0.12509169420285254\n",
    "10folds, Phase2,with aug, At fold 2, seed 23860,round 2 we find one with acc: 93.7888198757764, loss: 0.1884352898468142\n",
    "10folds, Phase2,with aug, At fold 3, seed 23860,round 1 we find one with acc: 94.40993788819875, loss: 0.14325493898080743\n",
    "10folds, Phase2,with aug, At fold 4, seed 23860,round 2 we find one with acc: 93.75, loss: 0.19179556965827943\n",
    "10folds, Phase2,with aug, At fold 5, seed 23860,round 1 we find one with acc: 92.5, loss: 0.16797470822930335\n",
    "10folds, Phase2,with aug, At fold 6, seed 23860,round 1 we find one with acc: 93.125, loss: 0.17726724594831467\n",
    "10folds, Phase2,with aug, At fold 7, seed 23860,round 1 we find one with acc: 96.25, loss: 0.112772735953331\n",
    "10folds, Phase2,with aug, At fold 8, seed 23860,round 2 we find one with acc: 92.5, loss: 0.20570084154605867\n",
    "10folds, Phase2,with aug, At fold 9, seed 23860,round 3 we find one with acc: 93.75, loss: 0.1788228675723076\n",
    "\n",
    "cv mean - 0.1811171\n",
    "10folds, Phase2,with aug, At fold 0, seed 26320,round 3 we find one with acc: 91.92546583850931, loss: 0.19614643043612842\n",
    "10folds, Phase2,with aug, At fold 1, seed 26320,round 1 we find one with acc: 93.7888198757764, loss: 0.2159437716747663\n",
    "10folds, Phase2,with aug, At fold 2, seed 26320,round 1 we find one with acc: 95.65217391304348, loss: 0.16495095036044624\n",
    "10folds, Phase2,with aug, At fold 3, seed 26320,round 2 we find one with acc: 94.40993788819875, loss: 0.15141874264856303\n",
    "10folds, Phase2,with aug, At fold 4, seed 26320,round 3 we find one with acc: 90.0, loss: 0.24522590041160583\n",
    "10folds, Phase2,with aug, At fold 5, seed 26320,round 1 we find one with acc: 93.125, loss: 0.19199223592877387\n",
    "10folds, Phase2,with aug, At fold 6, seed 26320,round 1 we find one with acc: 93.125, loss: 0.23435863852500916\n",
    "10folds, Phase2,with aug, At fold 7, seed 26320,round 2 we find one with acc: 92.5, loss: 0.178410342335701\n",
    "10folds, Phase2,with aug, At fold 8, seed 26320,round 2 we find one with acc: 97.5, loss: 0.11526270434260369\n",
    "10folds, Phase2,with aug, At fold 9, seed 26320,round 2 we find one with acc: 96.875, loss: 0.11746168062090874\n",
    "\n",
    "cv mean - 0.1859262\n",
    "10folds, Phase2,with aug, At fold 0, seed 26576,round 3 we find one with acc: 90.6832298136646, loss: 0.22635808156699128\n",
    "10folds, Phase2,with aug, At fold 1, seed 26576,round 3 we find one with acc: 93.7888198757764, loss: 0.19224824594414752\n",
    "10folds, Phase2,with aug, At fold 2, seed 26576,round 2 we find one with acc: 93.16770186335404, loss: 0.1994070429239214\n",
    "10folds, Phase2,with aug, At fold 3, seed 26576,round 2 we find one with acc: 94.40993788819875, loss: 0.159832681095378\n",
    "10folds, Phase2,with aug, At fold 4, seed 26576,round 2 we find one with acc: 95.625, loss: 0.1502698056399822\n",
    "10folds, Phase2,with aug, At fold 5, seed 26576,round 3 we find one with acc: 91.875, loss: 0.24664296209812164\n",
    "10folds, Phase2,with aug, At fold 6, seed 26576,round 1 we find one with acc: 96.875, loss: 0.13544742614030839\n",
    "10folds, Phase2,with aug, At fold 7, seed 26576,round 2 we find one with acc: 95.625, loss: 0.16195892915129662\n",
    "10folds, Phase2,with aug, At fold 8, seed 26576,round 1 we find one with acc: 93.75, loss: 0.1818329468369484\n",
    "10folds, Phase2,with aug, At fold 9, seed 26576,round 1 we find one with acc: 91.875, loss: 0.20526369661092758\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cnn, 10fold with noaug and no dense\n",
    "\n",
    "cv mean - 0.166565\n",
    "10folds, Phase6,with aug, At fold 0, seed 78321,round 3 we find one with acc: 97.51552795031056, loss: 0.12833880448008175\n",
    "10folds, Phase6,with aug, At fold 1, seed 78321,round 3 we find one with acc: 95.03105590062111, loss: 0.13315522735533508\n",
    "10folds, Phase6,with aug, At fold 2, seed 78321,round 1 we find one with acc: 93.16770186335404, loss: 0.17003973691452365\n",
    "10folds, Phase6,with aug, At fold 3, seed 78321,round 1 we find one with acc: 95.03105590062111, loss: 0.12178812601736613\n",
    "10folds, Phase6,with aug, At fold 4, seed 78321,round 1 we find one with acc: 92.5, loss: 0.19166728258132934\n",
    "10folds, Phase6,with aug, At fold 5, seed 78321,round 3 we find one with acc: 91.875, loss: 0.22051585614681243\n",
    "10folds, Phase6,with aug, At fold 6, seed 78321,round 2 we find one with acc: 93.75, loss: 0.16428363770246507\n",
    "10folds, Phase6,with aug, At fold 7, seed 78321,round 3 we find one with acc: 91.875, loss: 0.2238137185573578\n",
    "10folds, Phase6,with aug, At fold 8, seed 78321,round 1 we find one with acc: 94.375, loss: 0.17186198979616166\n",
    "10folds, Phase6,with aug, At fold 9, seed 78321,round 1 we find one with acc: 96.875, loss: 0.14018546640872956\n",
    "\n",
    "\n",
    "\n",
    "cv mean - 0.1600634\n",
    "10folds, Phase6,with aug, At fold 0, seed 73182,round 2 we find one with acc: 95.65217391304348, loss: 0.12256210533358296\n",
    "10folds, Phase6,with aug, At fold 1, seed 73182,round 3 we find one with acc: 91.92546583850931, loss: 0.1718225987246318\n",
    "10folds, Phase6,with aug, At fold 2, seed 73182,round 1 we find one with acc: 92.54658385093168, loss: 0.20526082295438516\n",
    "10folds, Phase6,with aug, At fold 3, seed 73182,round 1 we find one with acc: 95.65217391304348, loss: 0.11895036461375515\n",
    "10folds, Phase6,with aug, At fold 4, seed 73182,round 2 we find one with acc: 94.375, loss: 0.1674065411090851\n",
    "10folds, Phase6,with aug, At fold 5, seed 73182,round 2 we find one with acc: 96.875, loss: 0.13310225270688533\n",
    "10folds, Phase6,with aug, At fold 6, seed 73182,round 1 we find one with acc: 91.25, loss: 0.2060839593410492\n",
    "10folds, Phase6,with aug, At fold 7, seed 73182,round 1 we find one with acc: 93.125, loss: 0.16720427572727203\n",
    "10folds, Phase6,with aug, At fold 8, seed 73182,round 2 we find one with acc: 94.375, loss: 0.16059861928224564\n",
    "10folds, Phase6,with aug, At fold 9, seed 73182,round 3 we find one with acc: 95.625, loss: 0.14764260351657868\n",
    "\n",
    "\n",
    "cv mean - 0.165902\n",
    "10folds, Phase6,with aug, At fold 0, seed 78064,round 1 we find one with acc: 95.65217391304348, loss: 0.12140366106484987\n",
    "10folds, Phase6,with aug, At fold 1, seed 78064,round 3 we find one with acc: 94.40993788819875, loss: 0.14340442645808923\n",
    "10folds, Phase6,with aug, At fold 2, seed 78064,round 2 we find one with acc: 94.40993788819875, loss: 0.1506175421965048\n",
    "10folds, Phase6,with aug, At fold 3, seed 78064,round 3 we find one with acc: 92.54658385093168, loss: 0.18000449878828867\n",
    "10folds, Phase6,with aug, At fold 4, seed 78064,round 1 we find one with acc: 93.75, loss: 0.17445145100355147\n",
    "10folds, Phase6,with aug, At fold 5, seed 78064,round 2 we find one with acc: 93.125, loss: 0.16495558246970177\n",
    "10folds, Phase6,with aug, At fold 6, seed 78064,round 3 we find one with acc: 93.75, loss: 0.16144145727157594\n",
    "10folds, Phase6,with aug, At fold 7, seed 78064,round 3 we find one with acc: 93.75, loss: 0.15009843111038207\n",
    "10folds, Phase6,with aug, At fold 8, seed 78064,round 2 we find one with acc: 93.75, loss: 0.1790560483932495\n",
    "10folds, Phase6,with aug, At fold 9, seed 78064,round 2 we find one with acc: 93.125, loss: 0.23358665853738786\n",
    "\n",
    "cv mean - 0.1615705\n",
    "10folds, Phase6,with aug, At fold 0, seed 73381,round 1 we find one with acc: 95.65217391304348, loss: 0.19694542958869699\n",
    "10folds, Phase6,with aug, At fold 1, seed 73381,round 2 we find one with acc: 95.03105590062111, loss: 0.13702974810918667\n",
    "10folds, Phase6,with aug, At fold 2, seed 73381,round 1 we find one with acc: 92.54658385093168, loss: 0.1691791434650836\n",
    "10folds, Phase6,with aug, At fold 3, seed 73381,round 1 we find one with acc: 93.16770186335404, loss: 0.18530531176684065\n",
    "10folds, Phase6,with aug, At fold 4, seed 73381,round 2 we find one with acc: 90.0, loss: 0.25376694202423095\n",
    "10folds, Phase6,with aug, At fold 5, seed 73381,round 2 we find one with acc: 95.625, loss: 0.1413661628961563\n",
    "10folds, Phase6,with aug, At fold 6, seed 73381,round 3 we find one with acc: 96.25, loss: 0.12991672456264497\n",
    "10folds, Phase6,with aug, At fold 7, seed 73381,round 3 we find one with acc: 93.75, loss: 0.16675544679164886\n",
    "10folds, Phase6,with aug, At fold 8, seed 73381,round 2 we find one with acc: 98.125, loss: 0.08703880086541176\n",
    "10folds, Phase6,with aug, At fold 9, seed 73381,round 1 we find one with acc: 95.0, loss: 0.14840155839920044\n",
    "\n",
    "\n",
    "cv mean - 0.1638477\n",
    "10folds, Phase6,with aug, At fold 0, seed 77183,round 3 we find one with acc: 95.65217391304348, loss: 0.1418800802889818\n",
    "10folds, Phase6,with aug, At fold 1, seed 77183,round 2 we find one with acc: 96.27329192546584, loss: 0.1347842372907615\n",
    "10folds, Phase6,with aug, At fold 2, seed 77183,round 2 we find one with acc: 95.65217391304348, loss: 0.1367086296588738\n",
    "10folds, Phase6,with aug, At fold 3, seed 77183,round 1 we find one with acc: 96.8944099378882, loss: 0.12329475424304512\n",
    "10folds, Phase6,with aug, At fold 4, seed 77183,round 1 we find one with acc: 93.75, loss: 0.16645514070987702\n",
    "10folds, Phase6,with aug, At fold 5, seed 77183,round 3 we find one with acc: 95.0, loss: 0.19588240385055541\n",
    "10folds, Phase6,with aug, At fold 6, seed 77183,round 1 we find one with acc: 94.375, loss: 0.17654224634170532\n",
    "10folds, Phase6,with aug, At fold 7, seed 77183,round 2 we find one with acc: 92.5, loss: 0.22297025620937347\n",
    "10folds, Phase6,with aug, At fold 8, seed 77183,round 2 we find one with acc: 93.125, loss: 0.1905862122774124\n",
    "10folds, Phase6,with aug, At fold 9, seed 77183,round 1 we find one with acc: 94.375, loss: 0.1493726573884487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At round 0, fold 3, seed [4987], we find a good value with acc: 92.83489096573209, loss: 0.18720525540295418\r\n",
      "At round 0, fold 3, seed [4987], we find a good value with acc: 93.45794392523365, loss: 0.18422258188048626\r\n",
      "At round 0, fold 3, seed [4987], we find a good value with acc: 93.76947040498442, loss: 0.18761429014235642\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 93.14641744548287, loss: 0.18572754503410555\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 92.5233644859813, loss: 0.17600850301368215\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 93.76947040498442, loss: 0.17959555435774854\r\n",
      "At round 1, fold 3, seed [7386], we find a good value with acc: 92.83489096573209, loss: 0.18345521060848533\r\n",
      "At round 1, fold 4, seed [7386], we find a good value with acc: 93.125, loss: 0.19450552463531495\r\n",
      "At round 1, fold 4, seed [7386], we find a good value with acc: 92.5, loss: 0.19440045058727265\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.17605417614042573\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 92.5233644859813, loss: 0.19229212802518567\r\n",
      "At round 2, fold 1, seed [8868], we find a good value with acc: 93.45794392523365, loss: 0.17976307720410117\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 93.125, loss: 0.18329112231731415\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 92.8125, loss: 0.18368732035160065\r\n",
      "At round 2, fold 4, seed [8868], we find a good value with acc: 93.4375, loss: 0.19045546948909758\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 94.6875, loss: 0.17815355360507965\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 93.125, loss: 0.16909184455871581\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 92.5, loss: 0.1774096041917801\r\n",
      "At round 3, fold 4, seed [9915], we find a good value with acc: 95.0, loss: 0.15670025497674941\r\n",
      "At round 5, fold 4, seed [7433], we find a good value with acc: 93.4375, loss: 0.17777724266052247\r\n",
      "At round 1, fold 0, seed [7791], we find a good value with acc: 92.5233644859813, loss: 0.1843862771245178\r\n",
      "At round 1, fold 0, seed [7791], we find a good value with acc: 93.14641744548287, loss: 0.1838890655761196\r\n"
     ]
    }
   ],
   "source": [
    "! cat log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final vgg16:\n",
    "\n",
    "\n",
    "cv mean - 0.1792\n",
    "10folds, Phase3, At fold 0, seed 66318,round 2 we find one with acc: 93.7888198757764, loss: 0.18202660218899294\n",
    "10folds, Phase3, At fold 1, seed 66318,round 3 we find one with acc: 94.40993788819875, loss: 0.18795015445407132\n",
    "10folds, Phase3, At fold 2, seed 66318,round 1 we find one with acc: 95.03105590062111, loss: 0.15942718264479075\n",
    "10folds, Phase3, At fold 3, seed 66318,round 1 we find one with acc: 93.7888198757764, loss: 0.19316034842722163\n",
    "10folds, Phase3, At fold 4, seed 66318,round 1 we find one with acc: 91.25, loss: 0.23037294447422027\n",
    "10folds, Phase3, At fold 5, seed 66318,round 1 we find one with acc: 96.875, loss: 0.12658769488334656\n",
    "10folds, Phase3, At fold 6, seed 66318,round 1 we find one with acc: 96.875, loss: 0.0948967233300209\n",
    "10folds, Phase3, At fold 7, seed 66318,round 2 we find one with acc: 94.375, loss: 0.16402093470096588\n",
    "10folds, Phase3, At fold 8, seed 66318,round 3 we find one with acc: 94.375, loss: 0.18864904046058656\n",
    "10folds, Phase3, At fold 9, seed 66318,round 1 we find one with acc: 90.0, loss: 0.2649789094924927\n",
    "\n",
    "\n",
    "\n",
    "cv mean - 0.17396\n",
    "\n",
    "10folds, Phase3, At fold 0, seed 66568,round 3 we find one with acc: 93.7888198757764, loss: 0.18486198856963876\n",
    "10folds, Phase3, At fold 1, seed 66568,round 1 we find one with acc: 96.27329192546584, loss: 0.13517982119358846\n",
    "10folds, Phase3, At fold 2, seed 66568,round 2 we find one with acc: 91.92546583850931, loss: 0.19487843551435827\n",
    "10folds, Phase3, At fold 3, seed 66568,round 2 we find one with acc: 96.8944099378882, loss: 0.11909743643695524\n",
    "10folds, Phase3, At fold 4, seed 66568,round 1 we find one with acc: 93.125, loss: 0.2188066780567169\n",
    "10folds, Phase3, At fold 5, seed 66568,round 1 we find one with acc: 95.0, loss: 0.1428483471274376\n",
    "10folds, Phase3, At fold 6, seed 66568,round 1 we find one with acc: 95.0, loss: 0.1781529664993286\n",
    "10folds, Phase3, At fold 7, seed 66568,round 2 we find one with acc: 93.75, loss: 0.2242947995662689\n",
    "10folds, Phase3, At fold 8, seed 66568,round 1 we find one with acc: 96.25, loss: 0.14551309645175933\n",
    "10folds, Phase3, At fold 9, seed 66568,round 3 we find one with acc: 95.0, loss: 0.1959661602973938\n",
    "\n",
    "\n",
    "\n",
    "cv mean - 0.1765154\n",
    "10folds, Phase3, At fold 0, seed 64595,round 1 we find one with acc: 93.16770186335404, loss: 0.19256400897728732\n",
    "10folds, Phase3, At fold 1, seed 64595,round 1 we find one with acc: 95.03105590062111, loss: 0.1546385890763739\n",
    "10folds, Phase3, At fold 2, seed 64595,round 2 we find one with acc: 94.40993788819875, loss: 0.16077712400359398\n",
    "10folds, Phase3, At fold 3, seed 64595,round 1 we find one with acc: 95.65217391304348, loss: 0.1518434456879308\n",
    "10folds, Phase3, At fold 4, seed 64595,round 2 we find one with acc: 95.0, loss: 0.152125084400177\n",
    "10folds, Phase3, At fold 5, seed 64595,round 2 we find one with acc: 96.25, loss: 0.1252831369638443\n",
    "10folds, Phase3, At fold 6, seed 64595,round 1 we find one with acc: 89.375, loss: 0.2584298402070999\n",
    "10folds, Phase3, At fold 7, seed 64595,round 1 we find one with acc: 95.0, loss: 0.14254881143569947\n",
    "10folds, Phase3, At fold 8, seed 64595,round 2 we find one with acc: 95.0, loss: 0.16844869554042816\n",
    "10folds, Phase3, At fold 9, seed 64595,round 2 we find one with acc: 91.875, loss: 0.2584953665733337\n",
    "\n",
    "\n",
    "cv mean - 0.1780716\n",
    "10folds, Phase3, At fold 0, seed 66766,round 1 we find one with acc: 96.27329192546584, loss: 0.15913005390707752\n",
    "10folds, Phase3, At fold 1, seed 66766,round 1 we find one with acc: 94.40993788819875, loss: 0.13372691648994914\n",
    "10folds, Phase3, At fold 2, seed 66766,round 2 we find one with acc: 96.8944099378882, loss: 0.1284155405548789\n",
    "10folds, Phase3, At fold 3, seed 66766,round 2 we find one with acc: 91.30434782608695, loss: 0.2396365043169223\n",
    "10folds, Phase3, At fold 4, seed 66766,round 3 we find one with acc: 91.875, loss: 0.2216850608587265\n",
    "10folds, Phase3, At fold 5, seed 66766,round 2 we find one with acc: 96.25, loss: 0.14154378920793534\n",
    "10folds, Phase3, At fold 6, seed 66766,round 3 we find one with acc: 95.625, loss: 0.16289297193288804\n",
    "10folds, Phase3, At fold 7, seed 66766,round 2 we find one with acc: 91.25, loss: 0.20748940706253052\n",
    "10folds, Phase3, At fold 8, seed 66766,round 3 we find one with acc: 92.5, loss: 0.19332784414291382\n",
    "10folds, Phase3, At fold 9, seed 66766,round 3 we find one with acc: 95.0, loss: 0.19286755174398423\n",
    "\n",
    "cv mean - 0.1755483\n",
    "10folds, Phase3, At fold 0, seed 66127,round 2 we find one with acc: 94.40993788819875, loss: 0.1805281937122345\n",
    "10folds, Phase3, At fold 1, seed 66127,round 3 we find one with acc: 95.03105590062111, loss: 0.1524758154746168\n",
    "10folds, Phase3, At fold 2, seed 66127,round 3 we find one with acc: 93.7888198757764, loss: 0.18358832597732544\n",
    "10folds, Phase3, At fold 3, seed 66127,round 1 we find one with acc: 91.92546583850931, loss: 0.21925982481204204\n",
    "10folds, Phase3, At fold 4, seed 66127,round 2 we find one with acc: 93.125, loss: 0.178997765481472\n",
    "10folds, Phase3, At fold 5, seed 66127,round 2 we find one with acc: 93.125, loss: 0.20205674916505814\n",
    "10folds, Phase3, At fold 6, seed 66127,round 2 we find one with acc: 93.75, loss: 0.15578339695930482\n",
    "10folds, Phase3, At fold 7, seed 66127,round 2 we find one with acc: 92.5, loss: 0.19390811920166015\n",
    "10folds, Phase3, At fold 8, seed 66127,round 2 we find one with acc: 95.625, loss: 0.15662985146045685\n",
    "10folds, Phase3, At fold 9, seed 66127,round 1 we find one with acc: 96.25, loss: 0.132255420088768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! cat plain_cnn_models/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At fold 0, seed [10226],round 1 we find one with acc: 92.20338983050847, loss: 0.20852247148247088\r\n",
      "At fold 0, seed [10226],round 2 we find one with acc: 92.20338983050847, loss: 0.22576709382109722\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [10226],round 1 we find one with acc: 92.85714285714286, loss: 0.212315110020897\r\n",
      "At fold 1, seed [10226],round 2 we find one with acc: 93.19727891156462, loss: 0.21693102508580603\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [10226],round 1 we find one with acc: 91.49659863945578, loss: 0.2521475723811558\r\n",
      "At fold 2, seed [10226],round 2 we find one with acc: 92.17687074829932, loss: 0.24867962696114365\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [10226],round 1 we find one with acc: 95.91836734693878, loss: 0.14599180497786626\r\n",
      "At fold 3, seed [10226],round 2 we find one with acc: 95.91836734693878, loss: 0.1518747324415413\r\n",
      "At fold 3, seed [10226],round 3 we find one with acc: 94.89795918367346, loss: 0.15680616997739896\r\n",
      "At fold 4, seed [10226],round 1 we find one with acc: 93.19727891156462, loss: 100\r\n",
      "At fold 4, seed [10226],round 2 we find one with acc: 94.21768707482993, loss: 0.16818172849562704\r\n",
      "At fold 4, seed [10226],round 3 we find one with acc: 92.51700680272108, loss: 100\r\n",
      "At fold 0, seed [12999],round 1 we find one with acc: 93.55932203389831, loss: 0.21050052410465175\r\n",
      "At fold 0, seed [12999],round 2 we find one with acc: 90.84745762711864, loss: 100\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [12999],round 1 we find one with acc: 94.89795918367346, loss: 0.19402903470457816\r\n",
      "At fold 1, seed [12999],round 2 we find one with acc: 94.89795918367346, loss: 0.18548728865222866\r\n",
      "At fold 2, seed [12999],round 1 we find one with acc: 92.51700680272108, loss: 0.20708429306542792\r\n",
      "At fold 2, seed [12999],round 2 we find one with acc: 92.85714285714286, loss: 0.20986271188372657\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [12999],round 1 we find one with acc: 93.19727891156462, loss: 0.19991505855605715\r\n",
      "At fold 3, seed [12999],round 2 we find one with acc: 93.19727891156462, loss: 0.2098930901816102\r\n",
      "We are going to give up this round\r\n",
      "At fold 4, seed [12999],round 1 we find one with acc: 92.51700680272108, loss: 0.20366009912726019\r\n",
      "At fold 4, seed [12999],round 2 we find one with acc: 92.85714285714286, loss: 0.21807357212718653\r\n",
      "We are going to give up this round\r\n",
      "At fold 0, seed [18351],round 1 we find one with acc: 93.22033898305085, loss: 0.21538969057343774\r\n",
      "At fold 0, seed [18351],round 2 we find one with acc: 92.88135593220339, loss: 0.2221528811474978\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [18351],round 1 we find one with acc: 95.23809523809524, loss: 0.1443347283163849\r\n",
      "At fold 1, seed [18351],round 2 we find one with acc: 94.21768707482993, loss: 0.27243054805158756\r\n",
      "At fold 1, seed [18351],round 3 we find one with acc: 95.578231292517, loss: 0.14973348683240462\r\n",
      "At fold 2, seed [18351],round 1 we find one with acc: 91.83673469387755, loss: 0.24982292696732242\r\n",
      "At fold 2, seed [18351],round 2 we find one with acc: 92.17687074829932, loss: 0.2510375165614952\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [18351],round 1 we find one with acc: 93.5374149659864, loss: 0.17971686588055422\r\n",
      "At fold 3, seed [18351],round 2 we find one with acc: 93.19727891156462, loss: 0.20030122843323922\r\n",
      "At fold 3, seed [18351],round 3 we find one with acc: 92.17687074829932, loss: 0.208451639987579\r\n",
      "At fold 4, seed [18351],round 1 we find one with acc: 94.21768707482993, loss: 0.18366209331418382\r\n",
      "At fold 4, seed [18351],round 2 we find one with acc: 94.21768707482993, loss: 0.1842780665678232\r\n",
      "At fold 0, seed [12618],round 1 we find one with acc: 94.57627118644068, loss: 0.16903007970017903\r\n",
      "At fold 0, seed [12618],round 2 we find one with acc: 94.57627118644068, loss: 0.17128876323922207\r\n",
      "At fold 0, seed [12618],round 3 we find one with acc: 94.23728813559322, loss: 0.17655538664025774\r\n",
      "At fold 1, seed [12618],round 1 we find one with acc: 92.17687074829932, loss: 0.24410214736348107\r\n",
      "At fold 1, seed [12618],round 2 we find one with acc: 92.51700680272108, loss: 0.23244065076720957\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [12618],round 1 we find one with acc: 92.17687074829932, loss: 0.19889270032749695\r\n",
      "At fold 2, seed [12618],round 2 we find one with acc: 91.15646258503402, loss: 0.5686699188485438\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [12618],round 1 we find one with acc: 93.19727891156462, loss: 0.21964188680356864\r\n",
      "At fold 3, seed [12618],round 2 we find one with acc: 92.17687074829932, loss: 0.2194356652749639\r\n",
      "We are going to give up this round\r\n",
      "At fold 4, seed [12618],round 1 we find one with acc: 93.87755102040816, loss: 0.18431940573413355\r\n",
      "At fold 4, seed [12618],round 2 we find one with acc: 93.87755102040816, loss: 0.18851046132392624\r\n",
      "At fold 0, seed [10065],round 1 we find one with acc: 95.9322033898305, loss: 0.1564734804933354\r\n",
      "At fold 0, seed [10065],round 2 we find one with acc: 95.2542372881356, loss: 0.13990531280889348\r\n",
      "At fold 0, seed [10065],round 3 we find one with acc: 93.22033898305085, loss: 100\r\n",
      "At fold 1, seed [10065],round 1 we find one with acc: 93.19727891156462, loss: 0.21384957919315417\r\n",
      "At fold 1, seed [10065],round 2 we find one with acc: 93.19727891156462, loss: 0.2090544726978354\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [10065],round 1 we find one with acc: 94.89795918367346, loss: 0.18185917607375554\r\n",
      "At fold 2, seed [10065],round 2 we find one with acc: 92.85714285714286, loss: 0.1898043747661876\r\n",
      "At fold 3, seed [10065],round 1 we find one with acc: 90.81632653061224, loss: 0.2749191885497294\r\n",
      "At fold 3, seed [10065],round 2 we find one with acc: 91.15646258503402, loss: 0.24642901375991147\r\n",
      "We are going to give up this round\r\n",
      "At fold 4, seed [10065],round 1 we find one with acc: 92.85714285714286, loss: 0.16707331818990967\r\n",
      "At fold 4, seed [10065],round 2 we find one with acc: 93.5374149659864, loss: 0.1687004917941126\r\n",
      "At fold 4, seed [10065],round 3 we find one with acc: 94.21768707482993, loss: 0.16374690663449618\r\n",
      "At fold 0, seed [14653],round 1 we find one with acc: 94.91525423728814, loss: 0.33263416583255184\r\n",
      "At fold 0, seed [14653],round 2 we find one with acc: 94.57627118644068, loss: 0.16583899562641727\r\n",
      "At fold 0, seed [14653],round 3 we find one with acc: 94.91525423728814, loss: 0.15615026854862601\r\n",
      "At fold 1, seed [14653],round 1 we find one with acc: 92.17687074829932, loss: 0.23531836899770361\r\n",
      "At fold 1, seed [14653],round 2 we find one with acc: 92.17687074829932, loss: 0.23063994021642775\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [14653],round 1 we find one with acc: 91.15646258503402, loss: 0.24923188335636035\r\n",
      "At fold 2, seed [14653],round 2 we find one with acc: 90.47619047619048, loss: 0.26996394008600794\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [14653],round 1 we find one with acc: 94.89795918367346, loss: 0.13210602018500672\r\n",
      "At fold 3, seed [14653],round 2 we find one with acc: 95.578231292517, loss: 0.14009496848396705\r\n",
      "At fold 3, seed [14653],round 3 we find one with acc: 95.23809523809524, loss: 0.7902267250884958\r\n",
      "At fold 4, seed [14653],round 1 we find one with acc: 93.87755102040816, loss: 0.21331719699360074\r\n",
      "At fold 4, seed [14653],round 2 we find one with acc: 92.85714285714286, loss: 0.20360938292376848\r\n",
      "We are going to give up this round\r\n",
      "At fold 0, seed [12590],round 1 we find one with acc: 89.15254237288136, loss: 0.26214922297809085\r\n",
      "At fold 0, seed [12590],round 2 we find one with acc: 90.84745762711864, loss: 0.2669966722443952\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [12590],round 1 we find one with acc: 93.19727891156462, loss: 0.21223109149608482\r\n",
      "At fold 1, seed [12590],round 2 we find one with acc: 93.5374149659864, loss: 0.19794999944920444\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [12590],round 1 we find one with acc: 95.91836734693878, loss: 0.13085028254205272\r\n",
      "At fold 2, seed [12590],round 2 we find one with acc: 95.23809523809524, loss: 0.154161617058475\r\n",
      "At fold 2, seed [12590],round 3 we find one with acc: 95.578231292517, loss: 0.15407232952868047\r\n",
      "At fold 3, seed [12590],round 1 we find one with acc: 93.19727891156462, loss: 0.1963571060676964\r\n",
      "At fold 3, seed [12590],round 2 we find one with acc: 93.87755102040816, loss: 0.17884244004479882\r\n",
      "At fold 3, seed [12590],round 3 we find one with acc: 93.5374149659864, loss: 0.19016650937446933\r\n",
      "At fold 4, seed [12590],round 1 we find one with acc: 93.19727891156462, loss: 0.19328935150386525\r\n",
      "At fold 4, seed [12590],round 2 we find one with acc: 93.5374149659864, loss: 0.20135659982963483\r\n",
      "At fold 0, seed [11359],round 1 we find one with acc: 93.22033898305085, loss: 0.20975535744327609\r\n",
      "At fold 0, seed [11359],round 2 we find one with acc: 93.55932203389831, loss: 0.20627650623604402\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [11359],round 1 we find one with acc: 93.5374149659864, loss: 0.2055313732145595\r\n",
      "At fold 1, seed [11359],round 2 we find one with acc: 93.5374149659864, loss: 0.206689847493861\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [11359],round 1 we find one with acc: 92.85714285714286, loss: 0.21703838551936505\r\n",
      "At fold 2, seed [11359],round 2 we find one with acc: 92.85714285714286, loss: 0.20640345596942772\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [11359],round 1 we find one with acc: 94.21768707482993, loss: 100\r\n",
      "At fold 3, seed [11359],round 2 we find one with acc: 93.87755102040816, loss: 0.1680060422339407\r\n",
      "At fold 3, seed [11359],round 3 we find one with acc: 93.87755102040816, loss: 0.16235200229550706\r\n",
      "At fold 4, seed [11359],round 1 we find one with acc: 91.49659863945578, loss: 0.23318451044916297\r\n",
      "At fold 4, seed [11359],round 2 we find one with acc: 92.17687074829932, loss: 0.232550368726659\r\n",
      "We are going to give up this round\r\n",
      "At fold 0, seed [10146],round 1 we find one with acc: 92.54237288135593, loss: 0.18585776413901378\r\n",
      "At fold 0, seed [10146],round 2 we find one with acc: 94.23728813559322, loss: 0.1483704738697763\r\n",
      "At fold 0, seed [10146],round 3 we find one with acc: 93.55932203389831, loss: 0.16005539394031137\r\n",
      "At fold 1, seed [10146],round 1 we find one with acc: 94.21768707482993, loss: 0.1728037788348944\r\n",
      "At fold 1, seed [10146],round 2 we find one with acc: 93.19727891156462, loss: 0.18480514152114894\r\n",
      "At fold 1, seed [10146],round 3 we find one with acc: 93.5374149659864, loss: 0.18441093860029364\r\n",
      "At fold 2, seed [10146],round 1 we find one with acc: 93.19727891156462, loss: 0.20104691632041316\r\n",
      "At fold 2, seed [10146],round 2 we find one with acc: 93.19727891156462, loss: 0.20212454784686873\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [10146],round 1 we find one with acc: 91.83673469387755, loss: 0.22704859088067295\r\n",
      "At fold 3, seed [10146],round 2 we find one with acc: 93.19727891156462, loss: 0.23176834129151844\r\n",
      "We are going to give up this round\r\n",
      "At fold 4, seed [10146],round 1 we find one with acc: 93.87755102040816, loss: 0.21703883731851772\r\n",
      "At fold 4, seed [10146],round 2 we find one with acc: 93.87755102040816, loss: 0.20383583957037957\r\n",
      "We are going to give up this round\r\n",
      "At fold 0, seed [11594],round 1 we find one with acc: 92.88135593220339, loss: 0.23525163708096844\r\n",
      "At fold 0, seed [11594],round 2 we find one with acc: 91.52542372881356, loss: 0.23989092621762872\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [11594],round 1 we find one with acc: 94.21768707482993, loss: 0.20226322793636192\r\n",
      "At fold 1, seed [11594],round 2 we find one with acc: 94.5578231292517, loss: 0.17625289689115928\r\n",
      "At fold 1, seed [11594],round 3 we find one with acc: 94.5578231292517, loss: 0.18433465783287878\r\n",
      "At fold 2, seed [11594],round 1 we find one with acc: 93.5374149659864, loss: 0.1664510878930692\r\n",
      "At fold 2, seed [11594],round 2 we find one with acc: 93.5374149659864, loss: 0.1688727048181352\r\n",
      "At fold 2, seed [11594],round 3 we find one with acc: 92.85714285714286, loss: 0.1852557207552754\r\n",
      "At fold 3, seed [11594],round 1 we find one with acc: 92.85714285714286, loss: 0.2110604721875418\r\n",
      "At fold 3, seed [11594],round 2 we find one with acc: 93.5374149659864, loss: 0.22940468240757378\r\n",
      "We are going to give up this round\r\n",
      "At fold 4, seed [11594],round 1 we find one with acc: 93.19727891156462, loss: 0.18007444843751233\r\n",
      "At fold 4, seed [11594],round 2 we find one with acc: 93.87755102040816, loss: 0.2041512630423721\r\n",
      "At fold 0, seed [11999],round 1 we find one with acc: 91.86440677966101, loss: 0.23753390889299117\r\n",
      "At fold 0, seed [11999],round 2 we find one with acc: 91.1864406779661, loss: 0.24849258297580784\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [11999],round 1 we find one with acc: 91.49659863945578, loss: 0.21692536882802743\r\n",
      "At fold 1, seed [11999],round 2 we find one with acc: 91.15646258503402, loss: 0.22179822106750643\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [11999],round 1 we find one with acc: 94.21768707482993, loss: 0.17326597899806742\r\n",
      "At fold 2, seed [11999],round 2 we find one with acc: 94.21768707482993, loss: 0.1686980381709378\r\n",
      "At fold 2, seed [11999],round 3 we find one with acc: 93.5374149659864, loss: 0.17321149092547747\r\n",
      "At fold 3, seed [11999],round 1 we find one with acc: 95.23809523809524, loss: 0.1601234181549679\r\n",
      "At fold 3, seed [11999],round 2 we find one with acc: 96.59863945578232, loss: 0.13826261318865277\r\n",
      "At fold 3, seed [11999],round 3 we find one with acc: 96.25850340136054, loss: 0.15552468276044137\r\n",
      "At fold 4, seed [11999],round 1 we find one with acc: 94.21768707482993, loss: 0.21074317203087062\r\n",
      "At fold 4, seed [11999],round 2 we find one with acc: 94.5578231292517, loss: 0.20935553677228033\r\n",
      "We are going to give up this round\r\n",
      "At fold 0, seed [12022],round 1 we find one with acc: 92.88135593220339, loss: 0.21416487759452754\r\n",
      "At fold 0, seed [12022],round 2 we find one with acc: 92.88135593220339, loss: 0.20245921394582522\r\n",
      "We are going to give up this round\r\n",
      "At fold 1, seed [12022],round 1 we find one with acc: 94.21768707482993, loss: 0.2311188280582428\r\n",
      "At fold 1, seed [12022],round 2 we find one with acc: 93.5374149659864, loss: 0.2305563507842369\r\n",
      "We are going to give up this round\r\n",
      "At fold 2, seed [12022],round 1 we find one with acc: 92.85714285714286, loss: 0.2408237196758491\r\n",
      "At fold 2, seed [12022],round 2 we find one with acc: 92.17687074829932, loss: 0.2297836878672749\r\n",
      "We are going to give up this round\r\n",
      "At fold 3, seed [12022],round 1 we find one with acc: 95.91836734693878, loss: 0.15999178522500862\r\n",
      "At fold 3, seed [12022],round 2 we find one with acc: 95.23809523809524, loss: 0.1838024383550193\r\n",
      "At fold 3, seed [12022],round 3 we find one with acc: 95.578231292517, loss: 0.16492463159216506\r\n",
      "At fold 4, seed [12022],round 1 we find one with acc: 92.85714285714286, loss: 100\r\n",
      "At fold 4, seed [12022],round 2 we find one with acc: 92.85714285714286, loss: 0.3108378683223205\r\n",
      "We are going to give up this round\r\n",
      "\r\n",
      "Phase2, At fold 0, seed [11655],round 1 we find one with acc: 90.50847457627118, loss: 0.23276670307426128\r\n",
      "Phase2, At fold 0, seed [11655],round 2 we find one with acc: 92.54237288135593, loss: 0.24154692956451643\r\n",
      "Phase2, At fold 1, seed [11655],round 1 we find one with acc: 94.89795918367346, loss: 0.19842949276473246\r\n",
      "Phase2, At fold 1, seed [11655],round 2 we find one with acc: 95.23809523809524, loss: 0.190057825474512\r\n",
      "Phase2, At fold 2, seed [11655],round 1 we find one with acc: 92.51700680272108, loss: 0.19441758653744548\r\n",
      "Phase2, At fold 2, seed [11655],round 2 we find one with acc: 92.17687074829932, loss: 0.22204963183727394\r\n",
      "Phase2, At fold 3, seed [11655],round 1 we find one with acc: 92.51700680272108, loss: 0.21148873612183292\r\n",
      "Phase2, At fold 3, seed [11655],round 2 we find one with acc: 92.17687074829932, loss: 0.21181264093944005\r\n",
      "Phase2, At fold 4, seed [11655],round 1 we find one with acc: 94.5578231292517, loss: 0.13913460189894755\r\n",
      "Phase2, At fold 4, seed [11655],round 2 we find one with acc: 95.91836734693878, loss: 0.1518320623709231\r\n",
      "Phase2, At fold 4, seed [11655],round 3 we find one with acc: 94.89795918367346, loss: 0.16037461042505544\r\n"
     ]
    }
   ],
   "source": [
    "! cat fcn_models/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase2, At fold 0, seed 5555,round 1 we find one with acc: 94.57627118644068, loss: 0.17831873344415325\r\n",
      "Phase2, At fold 0, seed 5555,round 1 we find one with acc: 94.91525423728814, loss: 0.18487334703489886\r\n",
      "Phase2, At fold 1, seed 5555,round 1 we find one with acc: 90.81632653061224, loss: 0.2602530278924371\r\n",
      "Phase2, At fold 2, seed 5555,round 1 we find one with acc: 94.5578231292517, loss: 0.16718507228659935\r\n",
      "Phase2, At fold 3, seed 5555,round 1 we find one with acc: 93.19727891156462, loss: 0.2063273817909007\r\n",
      "Phase2, At fold 4, seed 5555,round 1 we find one with acc: 91.15646258503402, loss: 0.2500467705888813\r\n"
     ]
    }
   ],
   "source": [
    "! cat test_fcn_models/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase1, At fold 0, seed [33864],round 1 we find one with acc: 90.84745762711864, loss: 0.23743419323937368\r\n",
      "Phase1, At fold 0, seed [33864],round 2 we find one with acc: 91.86440677966101, loss: 0.20961492940530937\r\n",
      "Phase1, At fold 1, seed 33864,round 1 we find one with acc: 89.79591836734694, loss: 0.27762671159643704\r\n",
      "Phase1, At fold 1, seed 33864,round 2 we find one with acc: 88.77551020408163, loss: 0.28265695821265785\r\n",
      "Phase1, At fold 2, seed 33864,round 1 we find one with acc: 94.89795918367346, loss: 0.16098375871878903\r\n",
      "Phase1, At fold 2, seed 33864,round 2 we find one with acc: 94.5578231292517, loss: 0.15605350918307595\r\n",
      "Phase1, At fold 3, seed 33864,round 1 we find one with acc: 91.15646258503402, loss: 0.24878086889682172\r\n",
      "Phase1, At fold 3, seed 33864,round 2 we find one with acc: 89.45578231292517, loss: 0.271390911673202\r\n",
      "Phase1, At fold 4, seed 33864,round 1 we find one with acc: 89.79591836734694, loss: 0.25106191554037083\r\n",
      "Phase1, At fold 4, seed 33864,round 2 we find one with acc: 90.81632653061224, loss: 0.21282833682841995\r\n"
     ]
    }
   ],
   "source": [
    "! cat incep_models/log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5th Jan\n",
    "\n",
    "senet   \n",
    "densenet  \n",
    "cnn mean  \n",
    "fcn mean   \n",
    "resnet34 mean  \n",
    "resnet50 0.163 LB   \n",
    "inception v-3 median 0.1625 LB  \n",
    "vgg16 median 0.1650  \n",
    "vgg19 minmax 0.2,0.8, LB 0.1499  \n",
    "0.4,0.6 minmax **LB 0.1271**\n",
    "\n",
    "\n",
    "improve the inception:\n",
    "0.1, 0.9 **LB 0.1341**\n",
    "0.4, 0.6 **LB 0.1246**\n",
    "\n",
    "all the models so far\n",
    "0.4, 0.6 **LB 0.1298** (LOL, this is what I have expected)\n",
    "\n",
    "improve vgg19\n",
    "actually it doesn't change so much \n",
    "0.4, 0.6 **LB 0.1255**\n",
    "0.5, 0.5 **LB 0.1335**\n",
    "\n",
    "improve vgg16\n",
    "0.4, 0.6 **LB 0.1243**\n",
    "\n",
    "+my gbm because it's low correlation\n",
    "0.4, 0.6 **LB 0.1226**\n",
    "0.5, 0.5 **LB 0.1195**\n",
    "\n",
    "improve cnn\n",
    "0.5, 0.5 **LB 0.1166**\n",
    "\n",
    "improve resnet34\n",
    "0.5, 0.5 **LB 0.1169** as I expected.\n",
    "\n",
    "\n",
    "low, high\n",
    "0.2, 0.8 cutoff  get done tmr\n",
    "0.3, 0.7 cutoff  get done tmr\n",
    "0.4, 0.6 **LB 0.1199**\n",
    "0.5, 0.5 **LB 0.1166**\n",
    "0.55,0.45 **LB 0.1183**\n",
    "0.6，0.4 **LB 0.1142**\n",
    "0.7, 0.3 **LB 0.1381**\n",
    "\n",
    "\n",
    "0.4,0.6 push to 1 and 0, **LB 0.2018** means we are getting wrong for some of them.\n",
    "\n",
    "0.5, 0.5 best base **LB 0.1169**\n",
    "\n",
    "#### 14 Jan\n",
    "after 11 models,\n",
    "\n",
    "low, high\n",
    "0.6, 0.4 cutoff **LB 0.1112**\n",
    "0.5, 0.5 cutoff **LB 0.1151**\n",
    "\n",
    "\n",
    "low, high\n",
    "0.1, 0.9 cutoff push to 1, **LB 0.1280**\n",
    "0.2, 0.8 cutoff push to 1, **LB 0.1718**\n",
    "0.3, 0.7 cutoff push to 1, **LB 0.1668** Seems like there is one wrongness in the middle.\n",
    "\n",
    "don't push the one just come in after 0.2,0.8) but wrong conduct here LOL 18.6173\n",
    "0.3, 0.7 cutoff except (all come in when 0.2, 0.8 cutoff) push to 1, **LB 0.1230**\n",
    "\n",
    "\n",
    "0.2, 0.8 not set median, but max **LB 0.1214**\n",
    "0.3, 0.7 cutoff except (all come in when 0.2, 0.8 cutoff) push to 1,over 0.995 or below 0.005, **LB 0.1188**\n",
    "I just think logloss decrease by 0.0025, which is 1/2 log(0.005), so at least half is in this region?\n",
    "\n",
    "\n",
    "0.4, 0.6 cutoff except (all come in when 0.2, 0.8 cutoff) push to 1 **LB 0.2060** (so two wrongness between 0.6-0.7)\n",
    "0.55, 0.45 cutoff except (all come in when 0.2, 0.8 cutoff) push to 1 **0.2937**(so two wrongness between 0.45-0.6)\n",
    "\n",
    "\n",
    "10 models again(except my own resnet34):\n",
    "0.5,  0.5  cutoff **LB 0.1140**\n",
    "0.55, 0.45 cutoff **LB 0.1106**\n",
    "0.6,  0.4  cutoff **LB 0.1094**\n",
    "0.65, 0.35 cutoff **LB 0.1276**\n",
    "\n",
    "9 models (except my own resnet50 and fcn):\n",
    "0.5, 0.5 cutoff **LB 0.1140**\n",
    "0.6, 0.4 cutoff **LB 0.1101** (Don't remember exactly)\n",
    "\n",
    "11 models, add new one.\n",
    "0.55, 0.45 cutoff **LB 0.1090**\n",
    "using leakage: 0.55, 0.45 cutoff **LB 0.1090** to **LB 0.1028**(I might use it as final sub)  \n",
    "using leakage: 0.55, 0.45 cutoff only to 7 **LB 0.1030**\n",
    "\n",
    "sing leakage: 0.50, 0.50 cutoff **LB 0.1049**(I might use it as final sub)  \n",
    "sing leakage, leakage to one : 0.50, 0.50 cutoff  **LB 0.1048** (no wrongness at least appears)  \n",
    "\n",
    "\n",
    "\n",
    "add resnet34, 12 models : **LB 0.1107**\n",
    "add orifcn,   13 models : **LB 0.1189**\n",
    "### Different approach \n",
    "if 9 or more models in, then we see say are good.\n",
    "0.2, 0.8 cutoff **LB 0.1247**\n",
    "0.3, 0.7 cutoff **LB 0.1238**\n",
    "\n",
    "\n",
    "0.8, 0.2 pushto 1 and 0 cutoff **LB 0.1694**  which means I got wrong prediction here.\n",
    "So there are wrongness here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
