{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau,StepLR\n",
    "#torch.multiprocessing.set_start_method(\"spawn\")\n",
    "import vgg_fcn\n",
    "import vgg\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import progress_bar\n",
    "from skimage import transform as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_dir = 'data/processed/'\n",
    "\n",
    "train = pd.read_json(BASE_dir + 'train.json')\n",
    "#test = pd.read_json(BASE_dir + 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>iso1</th>\n",
       "      <th>iso2</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-27.878360999999998, -27.15416, -28.668615, -...</td>\n",
       "      <td>[-27.154118, -29.537888, -31.0306, -32.190483,...</td>\n",
       "      <td>dfd5f913</td>\n",
       "      <td>43.9239</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>64</td>\n",
       "      <td>57</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-12.242375, -14.920304999999999, -14.920363, ...</td>\n",
       "      <td>[-31.506321, -27.984554, -26.645678, -23.76760...</td>\n",
       "      <td>e25388fd</td>\n",
       "      <td>38.1562</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>17</td>\n",
       "      <td>92</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-24.603676, -24.603714, -24.871029, -23.15277...</td>\n",
       "      <td>[-24.870956, -24.092632, -20.653963, -19.41104...</td>\n",
       "      <td>58b2aaa0</td>\n",
       "      <td>45.2859</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -19.411043, -0.0, -0.0, -2...</td>\n",
       "      <td>91</td>\n",
       "      <td>60</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-22.454607, -23.082819, -23.998013, -23.99805...</td>\n",
       "      <td>[-27.889421, -27.519794, -27.165262, -29.10350...</td>\n",
       "      <td>4cfc3a18</td>\n",
       "      <td>43.8306</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-26.006956, -23.164886, -23.164886, -26.89116...</td>\n",
       "      <td>[-27.206915, -30.259186, -30.259186, -23.16495...</td>\n",
       "      <td>271f93f4</td>\n",
       "      <td>35.6256</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>68</td>\n",
       "      <td>44</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-20.769371, -20.769434, -25.906025, -25.90602...</td>\n",
       "      <td>[-29.288746, -29.712593, -28.884804, -28.88480...</td>\n",
       "      <td>b51d18b5</td>\n",
       "      <td>36.9034</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>120</td>\n",
       "      <td>153</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-26.673811, -23.666162, -27.622442, -28.31768...</td>\n",
       "      <td>[-24.557735, -26.97868, -27.622442, -29.073456...</td>\n",
       "      <td>31da1a04</td>\n",
       "      <td>34.4751</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -21.9427...</td>\n",
       "      <td>133</td>\n",
       "      <td>74</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-24.989119, -27.755224, -25.817074, -24.98927...</td>\n",
       "      <td>[-27.755173, -26.732174, -28.124943, -31.83772...</td>\n",
       "      <td>56929c16</td>\n",
       "      <td>41.1769</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>111</td>\n",
       "      <td>53</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-17.146641, -17.146572, -17.994583, -19.44553...</td>\n",
       "      <td>[-25.733608, -24.472507, -24.710424, -22.77215...</td>\n",
       "      <td>525ab75c</td>\n",
       "      <td>35.7829</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>8</td>\n",
       "      <td>116</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-24.020853, -23.551275, -27.18819, -29.126434...</td>\n",
       "      <td>[-28.702518, -33.563324, -29.571918, -29.12643...</td>\n",
       "      <td>192f56eb</td>\n",
       "      <td>43.3007</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-21.397552, -19.753859, -23.426783, -24.65221...</td>\n",
       "      <td>[-26.72291, -27.418192, -27.787899, -25.774536...</td>\n",
       "      <td>3aac67cd</td>\n",
       "      <td>44.624</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>110</td>\n",
       "      <td>65</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-22.020813, -22.020864, -20.345379, -18.07829...</td>\n",
       "      <td>[-29.018383, -26.519661, -26.214916, -27.16346...</td>\n",
       "      <td>161a6860</td>\n",
       "      <td>39.5067</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -18.078295, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[-21.112206, -21.638832, -25.436468, -23.22255...</td>\n",
       "      <td>[-27.30481, -28.415202999999998, -24.634125, -...</td>\n",
       "      <td>3c794f0c</td>\n",
       "      <td>41.8544</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>187</td>\n",
       "      <td>260</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[-23.864258, -27.755791, -26.047226, -24.62014...</td>\n",
       "      <td>[-23.626272, -24.620068, -28.546, -26.363146, ...</td>\n",
       "      <td>86730f0d</td>\n",
       "      <td>45.2909</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[-20.558899, -21.328135, -19.585205, -19.71788...</td>\n",
       "      <td>[-29.127485, -30.40094, -28.741528, -24.380484...</td>\n",
       "      <td>e356f7a3</td>\n",
       "      <td>34.7715</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>108</td>\n",
       "      <td>83</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[-28.282215, -27.896156, -25.882795, -25.88279...</td>\n",
       "      <td>[-31.608845, -29.110111, -32.851887, -32.85188...</td>\n",
       "      <td>87592c38</td>\n",
       "      <td>43.782</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>73</td>\n",
       "      <td>56</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[-21.345695, -21.345734, -21.166676, -20.65056...</td>\n",
       "      <td>[-26.343246, -25.143326, -23.374924, -22.92943...</td>\n",
       "      <td>1c18a39e</td>\n",
       "      <td>45.3568</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>97</td>\n",
       "      <td>49</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[-19.261189, -19.812914, -23.263891, -25.41662...</td>\n",
       "      <td>[-25.149176, -26.271551, -27.560766, -27.91539...</td>\n",
       "      <td>a210f335</td>\n",
       "      <td>38.7812</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>42</td>\n",
       "      <td>83</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[-25.950783, -26.571512, -22.943153, -21.94372...</td>\n",
       "      <td>[-29.623671, -30.093336, -27.594606, -29.17827...</td>\n",
       "      <td>958d155f</td>\n",
       "      <td>42.5145</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[-21.557806, -22.084446, -19.187838, -16.91901...</td>\n",
       "      <td>[-22.084385, -24.583221, -30.13426, -26.461437...</td>\n",
       "      <td>6d81d201</td>\n",
       "      <td>37.2802</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[-27.674824, -26.335936, -26.979677, -31.19679...</td>\n",
       "      <td>[-28.044491, -27.67487, -29.704073, -31.196793...</td>\n",
       "      <td>75126706</td>\n",
       "      <td>41.7973</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>92</td>\n",
       "      <td>51</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[-26.345797, -26.05139, -26.650714999999998, -...</td>\n",
       "      <td>[-30.488308, -26.05139, -22.924503, -22.924503...</td>\n",
       "      <td>112a6cfa</td>\n",
       "      <td>38.0669</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>76</td>\n",
       "      <td>57</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[-15.745731, -14.04358, -14.653421, -16.111671...</td>\n",
       "      <td>[-26.538006, -27.522421, -29.906204, -27.52253...</td>\n",
       "      <td>a29662a4</td>\n",
       "      <td>39.6636</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>38</td>\n",
       "      <td>162</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[-14.6148, -14.6148, -16.136662, -15.342532, -...</td>\n",
       "      <td>[-26.656, -26.656, -22.534969, -25.496277, -26...</td>\n",
       "      <td>bd1a1bdf</td>\n",
       "      <td>37.6866</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>95</td>\n",
       "      <td>111</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[-16.015327, -16.015327, -14.87892, -16.899502...</td>\n",
       "      <td>[-23.789312, -23.789312, -24.021, -23.78941499...</td>\n",
       "      <td>31e37d93</td>\n",
       "      <td>40.296</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>33</td>\n",
       "      <td>150</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[-19.261448, -19.671938, -20.712574, -20.10284...</td>\n",
       "      <td>[-27.220249, -28.671318, -30.910847, -25.69265...</td>\n",
       "      <td>76b8d446</td>\n",
       "      <td>39.234</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[-10.99748, -11.994458, -11.209444, -11.209444...</td>\n",
       "      <td>[-12.792967, -18.622711, -13.816119, -13.81611...</td>\n",
       "      <td>958d42a8</td>\n",
       "      <td>40.3904</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -7.268992, -3.272221...</td>\n",
       "      <td>[[-12.792967, -0.0, -13.816119, -13.816119, -1...</td>\n",
       "      <td>165</td>\n",
       "      <td>363</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[-31.042643, -31.60326, -32.202568, -30.51619,...</td>\n",
       "      <td>[-34.297188, -32.846218, -30.019676, -29.10457...</td>\n",
       "      <td>70830858</td>\n",
       "      <td>43.7895</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>64</td>\n",
       "      <td>57</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[-27.995171, -23.865738, -22.165567, -22.76487...</td>\n",
       "      <td>[-28.785341, -30.620794, -27.625595, -25.40263...</td>\n",
       "      <td>faf2c49e</td>\n",
       "      <td>42.5891</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[-26.390104, -28.888929, -28.888929, -27.72913...</td>\n",
       "      <td>[-25.230265, -26.706038, -26.706038, -24.45200...</td>\n",
       "      <td>02314c59</td>\n",
       "      <td>41.0303</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>[-21.309566, -20.840015, -22.515629, -21.15040...</td>\n",
       "      <td>[-26.709482, -26.709555, -27.998783, -29.51327...</td>\n",
       "      <td>84fe7f94</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>53</td>\n",
       "      <td>70</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>[-20.215452, -19.626303, -18.941971, -19.07453...</td>\n",
       "      <td>[-22.867964, -23.2917, -23.737179, -23.511507,...</td>\n",
       "      <td>04e6f331</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>78</td>\n",
       "      <td>102</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>[-23.334843, -25.328943, -26.213058, -25.90830...</td>\n",
       "      <td>[-31.073734, -29.581106, -33.572594, -32.87739...</td>\n",
       "      <td>92c90853</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>129</td>\n",
       "      <td>98</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>[-25.666952, -25.942781, -24.64403, -24.174473...</td>\n",
       "      <td>[-25.666952, -25.667017, -25.942844, -27.81134...</td>\n",
       "      <td>660a98a7</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>90</td>\n",
       "      <td>81</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>[-19.281799, -17.870247, -18.202303, -18.78553...</td>\n",
       "      <td>[-23.890778, -26.988886, -30.357046, -27.63272...</td>\n",
       "      <td>89670962</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>116</td>\n",
       "      <td>73</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>[-18.215658, -17.459957, -17.564053, -19.68742...</td>\n",
       "      <td>[-25.064114, -26.697582, -27.986814, -27.64621...</td>\n",
       "      <td>9d586019</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>101</td>\n",
       "      <td>68</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>[-17.248318, -16.214254, -15.532733, -15.95249...</td>\n",
       "      <td>[-21.718636, -24.217484, -24.443178, -25.15756...</td>\n",
       "      <td>5f49ea3b</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>111</td>\n",
       "      <td>71</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>[-24.588913, -24.588951, -24.85627, -23.363632...</td>\n",
       "      <td>[-29.853745, -28.110779, -23.363598, -23.83325...</td>\n",
       "      <td>968e1414</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>[-24.811455, -25.078775, -24.055771, -20.69634...</td>\n",
       "      <td>[-29.161131, -27.222977, -28.737434, -30.07641...</td>\n",
       "      <td>389d7eaf</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>70</td>\n",
       "      <td>75</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>[-19.654991, -24.04619, -21.321854, -18.991802...</td>\n",
       "      <td>[-26.099377, -25.814627, -29.197536, -32.12015...</td>\n",
       "      <td>65ca9e76</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>103</td>\n",
       "      <td>100</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>[-19.815434, -20.994926, -19.954279, -17.52577...</td>\n",
       "      <td>[-23.761044, -23.760979, -22.737862, -24.92068...</td>\n",
       "      <td>a09cae27</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>45</td>\n",
       "      <td>89</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>[-18.443468, -17.668488, -21.296959, -21.79350...</td>\n",
       "      <td>[-24.349022, -24.12347, -23.689157, -27.645437...</td>\n",
       "      <td>00c5b3e0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>95</td>\n",
       "      <td>69</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>[-21.710968, -21.162241, -24.336622, -22.92503...</td>\n",
       "      <td>[-21.710968, -23.840113, -26.023041, -23.60220...</td>\n",
       "      <td>7f9df2b0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-21.710968, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>76</td>\n",
       "      <td>60</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>[-20.982038, -17.765095, -17.460377, -17.36119...</td>\n",
       "      <td>[-28.291798, -27.30752, -22.995605, -25.124796...</td>\n",
       "      <td>a2303efc</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>64</td>\n",
       "      <td>56</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>[-13.383982, -12.767777, -14.186477, -17.88528...</td>\n",
       "      <td>[-24.233467, -24.233543, -23.185856, -23.38561...</td>\n",
       "      <td>cb62e5cb</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>101</td>\n",
       "      <td>66</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>[-22.688837, -21.349943, -22.898233, -22.89827...</td>\n",
       "      <td>[-30.048372, -31.63204, -34.326054, -32.231392...</td>\n",
       "      <td>9ff1e0f0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>130</td>\n",
       "      <td>132</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>[-22.613323, -20.475389, -19.295786, -19.15894...</td>\n",
       "      <td>[-24.401331, -23.91861, -23.461304, -26.983082...</td>\n",
       "      <td>39fd995a</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>89</td>\n",
       "      <td>68</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>[-16.95768, -21.794147, -24.581587, -24.819641...</td>\n",
       "      <td>[-23.27611, -29.501163, -35.945621, -29.501307...</td>\n",
       "      <td>544d0681</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>73</td>\n",
       "      <td>48</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>[-5.64585, -7.455446, -8.050229, -8.050307, -8...</td>\n",
       "      <td>[-14.208546, -13.801398, -14.709263, -15.80631...</td>\n",
       "      <td>cb0319fc</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-5.64585, -7.455446, -8.050229, -8.050307, -...</td>\n",
       "      <td>[[-14.208546, -13.801398, -14.709263, -15.8063...</td>\n",
       "      <td>44</td>\n",
       "      <td>92</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>[-28.554342, -26.37149, -25.17157, -25.171608,...</td>\n",
       "      <td>[-26.055567, -27.394543, -28.55442, -30.389864...</td>\n",
       "      <td>d86deb2b</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>65</td>\n",
       "      <td>47</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>[-25.950792, -26.255636, -23.834808, -25.09588...</td>\n",
       "      <td>[-30.093304, -28.75441, -27.240044, -29.178288...</td>\n",
       "      <td>cdee905a</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>98</td>\n",
       "      <td>196</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[-25.31617, -25.895485, -24.773212, -23.779497...</td>\n",
       "      <td>[-26.516129, -29.122847, -29.568417, -30.53455...</td>\n",
       "      <td>2539742b</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>148</td>\n",
       "      <td>156</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[-14.463013, -15.378237, -14.535848, -14.83290...</td>\n",
       "      <td>[-25.165276, -24.682676, -25.41721, -30.026262...</td>\n",
       "      <td>2ea3c9f1</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>117</td>\n",
       "      <td>74</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[-19.997332, -18.115284, -15.673027, -14.26133...</td>\n",
       "      <td>[-20.727669, -23.175728, -20.576469, -20.28193...</td>\n",
       "      <td>9cadda28</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[-14.200315, -15.292111, -15.60808, -18.324097...</td>\n",
       "      <td>[-24.02071, -24.455181, -24.020878, -23.607269...</td>\n",
       "      <td>8376a077</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[-30.999878, -29.976866, -28.233906, -29.50732...</td>\n",
       "      <td>[-27.847719, -28.233864, -24.712077999999998, ...</td>\n",
       "      <td>04e11240</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -24.452822, -0.0, -0.0, -0...</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>[-25.31155, -26.511555, -28.694487, -27.180115...</td>\n",
       "      <td>[-29.563713, -28.290375, -26.839405, -28.29046...</td>\n",
       "      <td>c7d6f6f8</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>141</td>\n",
       "      <td>118</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>[-18.141895, -18.141844, -19.01737, -19.701599...</td>\n",
       "      <td>[-25.305355, -29.387701, -28.963863, -26.16023...</td>\n",
       "      <td>bba1a0f1</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -10.697523, -9...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>80</td>\n",
       "      <td>68</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>[-22.455633, -25.794661, -26.954567, -22.83354...</td>\n",
       "      <td>[-26.070356, -22.093737, -21.577662, -24.53376...</td>\n",
       "      <td>7f66bb44</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>[-19.909191, -20.678406, -20.208834, -18.42441...</td>\n",
       "      <td>[-24.44487, -24.956001, -27.722103, -26.078417...</td>\n",
       "      <td>9d8f326c</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1604 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 band_1  \\\n",
       "0     [-27.878360999999998, -27.15416, -28.668615, -...   \n",
       "1     [-12.242375, -14.920304999999999, -14.920363, ...   \n",
       "2     [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
       "3     [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
       "4     [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
       "5     [-20.769371, -20.769434, -25.906025, -25.90602...   \n",
       "6     [-26.673811, -23.666162, -27.622442, -28.31768...   \n",
       "7     [-24.989119, -27.755224, -25.817074, -24.98927...   \n",
       "8     [-17.146641, -17.146572, -17.994583, -19.44553...   \n",
       "9     [-24.020853, -23.551275, -27.18819, -29.126434...   \n",
       "10    [-21.397552, -19.753859, -23.426783, -24.65221...   \n",
       "11    [-22.020813, -22.020864, -20.345379, -18.07829...   \n",
       "12    [-21.112206, -21.638832, -25.436468, -23.22255...   \n",
       "13    [-23.864258, -27.755791, -26.047226, -24.62014...   \n",
       "14    [-20.558899, -21.328135, -19.585205, -19.71788...   \n",
       "15    [-28.282215, -27.896156, -25.882795, -25.88279...   \n",
       "16    [-21.345695, -21.345734, -21.166676, -20.65056...   \n",
       "17    [-19.261189, -19.812914, -23.263891, -25.41662...   \n",
       "18    [-25.950783, -26.571512, -22.943153, -21.94372...   \n",
       "19    [-21.557806, -22.084446, -19.187838, -16.91901...   \n",
       "20    [-27.674824, -26.335936, -26.979677, -31.19679...   \n",
       "21    [-26.345797, -26.05139, -26.650714999999998, -...   \n",
       "22    [-15.745731, -14.04358, -14.653421, -16.111671...   \n",
       "23    [-14.6148, -14.6148, -16.136662, -15.342532, -...   \n",
       "24    [-16.015327, -16.015327, -14.87892, -16.899502...   \n",
       "25    [-19.261448, -19.671938, -20.712574, -20.10284...   \n",
       "26    [-10.99748, -11.994458, -11.209444, -11.209444...   \n",
       "27    [-31.042643, -31.60326, -32.202568, -30.51619,...   \n",
       "28    [-27.995171, -23.865738, -22.165567, -22.76487...   \n",
       "29    [-26.390104, -28.888929, -28.888929, -27.72913...   \n",
       "...                                                 ...   \n",
       "1574  [-21.309566, -20.840015, -22.515629, -21.15040...   \n",
       "1575  [-20.215452, -19.626303, -18.941971, -19.07453...   \n",
       "1576  [-23.334843, -25.328943, -26.213058, -25.90830...   \n",
       "1577  [-25.666952, -25.942781, -24.64403, -24.174473...   \n",
       "1578  [-19.281799, -17.870247, -18.202303, -18.78553...   \n",
       "1579  [-18.215658, -17.459957, -17.564053, -19.68742...   \n",
       "1580  [-17.248318, -16.214254, -15.532733, -15.95249...   \n",
       "1581  [-24.588913, -24.588951, -24.85627, -23.363632...   \n",
       "1582  [-24.811455, -25.078775, -24.055771, -20.69634...   \n",
       "1583  [-19.654991, -24.04619, -21.321854, -18.991802...   \n",
       "1584  [-19.815434, -20.994926, -19.954279, -17.52577...   \n",
       "1585  [-18.443468, -17.668488, -21.296959, -21.79350...   \n",
       "1586  [-21.710968, -21.162241, -24.336622, -22.92503...   \n",
       "1587  [-20.982038, -17.765095, -17.460377, -17.36119...   \n",
       "1588  [-13.383982, -12.767777, -14.186477, -17.88528...   \n",
       "1589  [-22.688837, -21.349943, -22.898233, -22.89827...   \n",
       "1590  [-22.613323, -20.475389, -19.295786, -19.15894...   \n",
       "1591  [-16.95768, -21.794147, -24.581587, -24.819641...   \n",
       "1592  [-5.64585, -7.455446, -8.050229, -8.050307, -8...   \n",
       "1593  [-28.554342, -26.37149, -25.17157, -25.171608,...   \n",
       "1594  [-25.950792, -26.255636, -23.834808, -25.09588...   \n",
       "1595  [-25.31617, -25.895485, -24.773212, -23.779497...   \n",
       "1596  [-14.463013, -15.378237, -14.535848, -14.83290...   \n",
       "1597  [-19.997332, -18.115284, -15.673027, -14.26133...   \n",
       "1598  [-14.200315, -15.292111, -15.60808, -18.324097...   \n",
       "1599  [-30.999878, -29.976866, -28.233906, -29.50732...   \n",
       "1600  [-25.31155, -26.511555, -28.694487, -27.180115...   \n",
       "1601  [-18.141895, -18.141844, -19.01737, -19.701599...   \n",
       "1602  [-22.455633, -25.794661, -26.954567, -22.83354...   \n",
       "1603  [-19.909191, -20.678406, -20.208834, -18.42441...   \n",
       "\n",
       "                                                 band_2        id inc_angle  \\\n",
       "0     [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
       "1     [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
       "2     [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
       "3     [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
       "4     [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
       "5     [-29.288746, -29.712593, -28.884804, -28.88480...  b51d18b5   36.9034   \n",
       "6     [-24.557735, -26.97868, -27.622442, -29.073456...  31da1a04   34.4751   \n",
       "7     [-27.755173, -26.732174, -28.124943, -31.83772...  56929c16   41.1769   \n",
       "8     [-25.733608, -24.472507, -24.710424, -22.77215...  525ab75c   35.7829   \n",
       "9     [-28.702518, -33.563324, -29.571918, -29.12643...  192f56eb   43.3007   \n",
       "10    [-26.72291, -27.418192, -27.787899, -25.774536...  3aac67cd    44.624   \n",
       "11    [-29.018383, -26.519661, -26.214916, -27.16346...  161a6860   39.5067   \n",
       "12    [-27.30481, -28.415202999999998, -24.634125, -...  3c794f0c   41.8544   \n",
       "13    [-23.626272, -24.620068, -28.546, -26.363146, ...  86730f0d   45.2909   \n",
       "14    [-29.127485, -30.40094, -28.741528, -24.380484...  e356f7a3   34.7715   \n",
       "15    [-31.608845, -29.110111, -32.851887, -32.85188...  87592c38    43.782   \n",
       "16    [-26.343246, -25.143326, -23.374924, -22.92943...  1c18a39e   45.3568   \n",
       "17    [-25.149176, -26.271551, -27.560766, -27.91539...  a210f335   38.7812   \n",
       "18    [-29.623671, -30.093336, -27.594606, -29.17827...  958d155f   42.5145   \n",
       "19    [-22.084385, -24.583221, -30.13426, -26.461437...  6d81d201   37.2802   \n",
       "20    [-28.044491, -27.67487, -29.704073, -31.196793...  75126706   41.7973   \n",
       "21    [-30.488308, -26.05139, -22.924503, -22.924503...  112a6cfa   38.0669   \n",
       "22    [-26.538006, -27.522421, -29.906204, -27.52253...  a29662a4   39.6636   \n",
       "23    [-26.656, -26.656, -22.534969, -25.496277, -26...  bd1a1bdf   37.6866   \n",
       "24    [-23.789312, -23.789312, -24.021, -23.78941499...  31e37d93    40.296   \n",
       "25    [-27.220249, -28.671318, -30.910847, -25.69265...  76b8d446    39.234   \n",
       "26    [-12.792967, -18.622711, -13.816119, -13.81611...  958d42a8   40.3904   \n",
       "27    [-34.297188, -32.846218, -30.019676, -29.10457...  70830858   43.7895   \n",
       "28    [-28.785341, -30.620794, -27.625595, -25.40263...  faf2c49e   42.5891   \n",
       "29    [-25.230265, -26.706038, -26.706038, -24.45200...  02314c59   41.0303   \n",
       "...                                                 ...       ...       ...   \n",
       "1574  [-26.709482, -26.709555, -27.998783, -29.51327...  84fe7f94        na   \n",
       "1575  [-22.867964, -23.2917, -23.737179, -23.511507,...  04e6f331        na   \n",
       "1576  [-31.073734, -29.581106, -33.572594, -32.87739...  92c90853        na   \n",
       "1577  [-25.666952, -25.667017, -25.942844, -27.81134...  660a98a7        na   \n",
       "1578  [-23.890778, -26.988886, -30.357046, -27.63272...  89670962        na   \n",
       "1579  [-25.064114, -26.697582, -27.986814, -27.64621...  9d586019        na   \n",
       "1580  [-21.718636, -24.217484, -24.443178, -25.15756...  5f49ea3b        na   \n",
       "1581  [-29.853745, -28.110779, -23.363598, -23.83325...  968e1414        na   \n",
       "1582  [-29.161131, -27.222977, -28.737434, -30.07641...  389d7eaf        na   \n",
       "1583  [-26.099377, -25.814627, -29.197536, -32.12015...  65ca9e76        na   \n",
       "1584  [-23.761044, -23.760979, -22.737862, -24.92068...  a09cae27        na   \n",
       "1585  [-24.349022, -24.12347, -23.689157, -27.645437...  00c5b3e0        na   \n",
       "1586  [-21.710968, -23.840113, -26.023041, -23.60220...  7f9df2b0        na   \n",
       "1587  [-28.291798, -27.30752, -22.995605, -25.124796...  a2303efc        na   \n",
       "1588  [-24.233467, -24.233543, -23.185856, -23.38561...  cb62e5cb        na   \n",
       "1589  [-30.048372, -31.63204, -34.326054, -32.231392...  9ff1e0f0        na   \n",
       "1590  [-24.401331, -23.91861, -23.461304, -26.983082...  39fd995a        na   \n",
       "1591  [-23.27611, -29.501163, -35.945621, -29.501307...  544d0681        na   \n",
       "1592  [-14.208546, -13.801398, -14.709263, -15.80631...  cb0319fc        na   \n",
       "1593  [-26.055567, -27.394543, -28.55442, -30.389864...  d86deb2b        na   \n",
       "1594  [-30.093304, -28.75441, -27.240044, -29.178288...  cdee905a        na   \n",
       "1595  [-26.516129, -29.122847, -29.568417, -30.53455...  2539742b        na   \n",
       "1596  [-25.165276, -24.682676, -25.41721, -30.026262...  2ea3c9f1        na   \n",
       "1597  [-20.727669, -23.175728, -20.576469, -20.28193...  9cadda28        na   \n",
       "1598  [-24.02071, -24.455181, -24.020878, -23.607269...  8376a077        na   \n",
       "1599  [-27.847719, -28.233864, -24.712077999999998, ...  04e11240        na   \n",
       "1600  [-29.563713, -28.290375, -26.839405, -28.29046...  c7d6f6f8        na   \n",
       "1601  [-25.305355, -29.387701, -28.963863, -26.16023...  bba1a0f1        na   \n",
       "1602  [-26.070356, -22.093737, -21.577662, -24.53376...  7f66bb44        na   \n",
       "1603  [-24.44487, -24.956001, -27.722103, -26.078417...  9d8f326c        na   \n",
       "\n",
       "      is_iceberg                                               iso1  \\\n",
       "0              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "2              1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "3              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "4              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "5              1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "6              1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "7              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "8              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "9              0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "10             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "11             0  [[-0.0, -0.0, -0.0, -18.078295, -0.0, -0.0, -0...   \n",
       "12             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "13             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "14             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "15             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "16             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "17             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "18             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "19             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "20             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "21             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "22             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "23             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "24             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "25             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "26             1  [[-0.0, -0.0, -0.0, -0.0, -7.268992, -3.272221...   \n",
       "27             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "28             1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "29             0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "...          ...                                                ...   \n",
       "1574           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1575           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1576           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1577           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1578           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1579           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1580           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1581           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1582           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1583           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1584           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1585           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1586           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1587           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1588           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1589           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1590           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1591           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1592           0  [[-5.64585, -7.455446, -8.050229, -8.050307, -...   \n",
       "1593           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1594           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1595           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1596           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1597           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1598           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1599           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1600           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1601           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -10.697523, -9...   \n",
       "1602           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1603           0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "\n",
       "                                                   iso2   s1   s2  size  \n",
       "0     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   64   57   121  \n",
       "1     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   17   92   109  \n",
       "2     [[-0.0, -0.0, -0.0, -19.411043, -0.0, -0.0, -2...   91   60   151  \n",
       "3     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   52   48   100  \n",
       "4     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   68   44   112  \n",
       "5     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  120  153   273  \n",
       "6     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -21.9427...  133   74   207  \n",
       "7     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  111   53   164  \n",
       "8     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...    8  116   124  \n",
       "9     [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   60   65   125  \n",
       "10    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  110   65   175  \n",
       "11    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   67   68   135  \n",
       "12    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  187  260   447  \n",
       "13    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   58   62   120  \n",
       "14    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  108   83   191  \n",
       "15    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   73   56   129  \n",
       "16    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   97   49   146  \n",
       "17    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   42   83   125  \n",
       "18    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   59   71   130  \n",
       "19    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   77   70   147  \n",
       "20    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   92   51   143  \n",
       "21    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   76   57   133  \n",
       "22    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   38  162   200  \n",
       "23    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   95  111   206  \n",
       "24    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   33  150   183  \n",
       "25    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   93  101   194  \n",
       "26    [[-12.792967, -0.0, -13.816119, -13.816119, -1...  165  363   528  \n",
       "27    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   64   57   121  \n",
       "28    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   78   81   159  \n",
       "29    [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   62   54   116  \n",
       "...                                                 ...  ...  ...   ...  \n",
       "1574  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   53   70   123  \n",
       "1575  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   78  102   180  \n",
       "1576  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  129   98   227  \n",
       "1577  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   90   81   171  \n",
       "1578  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  116   73   189  \n",
       "1579  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  101   68   169  \n",
       "1580  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  111   71   182  \n",
       "1581  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   73   86   159  \n",
       "1582  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   70   75   145  \n",
       "1583  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  103  100   203  \n",
       "1584  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   45   89   134  \n",
       "1585  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   95   69   164  \n",
       "1586  [[-21.710968, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   76   60   136  \n",
       "1587  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   64   56   120  \n",
       "1588  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  101   66   167  \n",
       "1589  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  130  132   262  \n",
       "1590  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   89   68   157  \n",
       "1591  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   73   48   121  \n",
       "1592  [[-14.208546, -13.801398, -14.709263, -15.8063...   44   92   136  \n",
       "1593  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   65   47   112  \n",
       "1594  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   98  196   294  \n",
       "1595  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  148  156   304  \n",
       "1596  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  117   74   191  \n",
       "1597  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...    0  140   140  \n",
       "1598  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   80   97   177  \n",
       "1599  [[-0.0, -0.0, -0.0, -24.452822, -0.0, -0.0, -0...  110   80   190  \n",
       "1600  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...  141  118   259  \n",
       "1601  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   80   68   148  \n",
       "1602  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   55   44    99  \n",
       "1603  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   58   64   122  \n",
       "\n",
       "[1604 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data\n",
    "use_cuda= True if torch.cuda.is_available() else False\n",
    "#use_cuda =False\n",
    "#dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor \n",
    "dtype = torch.FloatTensor \n",
    "data=  pd.read_json(BASE_dir + 'train.json')\n",
    "\n",
    "class iceberg_dataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label=  self.data[idx], self.label[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "                rotate = np.random.randint(4, size=1)\n",
    "                if rotate:\n",
    "                    img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                \n",
    "                scale1 = np.exp(np.random.uniform(np.log(1/1.1), np.log(1.1)))\n",
    "                tran = np.random.uniform(-5, 5)\n",
    "                aug = tf.AffineTransform(translation=tran, scale= (scale1, scale1))\n",
    "                img = tf.warp(img, inverse_map=aug)\n",
    "                pass\n",
    "#             temp = []\n",
    "#             for i in img:\n",
    "#                 temp.append(tf.rescale(i,224/75,mode='constant'))\n",
    "#             img = np.stack(temp)\n",
    "            img = torch.from_numpy(img).type(dtype)\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "class iceberg_angle_dataset(Dataset):\n",
    "    def __init__(self, data,angle,label,size=None, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "#         self.angle=torch.cat( (torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1),torch.from_numpy(size).type(torch.FloatTensor).unsqueeze(1)),1)\n",
    "        self.angle=torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1)\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label, angle=  self.data[idx], self.label[idx], self.angle[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            \n",
    "            #rotate, scale, shear, translation\n",
    "#             if self.test is False:\n",
    "#                 angle = np.random.uniform(0,360)\n",
    "#                 img = tf.rotate(img,angle=angle,resize=False)\n",
    "#                 scale1 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.2)))\n",
    "#                 scale2 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.1)))\n",
    "#                 #shear = np.random.uniform(-np.pi/18, np.pi/18)\n",
    "#                 #tran = np.random.uniform(-5, 5)\n",
    "#                 #aug = tf.AffineTransform(shear = shear, translation=tran, scale= (scale1, scale2))\n",
    "#                 aug = tf.AffineTransform(scale= (scale1, scale2))\n",
    "#                 img = tf.warp(img, inverse_map=aug)\n",
    "            \n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "            \n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "#                 rotate = np.random.randint(4, size=1)\n",
    "#                 if rotate:\n",
    "#                     img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "            pass\n",
    "        img = torch.from_numpy(img).type(dtype)\n",
    "#         img = self.transform(img)\n",
    "\n",
    "        return img, angle,label    \n",
    "    \n",
    "    \n",
    "def stack(row):\n",
    "    return np.stack(row[['c1','c2','c3']]).reshape(3,75,75)\n",
    "\n",
    "def raw_to_numpy(data):\n",
    "    img = []\n",
    "    data['c1'] = data['band_1'].apply(np.array)\n",
    "    data['c2'] = data['band_2'].apply(np.array)\n",
    "    data['c3'] = (data['c1'] + data['c2'])/2\n",
    "#     data['c3'] = (data['c1'] + data['c2'])/2\n",
    "    for _, row in data.iterrows():\n",
    "        img.append(stack(row))\n",
    "    return np.stack(img)\n",
    "\n",
    "def transform_compute(img):\n",
    "    train_mean = img.mean(axis=(0,2,3))\n",
    "    train_std = img.std(axis=(0,2,3))\n",
    "    return train_mean, train_std\n",
    "\n",
    "def data_aug(X, y):    \n",
    "    X_rot_30 = []\n",
    "    X_rot_60 = [] \n",
    "    X_h = np.flip(X, 3)\n",
    "    X_v = np.flip(X, 2)\n",
    "    for i in X:\n",
    "        X_rot_30.append(tf.rotate(i,angle=90,resize=False))\n",
    "        X_rot_60.append(tf.rotate(i,angle=270,resize=False))\n",
    "        \n",
    "    X_rot_30 = np.stack(X_rot_30)\n",
    "    X_rot_60 = np.stack(X_rot_60)\n",
    "    ch_y = np.concatenate((y,y,y,y,y))\n",
    "    ch_X = np.concatenate((X, X_h, X_v, X_rot_30, X_rot_60))\n",
    "    return ch_X, ch_y\n",
    "\n",
    "train_X = raw_to_numpy(data)#.transpose(0,2,3,1)\n",
    "train_X.shape     #1604 * 3 *75* 75   N*c*H*W\n",
    "train_y = data['is_iceberg'].values # if iceberg then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are 0\n",
      "We are 50\n",
      "We are 100\n",
      "We are 150\n",
      "We are 200\n",
      "We are 250\n",
      "We are 300\n",
      "We are 350\n",
      "We are 400\n",
      "We are 450\n",
      "We are 500\n",
      "We are 550\n",
      "We are 600\n",
      "We are 650\n",
      "We are 700\n",
      "We are 750\n",
      "We are 800\n",
      "We are 850\n",
      "We are 900\n",
      "We are 950\n",
      "We are 1000\n",
      "We are 1050\n",
      "We are 1100\n",
      "We are 1150\n",
      "We are 1200\n",
      "We are 1250\n",
      "We are 1300\n",
      "We are 1350\n",
      "We are 1400\n",
      "We are 1450\n",
      "We are 1500\n",
      "We are 1550\n",
      "We are 1600\n"
     ]
    }
   ],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "result = []\n",
    "for num,i in enumerate(train_X_del):\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(tf.rescale(j,224/75,mode='constant'))\n",
    "    img = np.stack(temp)\n",
    "    result.append(img)\n",
    "    if num%50==0:\n",
    "        print('We are %d'%num)\n",
    "train_X_del = np.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_index=list(range(1300))\n",
    "# val_index= list(range(1300,1604))\n",
    "# train_index=list(range(304,1604)) \n",
    "# val_index= list(range(304))\n",
    "# # train_X[train_index].shape\n",
    "\n",
    "# # data.inc_angle = data.inc_angle.map(lambda x: 0.0 if x == 'na' else x)\n",
    "# # train_index = np.where(data.inc_angle > 0)[0]\n",
    "# # val_index = np.where(data.inc_angle <= 0)[0]\n",
    "\n",
    "# # seed= np.random.RandomState(123)\n",
    "# # spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# # train_index, val_index = next(spliter.split(train_X))\n",
    "# train_mean, train_std = transform_compute(train_X[train_index])\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X[train_index], label=train_y[train_index], transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "## add augmentation \n",
    "# seed= np.random.RandomState(123)\n",
    "# spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# train_index, val_index = next(spliter.split(train_X))\n",
    "\n",
    "# train_X_af,train_y_af = data_aug(train_X[train_index], train_y[train_index])\n",
    "# train_mean, train_std = transform_compute(train_X_af)\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X_af, label=train_y_af, transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "# train_X_del = train_X[data.inc_angle!='na',:,:,:]\n",
    "# train_y_del = train_y[data.inc_angle!='na']\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "# af_train_X, af_train_y = data_aug(train_X_del[train_index], train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "af_train_X, af_train_y = train_X_del[train_index], train_y_del[train_index]\n",
    "\n",
    "train_dataset = iceberg_dataset(data= af_train_X, label=af_train_y, transform=train_transform)\n",
    "val_dataset = iceberg_dataset(data= train_X_del[val_index], label=train_y_del[val_index], transform=train_transform, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "## angle and size\n",
    "\n",
    "data['inc_angle'][data['inc_angle']=='na']=0\n",
    "train_X = train_X\n",
    "train_angle_del = data['inc_angle'].values\n",
    "train_angle = train_angle_del.astype(np.float)\n",
    "#train_size = train['size'].values\n",
    "train_y = train_y\n",
    "\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "#af_train_X,af_train_angle, af_train_y = data_aug(train_X_del[train_index], train_angle_del[train_index],train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "\n",
    "\n",
    "train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                    label=train_y[train_index],\n",
    "                                    transform=train_transform, test=True)\n",
    "\n",
    "val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                    label=train_y[val_index],\n",
    "                                    transform=train_transform, test=True)\n",
    "\n",
    "# train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],size=train_size[train_index],\n",
    "#                                     label=train_y[train_index],\n",
    "#                                     transform=train_transform)\n",
    "\n",
    "# val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],size= train_size[val_index],\n",
    "#                                     label=train_y[val_index],\n",
    "#                                     transform=train_transform, test=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "(0 ,0 ,.,.) = \n",
      " -1.0975e+00 -9.8599e-01 -1.0975e+00  ...  -1.8955e+00 -8.8212e-01 -6.9086e-01\n",
      " -1.0408e+00 -9.3292e-01 -1.0975e+00  ...  -2.9090e-01 -8.8212e-01 -4.3962e-01\n",
      " -1.0408e+00 -1.6371e+00 -1.6371e+00  ...  -2.8118e-02 -9.0055e-02 -4.7897e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -1.2174e+00 -8.3202e-01 -6.9066e-01  ...  -1.5604e+00 -1.6382e+00 -1.4150e+00\n",
      " -9.8645e-01 -1.0413e+00 -9.3338e-01  ...  -8.8258e-01 -1.6382e+00 -1.1573e+00\n",
      " -9.8645e-01 -1.2174e+00 -5.1905e-01  ...  -9.3402e-01 -1.1573e+00 -9.8712e-01\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -9.6188e-01 -1.2315e+00 -7.1806e-01  ...  -8.3805e-01 -1.0941e+00 -1.5338e+00\n",
      " -7.1804e-01 -8.3707e-01 -7.1806e-01  ...  -4.9642e-01 -1.5338e+00 -1.3787e+00\n",
      " -7.1804e-01 -4.9544e-01 -4.9545e-01  ...  -2.9163e-01 -2.9164e-01 -6.0535e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -1.9474e-01 -1.3784e+00 -2.2696e+00  ...  -2.7370e+00 -1.3794e+00 -2.9234e-01\n",
      " -1.6986e+00 -1.3784e+00 -1.8752e+00  ...  -3.9267e-01 -1.3794e+00 -7.1976e-01\n",
      " -1.6986e+00 -1.5335e+00 -1.3784e+00  ...  -7.1974e-01 -8.3878e-01 -9.6363e-01\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -1.1763e+00 -1.2203e+00 -1.0677e+00  ...  -1.6649e+00 -1.0884e+00 -1.1538e+00\n",
      " -1.0291e+00 -1.0085e+00 -1.0677e+00  ...  -4.1931e-01 -1.2842e+00 -9.1355e-01\n",
      " -1.0291e+00 -1.3363e+00 -1.3363e+00  ...  -1.4903e-01 -1.9124e-01 -5.9598e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -9.1637e-01 -1.1808e+00 -1.4813e+00  ...  -2.2822e+00 -1.7307e+00 -1.0945e+00\n",
      " -1.4287e+00 -1.3234e+00 -1.4711e+00  ...  -7.7633e-01 -1.7307e+00 -1.1092e+00\n",
      " -1.4287e+00 -1.5126e+00 -9.6756e-01  ...  -9.5704e-01 -1.1622e+00 -1.1018e+00\n",
      "     â‹® \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  8.1373e-01  1.2856e+00  1.5511e+00  ...   9.8270e-02  4.0307e-01  7.9458e-01\n",
      "  7.0098e-01  9.3631e-01  1.2439e+00  ...  -4.7197e-02  1.5308e-01  5.3687e-01\n",
      "  5.1616e-01  8.6735e-01  5.1614e-01  ...  -3.8268e-01  2.0615e-01  2.3206e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  8.4956e-01  4.0380e-01 -5.3760e-01  ...  -7.7992e-02 -8.5142e-01 -5.7973e-01\n",
      "  2.3281e-01 -1.4026e-01 -4.5799e-01  ...  -6.2204e-01 -4.9815e-01 -4.5883e-01\n",
      " -1.6354e-02 -1.0842e-01  1.3213e-02  ...  -3.1010e-01  9.8140e-02  9.8128e-02\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      " -1.2992e-01 -2.2298e-01  5.8315e-01  ...  -3.2083e-01 -1.3123e-01 -1.1234e+00\n",
      " -4.1987e-01 -2.2298e-01  8.4580e-01  ...  -8.6727e-01 -8.6728e-01 -1.2617e+00\n",
      " -3.1954e-01  7.8262e-01  6.5136e-01  ...   4.5306e-02 -8.6728e-01 -8.6730e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -6.3339e-01  5.1289e-01  4.6390e-02  ...  -4.2135e-01 -1.3141e-01 -3.2104e-01\n",
      " -4.2007e-01  3.6663e-01 -4.2011e-01  ...   4.3953e-01 -4.1632e-02 -4.2139e-01\n",
      " -7.4714e-01  4.6390e-02  2.1150e-01  ...   6.4994e-01 -5.2582e-01 -8.6748e-01\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      "  4.9670e-01  7.7681e-01  1.3167e+00  ...  -7.5895e-02  2.1625e-01  4.1262e-02\n",
      "  2.9074e-01  5.3880e-01  1.2244e+00  ...  -4.1836e-01 -2.8188e-01 -1.9597e-01\n",
      "  2.0947e-01  9.3960e-01  6.4180e-01  ...  -2.4062e-01 -2.4571e-01 -2.2806e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  2.9692e-01  5.0358e-01 -3.4572e-01  ...  -2.4078e-01 -6.3876e-01 -5.3805e-01\n",
      " -2.8404e-02  6.7673e-02 -4.9920e-01  ...  -2.2819e-01 -3.5803e-01 -5.0034e-01\n",
      " -3.4385e-01 -5.3227e-02  1.0318e-01  ...   7.8088e-02 -1.6727e-01 -3.1942e-01\n",
      "     â‹® \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -1.2702e+00 -6.6056e-01 -7.8267e-02  ...  -5.2733e-01 -4.4373e-01 -1.8021e-01\n",
      " -3.6351e-01 -9.6538e-01 -1.0807e+00  ...  -6.1534e-01 -3.6411e-01 -1.4552e-02\n",
      " -3.6351e-01 -1.3384e+00 -1.1415e+00  ...  -8.0662e-01 -2.8811e-01 -2.1542e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -5.7034e-01 -1.1417e+00 -1.0809e+00  ...  -4.4389e-01 -4.6560e-02 -1.1214e-01\n",
      " -1.1417e+00 -7.0783e-01 -6.1494e-01  ...  -5.2750e-01 -4.6560e-02 -4.6569e-02\n",
      " -1.5618e+00 -1.2704e+00 -9.6556e-01  ...  -4.4390e-01 -4.4391e-01 -4.0363e-01\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      " -1.7590e+00 -7.2161e-01 -7.2163e-01  ...  -1.9495e+00 -1.5834e+00 -9.7860e-01\n",
      " -9.7768e-01 -1.2623e+00 -1.1160e+00  ...  -1.9495e+00 -1.9495e+00 -1.5834e+00\n",
      " -9.7768e-01 -8.4646e-01 -7.2164e-01  ...  -2.3770e+00 -1.5834e+00 -9.7861e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -3.8022e-01 -6.0285e-01 -1.1163e+00  ...  -1.5837e+00 -2.1546e+00 -1.5837e+00\n",
      " -9.7794e-01 -8.4671e-01 -1.2625e+00  ...  -1.5837e+00 -2.3772e+00 -1.9498e+00\n",
      " -1.4176e+00 -1.1163e+00 -1.7593e+00  ...  -1.7602e+00 -1.7602e+00 -1.1172e+00\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      " -1.6489e+00 -7.7151e-01 -3.7468e-01  ...  -1.2275e+00 -1.0075e+00 -5.5859e-01\n",
      " -6.8310e-01 -1.2200e+00 -1.2335e+00  ...  -1.2875e+00 -1.1163e+00 -7.1502e-01\n",
      " -6.8310e-01 -1.2891e+00 -1.0993e+00  ...  -1.6082e+00 -9.0145e-01 -5.8259e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -5.5800e-01 -1.0465e+00 -1.2337e+00  ...  -1.0077e+00 -9.9118e-01 -7.8164e-01\n",
      " -1.2135e+00 -8.5943e-01 -9.8129e-01  ...  -1.0647e+00 -1.0903e+00 -9.0000e-01\n",
      " -1.6957e+00 -1.3628e+00 -1.4415e+00  ...  -1.0863e+00 -1.0864e+00 -7.7258e-01\n",
      "...   \n",
      "     â‹® \n",
      "\n",
      "(29,0 ,.,.) = \n",
      " -9.3587e-01 -1.1591e+00 -1.2199e+00  ...  -1.1012e+00 -1.0446e+00 -8.8525e-01\n",
      " -1.4168e+00 -1.8978e+00 -1.8074e+00  ...  -1.0446e+00 -8.8523e-01 -8.8525e-01\n",
      " -2.0947e+00 -1.7217e+00 -1.4880e+00  ...  -1.2838e+00 -1.5631e+00 -1.8986e+00\n",
      "                 ...                   â‹±                   ...                \n",
      " -1.0439e+00 -6.4872e-01 -4.8141e-01  ...  -1.4178e+00 -9.3683e-01 -7.8702e-01\n",
      " -7.3910e-01 -5.6297e-01 -8.8459e-01  ...  -8.3546e-01 -7.8701e-01 -1.0447e+00\n",
      " -1.1593e+00 -9.8910e-01 -1.1006e+00  ...  -5.6376e-01 -8.3547e-01 -1.1601e+00\n",
      "\n",
      "(29,1 ,.,.) = \n",
      " -1.3822e+00 -9.6644e-01 -9.6645e-01  ...  -1.0989e+00 -2.0698e+00 -1.8802e+00\n",
      " -4.9994e-01  2.3609e-01  3.1481e-01  ...  -1.0989e+00 -7.2380e-01 -7.2382e-01\n",
      "  3.9120e-01  3.9119e-01 -1.0559e-01  ...  -1.0678e-01 -2.9640e-01 -3.9677e-01\n",
      "                 ...                   â‹±                   ...                \n",
      " -2.9537e-01 -7.2280e-01 -9.6668e-01  ...  -8.4303e-01 -8.4305e-01 -1.0991e+00\n",
      " -6.0906e-01 -6.0907e-01 -7.2282e-01  ...  -8.4303e-01 -1.0991e+00 -8.4307e-01\n",
      " -7.2279e-01 -8.4183e-01 -1.3825e+00  ...  -9.6787e-01 -9.6788e-01 -2.9663e-01\n",
      "\n",
      "(29,2 ,.,.) = \n",
      " -1.2533e+00 -1.2203e+00 -1.2617e+00  ...  -1.2398e+00 -1.6336e+00 -1.4406e+00\n",
      " -1.1882e+00 -1.1882e+00 -1.0916e+00  ...  -1.2012e+00 -9.2560e-01 -9.2561e-01\n",
      " -1.2533e+00 -9.9910e-01 -1.0611e+00  ...  -9.2246e-01 -1.1972e+00 -1.4706e+00\n",
      "                 ...                   â‹±                   ...                \n",
      " -8.4295e-01 -7.6397e-01 -7.5855e-01  ...  -1.3416e+00 -1.0139e+00 -1.0258e+00\n",
      " -7.7491e-01 -6.5489e-01 -9.2472e-01  ...  -9.4477e-01 -1.0258e+00 -1.0874e+00\n",
      " -1.1119e+00 -1.0489e+00 -1.3657e+00  ...  -8.1520e-01 -1.0004e+00 -9.2268e-01\n",
      "     â‹® \n",
      "\n",
      "(30,0 ,.,.) = \n",
      "  8.1044e-01  1.4225e+00  1.7833e+00  ...   8.0962e-01  9.9279e-01  1.4459e+00\n",
      "  8.4527e-01  1.1443e+00  1.9337e+00  ...   8.4445e-01  7.3769e-01  1.8687e+00\n",
      "  1.0403e+00  1.6609e+00  1.6180e+00  ...   5.6359e-01  3.9196e-01  1.1144e+00\n",
      "                 ...                   â‹±                   ...                \n",
      "  1.1147e+00  7.0079e-01  5.0151e-01  ...   1.6806e+00  1.1715e+00  1.1715e+00\n",
      "  1.1581e+00  1.3849e+00  6.0411e-01  ...   1.6167e+00  1.4089e+00  1.6701e+00\n",
      "  1.7925e+00  1.4097e+00  1.3849e+00  ...   1.1139e+00  1.3334e+00  1.6489e+00\n",
      "\n",
      "(30,1 ,.,.) = \n",
      "  1.0531e+00  1.1081e+00  1.3668e+00  ...   7.5745e-01  1.1069e+00  1.3655e+00\n",
      "  9.9688e-01  1.3171e+00  3.5383e-01  ...   1.1608e+00  7.5743e-01  1.3158e+00\n",
      "  1.9377e+00  1.3668e+00  1.2147e+00  ...   4.9479e-01  4.2080e-02  3.5259e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  8.7969e-01  5.6340e-01 -6.1236e-01  ...   7.5659e-01  6.9339e-01  6.9339e-01\n",
      " -4.1392e-02 -1.2815e-01  1.2369e-01  ...   1.2126e+00  1.2643e+00  1.4134e+00\n",
      " -1.2815e-01 -9.5401e-01  4.2439e-02  ...   1.3150e+00  1.5543e+00  1.5542e+00\n",
      "\n",
      "(30,2 ,.,.) = \n",
      "  1.0213e+00  1.4629e+00  1.8239e+00  ...   8.8906e-01  1.1695e+00  1.5935e+00\n",
      "  1.0200e+00  1.3664e+00  1.4753e+00  ...   1.0924e+00  8.4003e-01  1.8595e+00\n",
      "  1.5718e+00  1.7405e+00  1.6436e+00  ...   6.0442e-01  2.8586e-01  9.1647e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  1.1514e+00  7.2847e-01  6.9093e-02  ...   1.4822e+00  1.1071e+00  1.1071e+00\n",
      "  7.7081e-01  8.8674e-01  4.6678e-01  ...   1.6418e+00  1.5231e+00  1.7675e+00\n",
      "  1.1645e+00  5.3585e-01  9.6269e-01  ...   1.3447e+00  1.6008e+00  1.8158e+00\n",
      "     â‹® \n",
      "\n",
      "(31,0 ,.,.) = \n",
      "  1.1330e-02  2.6510e-01  2.4138e-01  ...  -1.9357e-01  1.1637e-01  3.3315e-01\n",
      " -1.9245e-01  2.8849e-01  2.4138e-01  ...   4.6352e-01  5.4507e-01  2.4025e-01\n",
      " -3.8936e-01  9.1576e-02 -1.6337e-02  ...   2.6398e-01 -1.6293e-01 -4.6169e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  4.6442e-01  8.3746e-01  4.6439e-01  ...  -2.5691e-01 -6.9555e-01  2.4002e-01\n",
      " -4.4628e-02  6.4264e-01  6.4262e-01  ...  -4.6188e-01 -6.9555e-01 -3.2251e-01\n",
      " -4.4628e-02  3.1132e-01  4.6439e-01  ...  -3.2248e-01 -7.3788e-01 -4.9866e-01\n",
      "\n",
      "(31,1 ,.,.) = \n",
      "  1.9020e-01 -3.0659e-01 -1.5833e+00  ...   4.7305e-01 -4.0136e-01  5.3946e-01\n",
      "  9.6360e-01  1.1252e+00  9.0732e-01  ...   2.6264e-01  4.0479e-01  8.4809e-01\n",
      "  1.0186e+00  1.5122e+00  1.4668e+00  ...   7.2909e-01  1.8842e-01  5.3944e-01\n",
      "                 ...                   â‹±                   ...                \n",
      "  1.3735e+00  7.3047e-01  7.9065e-01  ...  -4.8239e-02  6.0389e-01  1.1231e+00\n",
      "  1.3257e+00  1.9633e+00  2.0006e+00  ...   1.3717e+00  6.0389e-01  9.6146e-01\n",
      "  1.3257e+00  1.3735e+00  1.1249e+00  ...   1.0704e+00  1.3717e+00  1.5100e+00\n",
      "\n",
      "(31,2 ,.,.) = \n",
      "  9.2417e-02  4.4144e-02 -5.4056e-01  ...   7.8731e-02 -9.9422e-02  4.6727e-01\n",
      "  2.9794e-01  6.9767e-01  5.6853e-01  ...   4.3284e-01  5.5172e-01  5.4139e-01\n",
      "  1.8825e-01  7.3578e-01  6.4206e-01  ...   5.0457e-01 -2.7130e-02 -7.4424e-02\n",
      "                 ...                   â‹±                   ...                \n",
      "  9.2813e-01  8.9601e-01  6.6856e-01  ...  -1.9657e-01 -2.0510e-01  6.6371e-01\n",
      "  5.5992e-01  1.3122e+00  1.3288e+00  ...   2.9608e-01 -2.0510e-01  2.0835e-01\n",
      "  5.5992e-01  8.2378e-01  8.1738e-01  ...   2.5688e-01  1.0797e-01  3.3259e-01\n",
      "[torch.FloatTensor of size 32x3x75x75]\n",
      ", \n",
      " 0.4275  1.2600\n",
      " 0.4142  1.4200\n",
      " 0.4799  3.0600\n",
      " 0.0000  2.7300\n",
      " 0.4830  2.1800\n",
      " 0.4138  2.1900\n",
      " 0.4807  2.8300\n",
      " 0.3830  1.9000\n",
      " 0.3486  1.1800\n",
      " 0.4883  1.3700\n",
      " 0.3923  2.4300\n",
      " 0.4729  2.1100\n",
      " 0.3778  1.2000\n",
      " 0.4057  1.6900\n",
      " 0.0000  1.6700\n",
      " 0.4150  1.9500\n",
      " 0.4602  1.6800\n",
      " 0.0000  1.3000\n",
      " 0.4807  1.7100\n",
      " 0.4713  1.2500\n",
      " 0.4790  1.3100\n",
      " 0.4142  1.5000\n",
      " 0.4275  2.3100\n",
      " 0.3864  1.9100\n",
      " 0.0000  2.0400\n",
      " 0.4883  3.5800\n",
      " 0.4012  2.1000\n",
      " 0.4870  1.0300\n",
      " 0.4812  1.3400\n",
      " 0.4237  1.4900\n",
      " 0.3830  3.0300\n",
      " 0.3579  2.3400\n",
      "[torch.FloatTensor of size 32x2]\n",
      ", \n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      "[torch.LongTensor of size 32]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-612a0203c5c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# use_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.from_numpy(train_X).type(torch.FloatTensor)[1].shape\n",
    "train_X[1]\n",
    "use_cuda\n",
    "# for i in train_loader:\n",
    "#     print(i.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "#         scheduler.step()\n",
    "        scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        net.train()\n",
    "        for j,(batch_x, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            y = Variable(batch_y)\n",
    "            out = net(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_fcn_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                acc= best_acc+ 0.01\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_fcn_loss.pth')\n",
    "            if loss_avg/total > val_loss:\n",
    "                e_s = 0\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x, val_y) in enumerate(val_load):\n",
    "        #len(val_x.size())==1\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        \n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        if len(out.size())==1: #in case it's one dimensional\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####train with angle and other\n",
    "\n",
    "\n",
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'cnn_ang_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'cnn_ang_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /home/FDSM_lhn/.torch/models/vgg16_bn-6c64b313.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  8.2833e-02  2.7968e-02  7.7096e-02\n",
      "  4.9341e-02 -3.3441e-02  1.9572e-02\n",
      "  8.0300e-02  7.7076e-02  8.3349e-02\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -4.4296e-02 -1.7748e-01 -4.8706e-02\n",
      " -1.1003e-01 -2.7530e-01 -1.3474e-01\n",
      " -5.9982e-03 -6.1375e-02  1.6822e-02\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  2.7480e-02 -6.6769e-02  4.3955e-02\n",
      " -2.6662e-02 -1.4995e-01 -3.3615e-02\n",
      "  5.2778e-02  1.7143e-02  8.6744e-02\n",
      "     â‹® \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -1.2628e-02  3.0218e-02 -2.6930e-02\n",
      " -1.3764e-02  1.1993e-01 -6.6263e-03\n",
      " -2.6019e-02 -8.3535e-03 -3.9197e-02\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      " -4.0557e-02  1.3983e-02 -5.4278e-02\n",
      "  1.5412e-02  1.8198e-01  1.7598e-02\n",
      " -1.7032e-02  1.1284e-02 -2.4226e-02\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      " -6.5683e-02  5.9252e-02 -5.3020e-02\n",
      "  3.8278e-02  2.7292e-01  5.9491e-02\n",
      " -4.1218e-02  3.6159e-02 -3.0478e-02\n",
      "     â‹® \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  1.4962e-06 -1.1430e-06  1.2536e-06\n",
      " -1.0341e-06 -5.1964e-06 -1.1568e-06\n",
      "  2.5825e-06  2.5617e-07  1.6146e-06\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      "  3.0030e-06  4.2831e-07  2.3388e-06\n",
      "  2.8718e-07 -3.6006e-06  1.4238e-07\n",
      "  2.9393e-06  8.8762e-07  2.3288e-06\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      "  3.4751e-06  1.3777e-06  2.7114e-06\n",
      "  1.1414e-06 -2.1910e-06  9.4626e-07\n",
      "  2.9918e-06  1.3247e-06  2.5529e-06\n",
      "...   \n",
      "     â‹® \n",
      "\n",
      "(61,0 ,.,.) = \n",
      " -7.2593e-03 -2.9968e-03 -6.5453e-03\n",
      " -1.4693e-03  3.5825e-03 -1.3039e-03\n",
      " -3.8401e-03  9.9191e-04 -4.4511e-03\n",
      "\n",
      "(61,1 ,.,.) = \n",
      "  8.8825e-03  1.6103e-02  1.0071e-02\n",
      "  1.3242e-02  2.0966e-02  1.3489e-02\n",
      "  3.3097e-03  9.6999e-03  2.8076e-03\n",
      "\n",
      "(61,2 ,.,.) = \n",
      " -3.5536e-03 -1.2907e-03 -3.8489e-04\n",
      " -3.1573e-03 -1.0735e-03 -6.6944e-04\n",
      " -7.7921e-03 -5.8025e-03 -6.2926e-03\n",
      "     â‹® \n",
      "\n",
      "(62,0 ,.,.) = \n",
      "  6.9319e-07 -1.6391e-07 -5.8921e-07\n",
      " -1.0413e-06 -2.3112e-06 -2.8000e-06\n",
      " -7.2569e-07 -2.0565e-06 -2.1360e-06\n",
      "\n",
      "(62,1 ,.,.) = \n",
      "  2.1572e-06  1.4367e-06  4.4553e-07\n",
      "  7.0185e-07 -4.5378e-07 -1.5195e-06\n",
      "  6.2624e-07 -6.7885e-07 -1.1442e-06\n",
      "\n",
      "(62,2 ,.,.) = \n",
      " -1.2531e-06 -1.3574e-06 -2.2934e-06\n",
      " -1.6023e-06 -2.0898e-06 -3.1680e-06\n",
      " -1.9390e-06 -2.6559e-06 -3.1679e-06\n",
      "     â‹® \n",
      "\n",
      "(63,0 ,.,.) = \n",
      " -7.9026e-02 -5.1706e-02 -7.3315e-02\n",
      " -8.8093e-02 -4.9941e-02 -5.7828e-02\n",
      " -8.6714e-02 -5.7272e-02 -6.3606e-02\n",
      "\n",
      "(63,1 ,.,.) = \n",
      "  1.3644e-01  3.2008e-01  1.9282e-01\n",
      "  2.5841e-01  4.7524e-01  3.3955e-01\n",
      "  1.1912e-01  2.9530e-01  1.8259e-01\n",
      "\n",
      "(63,2 ,.,.) = \n",
      " -6.7480e-02 -1.1041e-01 -9.0524e-02\n",
      " -1.4039e-01 -1.9100e-01 -1.6377e-01\n",
      " -1.2401e-01 -1.7676e-01 -1.5988e-01\n",
      "[torch.FloatTensor of size 64x3x3x3]\n",
      "\n",
      "Parameter containing:\n",
      " 1.9746e-06\n",
      " 5.5996e-07\n",
      " 3.6108e-11\n",
      " 1.1847e-07\n",
      "-2.2921e-08\n",
      "-2.8038e-07\n",
      "-5.8259e-07\n",
      " 1.4943e-10\n",
      " 2.1728e-11\n",
      " 7.5885e-06\n",
      "-9.8064e-07\n",
      " 2.9582e-11\n",
      "-7.7205e-07\n",
      " 5.6764e-08\n",
      "-1.0149e-10\n",
      " 5.8744e-07\n",
      " 8.2221e-06\n",
      "-3.8819e-07\n",
      " 1.5958e-06\n",
      " 8.8312e-08\n",
      "-5.0521e-07\n",
      " 9.6598e-11\n",
      "-1.1969e-08\n",
      " 4.3181e-07\n",
      "-3.8046e-11\n",
      " 1.0929e-07\n",
      " 2.1448e-10\n",
      "-3.2890e-07\n",
      "-6.9921e-06\n",
      " 2.9166e-11\n",
      " 5.7615e-07\n",
      " 6.3863e-07\n",
      "-1.3346e-08\n",
      "-9.4608e-07\n",
      "-2.4839e-11\n",
      "-3.5820e-09\n",
      " 4.9026e-12\n",
      " 1.5571e-10\n",
      "-7.8320e-07\n",
      " 1.4180e-11\n",
      "-1.3783e-06\n",
      "-5.7047e-07\n",
      "-2.6926e-08\n",
      "-5.4137e-10\n",
      " 2.6683e-11\n",
      "-3.3223e-07\n",
      "-1.1495e-06\n",
      " 3.9263e-07\n",
      " 1.6303e-06\n",
      " 3.2782e-08\n",
      " 5.4769e-08\n",
      "-4.7606e-08\n",
      " 2.5208e-08\n",
      " 3.1257e-07\n",
      " 1.6343e-11\n",
      "-1.2554e-07\n",
      " 3.8658e-09\n",
      "-4.6073e-06\n",
      " 1.5416e-07\n",
      "-3.0912e-10\n",
      " 3.6591e-08\n",
      " 2.9888e-08\n",
      " 3.5395e-11\n",
      " 6.9483e-08\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      " 2.3067e-01\n",
      " 3.7418e-01\n",
      " 5.1264e-06\n",
      " 7.7619e-02\n",
      " 1.6270e-01\n",
      " 1.2621e-01\n",
      " 1.8174e-01\n",
      " 2.1055e-05\n",
      " 2.9268e-05\n",
      " 5.3905e-01\n",
      " 2.0345e-01\n",
      " 2.0079e-06\n",
      " 1.5919e-01\n",
      "-5.1384e-02\n",
      "-6.0270e-06\n",
      " 4.3548e-01\n",
      " 6.5458e-01\n",
      " 1.7651e-01\n",
      " 5.9792e-01\n",
      " 3.7914e-02\n",
      " 2.0001e-01\n",
      " 1.6084e-05\n",
      " 8.1153e-03\n",
      " 2.1720e-01\n",
      " 2.9105e-06\n",
      " 7.3273e-02\n",
      " 6.0431e-05\n",
      " 1.6673e-01\n",
      " 5.8564e-01\n",
      "-1.0091e-06\n",
      " 4.4453e-01\n",
      " 1.9139e-01\n",
      "-1.7290e-01\n",
      " 4.4743e-01\n",
      " 2.1942e-06\n",
      " 4.1710e-04\n",
      "-2.8761e-06\n",
      " 6.9884e-06\n",
      " 4.2440e-01\n",
      " 3.9954e-07\n",
      " 3.3609e-01\n",
      " 2.3284e-01\n",
      " 2.0346e-01\n",
      " 3.2533e-05\n",
      " 1.9559e-05\n",
      " 1.3705e-01\n",
      " 4.0862e-01\n",
      " 2.1130e-01\n",
      " 2.3400e-01\n",
      " 3.4692e-02\n",
      " 3.5274e-02\n",
      " 1.5522e-02\n",
      " 5.3262e-02\n",
      " 3.3355e-01\n",
      " 8.4932e-06\n",
      " 3.6374e-01\n",
      " 6.2730e-04\n",
      " 6.0341e-01\n",
      " 1.1846e-01\n",
      " 2.2492e-05\n",
      " 1.9188e-03\n",
      "-1.3975e-02\n",
      " 2.9738e-06\n",
      " 1.7551e-01\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      "-9.3128e-02\n",
      "-5.0101e-01\n",
      "-1.4874e-05\n",
      "-7.0805e-02\n",
      "-1.5052e-01\n",
      "-1.2445e-02\n",
      "-3.9412e-03\n",
      "-6.6548e-05\n",
      "-1.2268e-04\n",
      "-6.5362e-01\n",
      " 8.3145e-02\n",
      "-1.1838e-05\n",
      " 4.5298e-01\n",
      "-6.8276e-02\n",
      "-1.3310e-04\n",
      " 2.1005e-01\n",
      "-6.1120e-01\n",
      " 1.7398e-01\n",
      " 4.0956e-01\n",
      "-7.0594e-02\n",
      "-1.1281e-01\n",
      "-4.0101e-05\n",
      "-2.8990e-02\n",
      " 3.8560e-01\n",
      "-2.3817e-05\n",
      "-5.2463e-02\n",
      "-1.3762e-04\n",
      " 2.9208e-01\n",
      " 4.5790e-01\n",
      "-1.1924e-05\n",
      " 2.1546e-01\n",
      "-2.0923e-01\n",
      "-7.9768e-02\n",
      " 2.2600e-01\n",
      "-1.4595e-05\n",
      "-2.3014e-03\n",
      "-9.3816e-06\n",
      "-1.9920e-05\n",
      " 2.2298e-01\n",
      "-2.7183e-06\n",
      "-4.4838e-01\n",
      " 5.7697e-01\n",
      "-4.7457e-03\n",
      "-9.4962e-05\n",
      "-9.4551e-05\n",
      "-8.4296e-02\n",
      "-2.8848e-01\n",
      "-2.0530e-01\n",
      " 5.8964e-01\n",
      "-6.0761e-02\n",
      "-3.2889e-02\n",
      "-2.6551e-02\n",
      "-5.6209e-02\n",
      "-4.6095e-01\n",
      "-1.9802e-05\n",
      "-4.3100e-01\n",
      "-2.1841e-03\n",
      "-6.7886e-01\n",
      "-1.1498e-01\n",
      "-1.0471e-04\n",
      "-6.7467e-03\n",
      " 6.1116e-02\n",
      "-1.0341e-05\n",
      " 3.2043e-01\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -3.1687e-04  9.2008e-03 -4.1393e-03\n",
      "  2.5380e-02  3.3246e-02  2.3549e-02\n",
      "  2.0415e-02  2.8614e-02  1.9561e-02\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  6.9795e-02  1.1353e-01  6.1226e-02\n",
      "  1.1009e-01  2.0142e-01  1.1042e-01\n",
      "  6.9220e-02  1.1124e-01  7.6604e-02\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  5.0746e-08  1.4080e-07  7.9115e-08\n",
      "  2.1648e-07  2.4772e-07  2.0241e-07\n",
      "  1.9082e-07  2.1866e-07  1.7150e-07\n",
      "   ...\n",
      "\n",
      "(0 ,61,.,.) = \n",
      " -9.5418e-04 -5.4759e-04 -1.0346e-03\n",
      " -1.3292e-03 -1.0198e-03 -1.3978e-03\n",
      " -2.0238e-03 -1.5585e-03 -2.0559e-03\n",
      "\n",
      "(0 ,62,.,.) = \n",
      "  7.2453e-08  8.9997e-08  7.9440e-09\n",
      "  9.8519e-08  1.6578e-07  5.7280e-08\n",
      "  8.8117e-08  1.2119e-07 -1.8201e-08\n",
      "\n",
      "(0 ,63,.,.) = \n",
      " -1.5851e-02 -1.0441e-02 -9.6404e-03\n",
      " -1.1900e-02 -2.9816e-03 -4.3425e-03\n",
      " -1.6189e-02 -8.1316e-03 -8.6842e-03\n",
      "     â‹® \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -9.3163e-03 -1.7730e-02 -1.9647e-02\n",
      " -1.2322e-02 -1.8934e-02 -2.3936e-02\n",
      " -9.7103e-03 -1.8752e-02 -2.9093e-02\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      " -3.3407e-04  2.2600e-02  2.9935e-02\n",
      " -1.1070e-02  1.7435e-02  3.3275e-02\n",
      " -1.8749e-02  7.1068e-03  2.0931e-02\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      "  3.2989e-07 -4.3301e-07 -5.0588e-07\n",
      "  5.1469e-07 -4.1246e-07 -7.7350e-07\n",
      "  4.9948e-07 -1.3377e-07 -5.4100e-07\n",
      "   ...\n",
      "\n",
      "(1 ,61,.,.) = \n",
      " -5.3626e-03 -9.2405e-03 -9.2927e-03\n",
      " -2.6815e-03 -6.5807e-03 -6.7688e-03\n",
      " -1.6992e-03 -5.1117e-03 -5.2699e-03\n",
      "\n",
      "(1 ,62,.,.) = \n",
      " -4.8396e-07 -7.4237e-07 -8.9872e-07\n",
      " -4.8875e-07 -7.5347e-07 -8.9965e-07\n",
      " -3.3570e-07 -5.7067e-07 -7.3355e-07\n",
      "\n",
      "(1 ,63,.,.) = \n",
      "  8.9034e-03  9.0486e-03  5.3425e-02\n",
      " -1.0063e-02 -1.2765e-02  3.8190e-02\n",
      " -8.0792e-03 -1.6299e-02  3.0199e-02\n",
      "     â‹® \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -8.7691e-04  3.9703e-04  5.8208e-03\n",
      " -1.2479e-02 -1.0666e-02 -7.2842e-03\n",
      " -2.5273e-02 -1.8921e-02 -1.5513e-02\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      "  1.3938e-02  1.8136e-02  1.4378e-02\n",
      "  1.8014e-02  1.8884e-02  1.4501e-02\n",
      "  1.0129e-02  1.3979e-02  1.2579e-02\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      "  6.2389e-08 -5.6536e-08 -3.5545e-08\n",
      " -1.6294e-07 -2.8903e-07 -1.4175e-07\n",
      " -3.2919e-08 -5.9533e-08  9.8579e-09\n",
      "   ...\n",
      "\n",
      "(2 ,61,.,.) = \n",
      "  3.6600e-03  3.6485e-03  4.3215e-03\n",
      "  1.9849e-03  2.1437e-03  2.8031e-03\n",
      "  1.5673e-03  1.7667e-03  2.4789e-03\n",
      "\n",
      "(2 ,62,.,.) = \n",
      "  3.2328e-07  3.6131e-07  3.2279e-07\n",
      "  3.5973e-07  4.4204e-07  4.1438e-07\n",
      "  3.8414e-07  4.2860e-07  3.6035e-07\n",
      "\n",
      "(2 ,63,.,.) = \n",
      " -3.8348e-02 -3.3698e-02 -4.7959e-02\n",
      " -2.4765e-02 -1.8905e-02 -3.3011e-02\n",
      " -2.1240e-02 -1.6367e-02 -3.0338e-02\n",
      "...   \n",
      "     â‹® \n",
      "\n",
      "(61,0 ,.,.) = \n",
      " -3.4783e-02 -3.6318e-02 -3.2563e-02\n",
      " -4.5068e-02 -4.4494e-02 -3.9024e-02\n",
      " -4.4996e-02 -4.6022e-02 -4.2578e-02\n",
      "\n",
      "(61,1 ,.,.) = \n",
      "  3.0575e-02  1.6404e-02  1.3138e-02\n",
      "  1.9889e-02  1.0564e-02  5.4449e-03\n",
      "  1.9371e-02  1.5184e-02  6.9697e-03\n",
      "\n",
      "(61,2 ,.,.) = \n",
      " -1.4501e-07 -1.4209e-07 -1.1747e-07\n",
      " -5.6741e-08 -6.8045e-08 -7.2951e-08\n",
      " -3.2316e-08 -3.8053e-08 -8.8467e-08\n",
      "   ...\n",
      "\n",
      "(61,61,.,.) = \n",
      "  8.3727e-05  1.8934e-03  3.1943e-03\n",
      " -4.2991e-04  1.4459e-03  2.6886e-03\n",
      "  1.6348e-03  3.5630e-03  4.7082e-03\n",
      "\n",
      "(61,62,.,.) = \n",
      " -1.8802e-08 -3.4320e-08 -5.1124e-08\n",
      " -2.5359e-08 -4.6603e-08 -5.9105e-08\n",
      " -4.9592e-08 -7.6563e-08 -6.8753e-08\n",
      "\n",
      "(61,63,.,.) = \n",
      " -9.5343e-02 -1.1535e-01 -7.4464e-02\n",
      " -1.2405e-01 -1.4785e-01 -1.0193e-01\n",
      " -7.1116e-02 -9.1655e-02 -4.8129e-02\n",
      "     â‹® \n",
      "\n",
      "(62,0 ,.,.) = \n",
      "  9.9307e-03 -1.1443e-02  2.1514e-03\n",
      " -8.0224e-03 -1.1739e-02  6.5765e-03\n",
      " -1.5765e-03 -5.3312e-03 -2.5980e-03\n",
      "\n",
      "(62,1 ,.,.) = \n",
      "  1.9963e-02  6.2013e-04 -2.6343e-02\n",
      "  2.4374e-02  7.4878e-03 -4.3597e-02\n",
      "  3.4510e-02  2.2573e-02 -2.8832e-02\n",
      "\n",
      "(62,2 ,.,.) = \n",
      " -2.5169e-07 -3.1343e-09  3.0158e-07\n",
      " -4.1344e-07 -1.5397e-07  4.0759e-07\n",
      " -5.2864e-08  4.5518e-08  3.4914e-07\n",
      "   ...\n",
      "\n",
      "(62,61,.,.) = \n",
      " -7.6767e-04 -9.8334e-05  1.6991e-03\n",
      " -2.9773e-03 -1.9660e-03 -4.4458e-04\n",
      " -2.6758e-03 -1.6267e-03 -3.7898e-04\n",
      "\n",
      "(62,62,.,.) = \n",
      "  2.7793e-07  5.3528e-07 -1.9376e-07\n",
      "  2.8915e-07  5.2619e-07 -2.5986e-07\n",
      "  2.3689e-07  3.7579e-07 -2.3970e-07\n",
      "\n",
      "(62,63,.,.) = \n",
      "  4.4154e-02  2.2987e-02  2.0103e-03\n",
      "  3.0756e-02 -6.6150e-03 -2.8364e-02\n",
      "  3.1916e-02 -6.6919e-03 -2.0433e-02\n",
      "     â‹® \n",
      "\n",
      "(63,0 ,.,.) = \n",
      "  4.8507e-02  5.3498e-02  4.8655e-02\n",
      "  1.6311e-03  1.4142e-02  8.9667e-03\n",
      " -2.4598e-02 -1.3986e-02 -1.8941e-02\n",
      "\n",
      "(63,1 ,.,.) = \n",
      " -4.6671e-02 -5.4787e-02 -5.5884e-02\n",
      "  3.9568e-02  4.7798e-02  1.9962e-02\n",
      "  6.5324e-02  9.9472e-02  5.1136e-02\n",
      "\n",
      "(63,2 ,.,.) = \n",
      "  1.1564e-06  1.2681e-06  9.5072e-07\n",
      " -3.2756e-07 -3.1356e-07 -2.5024e-07\n",
      " -1.4884e-06 -1.5527e-06 -1.2490e-06\n",
      "   ...\n",
      "\n",
      "(63,61,.,.) = \n",
      "  7.7883e-03  8.0076e-03  9.4325e-03\n",
      " -1.9020e-03 -2.1456e-03  1.1069e-04\n",
      " -6.2019e-03 -7.0953e-03 -4.4058e-03\n",
      "\n",
      "(63,62,.,.) = \n",
      "  8.4773e-07  7.6812e-07  7.1526e-07\n",
      "  5.9609e-08 -1.2737e-08  9.0013e-08\n",
      " -4.5088e-07 -4.0405e-07 -2.7113e-07\n",
      "\n",
      "(63,63,.,.) = \n",
      " -5.2971e-02 -7.8899e-02 -6.9748e-02\n",
      " -1.1066e-04 -1.7997e-02 -2.5264e-02\n",
      "  8.4192e-02  7.6318e-02  6.1213e-02\n",
      "[torch.FloatTensor of size 64x64x3x3]\n",
      "\n",
      "Parameter containing:\n",
      " 1.0274e-07\n",
      "-2.1620e-06\n",
      " 1.6119e-07\n",
      "-4.8117e-06\n",
      "-3.3856e-07\n",
      "-8.3249e-06\n",
      "-1.0947e-06\n",
      "-1.0679e-05\n",
      " 3.5612e-08\n",
      "-5.8685e-07\n",
      "-5.5354e-06\n",
      "-1.9366e-06\n",
      "-3.2250e-06\n",
      "-3.0011e-06\n",
      "-5.6798e-06\n",
      "-2.5092e-06\n",
      "-4.2109e-07\n",
      "-1.9885e-06\n",
      " 1.6101e-06\n",
      "-5.4363e-06\n",
      "-1.3502e-05\n",
      "-8.4464e-07\n",
      "-2.5892e-06\n",
      "-6.1020e-06\n",
      "-1.5837e-06\n",
      " 2.9279e-05\n",
      " 2.9632e-06\n",
      " 3.6816e-06\n",
      "-4.7990e-06\n",
      " 9.5714e-06\n",
      "-3.9380e-06\n",
      "-2.1725e-06\n",
      "-3.4293e-06\n",
      "-5.6248e-08\n",
      " 1.8081e-06\n",
      " 5.1210e-06\n",
      "-1.0822e-06\n",
      "-5.6090e-07\n",
      " 1.3219e-06\n",
      " 4.9164e-06\n",
      " 1.5121e-06\n",
      "-2.2998e-05\n",
      "-9.8426e-06\n",
      " 2.7048e-11\n",
      " 1.7769e-06\n",
      "-3.8923e-06\n",
      " 2.4520e-12\n",
      " 7.9814e-07\n",
      " 1.1222e-05\n",
      "-1.6759e-06\n",
      " 5.4139e-07\n",
      "-5.5356e-08\n",
      " 1.8710e-06\n",
      " 4.3739e-06\n",
      " 2.9721e-06\n",
      " 2.0469e-06\n",
      " 6.2706e-06\n",
      "-1.0834e-06\n",
      " 1.6183e-05\n",
      "-3.1801e-06\n",
      "-8.6288e-06\n",
      "-1.6993e-06\n",
      " 1.0233e-05\n",
      "-3.1451e-06\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      " 2.3055e-01\n",
      " 2.1686e-01\n",
      " 2.2532e-01\n",
      " 4.2643e-01\n",
      " 2.1964e-01\n",
      " 4.5784e-01\n",
      " 2.2973e-01\n",
      " 4.4448e-01\n",
      " 2.9997e-01\n",
      " 1.8322e-01\n",
      " 5.0692e-01\n",
      " 3.0100e-01\n",
      " 4.8251e-01\n",
      " 2.1835e-01\n",
      " 2.6077e-01\n",
      " 2.6645e-01\n",
      " 2.4489e-01\n",
      " 2.4953e-01\n",
      " 1.7737e-01\n",
      " 2.6715e-01\n",
      " 3.4460e-01\n",
      " 2.7725e-01\n",
      " 3.0442e-01\n",
      " 2.9500e-01\n",
      " 2.2831e-01\n",
      " 4.0756e-01\n",
      " 1.9665e-01\n",
      " 2.2962e-01\n",
      " 2.7667e-01\n",
      " 3.2992e-01\n",
      " 2.5880e-01\n",
      " 2.4771e-01\n",
      " 2.9929e-01\n",
      " 2.0024e-01\n",
      " 2.3436e-01\n",
      " 3.2613e-01\n",
      " 2.3405e-01\n",
      " 2.2728e-01\n",
      " 3.2198e-01\n",
      " 2.3106e-01\n",
      " 3.2744e-01\n",
      " 3.9775e-01\n",
      " 3.7753e-01\n",
      " 5.4077e-06\n",
      " 2.8514e-01\n",
      " 1.9967e-01\n",
      " 1.0410e-07\n",
      " 2.6059e-01\n",
      " 2.0736e-01\n",
      " 2.3205e-01\n",
      " 2.3321e-01\n",
      " 3.3300e-01\n",
      " 4.2012e-01\n",
      " 3.4278e-01\n",
      " 2.7487e-01\n",
      " 1.8870e-01\n",
      " 2.1679e-01\n",
      " 2.5635e-01\n",
      " 2.5877e-01\n",
      " 3.0985e-01\n",
      " 2.7984e-01\n",
      " 2.2003e-01\n",
      " 3.2800e-01\n",
      " 4.1262e-01\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      "-3.7062e-01\n",
      " 2.6295e-01\n",
      "-1.7605e-01\n",
      " 6.8571e-02\n",
      " 2.3052e-01\n",
      " 1.9489e-03\n",
      "-1.7513e-01\n",
      " 9.3355e-03\n",
      " 2.3135e-01\n",
      " 5.4623e-02\n",
      "-4.9197e-02\n",
      " 3.1020e-02\n",
      " 7.8682e-02\n",
      " 3.7721e-01\n",
      " 2.4486e-01\n",
      "-1.6066e-01\n",
      " 4.3116e-01\n",
      "-8.3087e-03\n",
      " 4.0230e-01\n",
      " 1.4394e-01\n",
      " 8.2120e-02\n",
      "-9.3815e-02\n",
      " 1.3599e-01\n",
      " 2.8395e-02\n",
      " 1.7725e-01\n",
      " 5.9483e-02\n",
      " 5.9693e-01\n",
      " 5.2901e-01\n",
      "-1.3571e-01\n",
      "-1.6225e-02\n",
      " 2.9824e-02\n",
      " 2.1963e-02\n",
      "-1.6131e-01\n",
      " 1.6303e-01\n",
      "-3.7094e-01\n",
      " 3.8517e-02\n",
      "-3.4366e-01\n",
      "-9.4353e-02\n",
      "-4.0492e-03\n",
      " 3.9091e-01\n",
      "-1.6492e-01\n",
      " 6.7708e-02\n",
      " 3.4767e-02\n",
      "-6.1258e-05\n",
      "-2.4348e-01\n",
      " 1.8170e-01\n",
      "-9.7656e-07\n",
      "-6.6097e-02\n",
      " 2.1083e-01\n",
      "-2.8826e-02\n",
      "-6.0244e-01\n",
      "-1.1723e-02\n",
      "-2.2677e-01\n",
      "-8.5106e-03\n",
      " 2.8603e-02\n",
      "-2.8826e-02\n",
      " 8.0614e-01\n",
      "-1.5546e-01\n",
      " 5.4273e-01\n",
      " 2.9896e-02\n",
      " 1.3769e-01\n",
      " 4.7820e-02\n",
      " 6.7074e-02\n",
      " 3.1224e-02\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -2.0672e-03  2.5674e-03 -8.2123e-04\n",
      "  1.6905e-03  5.6020e-03  6.3168e-04\n",
      "  1.6831e-03  9.4584e-03  5.9482e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -2.0416e-02  2.6421e-02  3.9049e-02\n",
      " -5.5559e-02 -1.8838e-02  3.3665e-04\n",
      " -6.4158e-02 -5.6599e-02 -4.6646e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -1.4476e-02 -3.0864e-02 -1.2178e-02\n",
      " -4.5818e-04 -4.1343e-03  3.9392e-03\n",
      "  1.7197e-02  1.7915e-02  1.1381e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,61 ,.,.) = \n",
      " -1.0279e-02 -3.6467e-03 -3.1352e-03\n",
      " -7.8919e-03 -1.4324e-02 -1.7872e-03\n",
      " -1.7932e-03 -1.0601e-02  7.2426e-04\n",
      "\n",
      "( 0 ,62 ,.,.) = \n",
      " -2.5662e-02 -2.5906e-02 -1.9927e-03\n",
      " -1.9894e-02  1.3529e-02  6.4999e-03\n",
      " -4.7948e-02 -1.3617e-02  1.6496e-02\n",
      "\n",
      "( 0 ,63 ,.,.) = \n",
      " -1.1058e-02 -3.4339e-02 -2.4041e-02\n",
      "  1.6165e-02 -5.3347e-02  5.1757e-03\n",
      "  2.6568e-02  1.7181e-02  2.9874e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  3.0372e-03 -2.4366e-03  7.8615e-03\n",
      " -8.5718e-04 -1.4945e-02 -4.5039e-03\n",
      "  1.9042e-03 -1.0755e-02 -8.4899e-03\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -5.1485e-02 -3.5197e-02 -1.6506e-02\n",
      " -2.6449e-02 -8.3799e-03  1.2144e-02\n",
      "  5.6413e-03  1.4015e-02  2.6593e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  9.4968e-03  6.4794e-03  4.9610e-03\n",
      "  1.1407e-02  4.3738e-03 -1.0503e-02\n",
      "  1.8774e-02  7.2685e-03 -9.1165e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,61 ,.,.) = \n",
      "  1.6784e-02  6.9003e-03  2.5801e-02\n",
      "  2.2569e-02  1.4439e-02  2.8842e-02\n",
      "  5.3091e-03  7.3576e-04  2.5020e-03\n",
      "\n",
      "( 1 ,62 ,.,.) = \n",
      " -1.0667e-02 -3.5296e-02 -2.2553e-03\n",
      "  3.5478e-03 -4.9600e-02 -2.9474e-02\n",
      " -5.0920e-02 -7.2972e-02 -2.6545e-02\n",
      "\n",
      "( 1 ,63 ,.,.) = \n",
      "  1.4756e-02  1.4417e-02  4.6485e-02\n",
      "  5.2972e-02  3.9179e-02  1.6038e-02\n",
      "  4.3446e-02  3.8773e-02  1.2047e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  1.4160e-03  6.3709e-04  1.5161e-03\n",
      "  5.2520e-04 -5.7870e-04  6.7542e-04\n",
      "  9.1210e-04  4.3226e-05  9.5178e-04\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -1.0596e-03 -8.1056e-04 -7.4745e-04\n",
      " -2.0429e-03 -2.2354e-03 -8.6366e-04\n",
      " -1.7272e-03 -2.1286e-03 -1.1452e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  7.8608e-04  6.5911e-04 -2.3905e-04\n",
      "  1.2784e-03  1.3383e-03  2.0505e-04\n",
      "  1.6084e-03  1.5256e-03  7.2223e-04\n",
      "    ... \n",
      "\n",
      "( 2 ,61 ,.,.) = \n",
      " -4.6325e-04 -1.7430e-03 -6.5802e-04\n",
      " -1.0170e-03 -2.6022e-03 -1.0706e-03\n",
      " -2.5908e-04 -1.4051e-03 -3.4398e-04\n",
      "\n",
      "( 2 ,62 ,.,.) = \n",
      " -1.4031e-03 -2.6658e-04  1.2947e-03\n",
      " -8.4647e-04  5.0792e-04  8.9018e-04\n",
      " -1.3913e-03 -1.7697e-04  5.5116e-04\n",
      "\n",
      "( 2 ,63 ,.,.) = \n",
      " -1.0895e-03 -2.0746e-03 -9.1785e-04\n",
      " -1.6172e-03 -2.2246e-03 -1.5093e-03\n",
      " -6.4469e-04 -5.3017e-05 -4.2301e-04\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(125, 0 ,.,.) = \n",
      " -1.6138e-02  4.0423e-03 -1.1889e-03\n",
      " -2.0278e-02  5.8775e-03  1.5332e-02\n",
      " -2.5744e-02  1.3270e-02  2.8739e-03\n",
      "\n",
      "(125, 1 ,.,.) = \n",
      " -4.5737e-02  8.1135e-03 -1.9345e-02\n",
      " -7.9115e-02  1.3464e-02  7.0970e-02\n",
      " -6.8451e-02 -4.8306e-03  4.9130e-02\n",
      "\n",
      "(125, 2 ,.,.) = \n",
      "  3.1771e-02  1.0805e-02 -4.5175e-03\n",
      "  3.6737e-02  1.4954e-02 -3.1685e-02\n",
      "  2.0444e-02  3.8733e-02  3.8703e-02\n",
      "    ... \n",
      "\n",
      "(125,61 ,.,.) = \n",
      " -1.1805e-02 -1.7288e-02  8.7127e-03\n",
      " -2.1283e-02 -4.8414e-02  2.1169e-02\n",
      " -4.9926e-03 -4.6989e-03  2.2400e-02\n",
      "\n",
      "(125,62 ,.,.) = \n",
      " -1.1741e-01  2.7597e-03  5.9594e-02\n",
      " -2.2764e-02 -1.6663e-01  1.3157e-02\n",
      " -2.5623e-02  2.9366e-03  7.8335e-02\n",
      "\n",
      "(125,63 ,.,.) = \n",
      "  5.1190e-02  5.6033e-03  2.8041e-02\n",
      "  6.8527e-03 -7.7303e-02  3.4446e-02\n",
      "  3.8464e-02  2.9395e-02  1.0033e-02\n",
      "      â‹®  \n",
      "\n",
      "(126, 0 ,.,.) = \n",
      " -1.2704e-02 -1.5267e-02 -2.7453e-02\n",
      " -7.4039e-03 -4.9990e-03 -1.5462e-02\n",
      " -1.5814e-03  1.1127e-03 -9.4775e-03\n",
      "\n",
      "(126, 1 ,.,.) = \n",
      " -3.9100e-03  4.7484e-03  1.3672e-02\n",
      " -4.5717e-02 -3.7467e-02 -6.6261e-03\n",
      " -5.3098e-02 -4.8997e-02 -2.0914e-02\n",
      "\n",
      "(126, 2 ,.,.) = \n",
      " -3.3576e-02 -3.1464e-02 -3.0477e-02\n",
      " -1.0869e-02 -1.3689e-02 -1.0271e-02\n",
      " -1.6619e-02 -2.1555e-02 -1.5186e-02\n",
      "    ... \n",
      "\n",
      "(126,61 ,.,.) = \n",
      " -3.6007e-02 -2.3241e-02 -4.2688e-02\n",
      " -1.4193e-02  1.0207e-02 -9.2449e-03\n",
      " -2.5373e-02  2.5031e-03 -1.1182e-02\n",
      "\n",
      "(126,62 ,.,.) = \n",
      " -3.5571e-02 -2.5406e-02 -1.4846e-02\n",
      "  4.2270e-04  5.7745e-03 -5.7657e-03\n",
      " -2.1703e-02 -1.6463e-02  7.7838e-04\n",
      "\n",
      "(126,63 ,.,.) = \n",
      "  7.6314e-03 -2.2272e-02  1.5970e-02\n",
      "  1.2521e-02  4.3690e-04  1.7610e-02\n",
      "  3.7398e-02  4.5797e-02  4.0176e-02\n",
      "      â‹®  \n",
      "\n",
      "(127, 0 ,.,.) = \n",
      " -8.6455e-03  2.2910e-02 -2.1552e-02\n",
      " -5.6681e-02 -2.3504e-02  2.3508e-02\n",
      " -3.1436e-02 -2.5876e-02  3.8993e-03\n",
      "\n",
      "(127, 1 ,.,.) = \n",
      " -2.6675e-02  7.6358e-02  6.6590e-02\n",
      "  3.1977e-02 -8.9016e-03 -4.5789e-02\n",
      "  7.6452e-03 -1.1744e-02 -3.1159e-02\n",
      "\n",
      "(127, 2 ,.,.) = \n",
      " -7.5351e-03 -2.5156e-02 -2.4442e-03\n",
      "  2.7537e-02 -3.2334e-02 -1.5688e-02\n",
      " -1.7720e-02  5.2083e-03 -1.8513e-02\n",
      "    ... \n",
      "\n",
      "(127,61 ,.,.) = \n",
      "  8.0883e-03 -3.5549e-02  6.0849e-03\n",
      "  4.2641e-02 -1.5166e-02  1.9979e-02\n",
      "  4.1691e-02 -5.7005e-04  3.9474e-03\n",
      "\n",
      "(127,62 ,.,.) = \n",
      " -5.9990e-02  7.3944e-02 -1.1503e-02\n",
      " -1.7334e-01  1.0566e-01 -1.9893e-02\n",
      " -2.8086e-02 -9.0419e-04  2.4082e-02\n",
      "\n",
      "(127,63 ,.,.) = \n",
      "  3.1020e-02  9.9124e-02  4.8760e-03\n",
      " -2.4479e-04  8.0535e-02  1.5998e-02\n",
      "  8.0483e-02  4.7323e-02  1.2034e-02\n",
      "[torch.FloatTensor of size 128x64x3x3]\n",
      "\n",
      "Parameter containing:\n",
      " 8.1684e-08\n",
      "-8.9533e-08\n",
      "-2.2585e-09\n",
      "-1.1890e-07\n",
      "-6.2678e-09\n",
      " 6.8076e-08\n",
      " 3.3775e-07\n",
      " 1.2497e-07\n",
      "-2.4026e-07\n",
      "-9.3520e-09\n",
      "-3.8737e-08\n",
      " 2.2414e-12\n",
      "-4.5920e-07\n",
      " 4.4784e-09\n",
      "-1.9286e-11\n",
      "-5.4712e-12\n",
      "-3.6055e-07\n",
      "-2.9518e-07\n",
      " 5.2773e-09\n",
      " 7.0027e-08\n",
      "-1.2660e-07\n",
      "-3.7252e-09\n",
      " 2.2451e-08\n",
      "-2.6759e-07\n",
      "-9.2210e-08\n",
      "-1.0686e-08\n",
      "-2.1665e-12\n",
      " 8.3816e-08\n",
      " 9.5176e-08\n",
      "-3.3113e-08\n",
      "-1.0645e-07\n",
      " 3.3417e-07\n",
      " 4.5273e-07\n",
      "-8.4234e-07\n",
      "-1.1013e-07\n",
      "-1.5436e-07\n",
      " 2.7901e-07\n",
      " 2.8947e-07\n",
      " 1.4251e-14\n",
      " 1.4201e-07\n",
      "-1.5105e-07\n",
      " 2.3387e-07\n",
      " 8.3078e-08\n",
      " 1.4567e-12\n",
      " 5.5169e-07\n",
      " 8.1978e-12\n",
      " 7.7368e-09\n",
      "-1.5182e-07\n",
      " 2.3423e-10\n",
      "-1.8632e-08\n",
      " 7.9720e-09\n",
      "-2.7897e-08\n",
      " 5.1228e-08\n",
      "-1.6681e-08\n",
      " 4.3482e-07\n",
      " 2.6831e-09\n",
      "-1.3934e-07\n",
      " 3.1743e-07\n",
      "-6.2643e-08\n",
      " 3.1040e-07\n",
      "-2.2338e-07\n",
      " 1.6164e-07\n",
      "-1.8021e-08\n",
      "-2.0823e-07\n",
      " 1.8511e-07\n",
      " 3.0678e-08\n",
      "-6.0503e-12\n",
      " 2.2810e-07\n",
      "-7.5208e-08\n",
      "-3.6513e-07\n",
      " 2.7936e-07\n",
      " 3.8305e-12\n",
      "-8.2519e-08\n",
      " 1.3687e-07\n",
      " 5.1062e-08\n",
      " 9.5195e-12\n",
      " 8.9080e-12\n",
      " 1.2776e-09\n",
      "-6.3464e-09\n",
      " 1.3652e-07\n",
      " 4.8604e-12\n",
      "-5.3947e-07\n",
      "-1.7686e-08\n",
      "-7.7479e-09\n",
      " 4.6954e-12\n",
      " 6.5687e-08\n",
      " 1.3834e-08\n",
      " 3.2568e-07\n",
      "-2.8991e-08\n",
      " 5.5866e-07\n",
      "-6.4197e-08\n",
      "-4.3543e-07\n",
      " 1.0348e-11\n",
      "-9.1774e-08\n",
      "-2.3048e-07\n",
      " 7.4038e-08\n",
      " 4.3866e-07\n",
      "-4.0706e-07\n",
      "-1.3133e-07\n",
      "-6.6801e-08\n",
      " 8.0323e-08\n",
      " 1.8593e-07\n",
      " 1.2897e-11\n",
      "-3.5476e-08\n",
      "-9.0569e-10\n",
      "-4.5256e-07\n",
      " 3.5446e-12\n",
      "-1.4720e-07\n",
      " 2.9955e-09\n",
      " 1.1503e-07\n",
      "-7.1991e-08\n",
      " 5.8882e-15\n",
      " 1.2999e-07\n",
      " 2.1539e-07\n",
      " 3.6542e-07\n",
      "-6.8343e-12\n",
      "-1.9941e-07\n",
      "-1.9079e-07\n",
      "-1.0040e-07\n",
      " 8.9469e-07\n",
      " 1.4363e-07\n",
      " 3.7731e-08\n",
      "-7.2101e-10\n",
      " 1.8937e-07\n",
      " 2.4545e-07\n",
      " 1.4294e-07\n",
      " 2.6184e-08\n",
      "-1.3952e-07\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      " 2.6590e-01\n",
      " 2.9366e-01\n",
      "-5.9801e-03\n",
      " 1.9862e-01\n",
      " 4.2509e-01\n",
      " 2.4951e-01\n",
      " 2.2022e-01\n",
      " 2.0049e-01\n",
      " 2.3356e-01\n",
      " 2.1902e-01\n",
      " 8.5906e-02\n",
      " 2.1768e-05\n",
      " 5.0767e-01\n",
      " 1.1281e-01\n",
      " 9.4283e-06\n",
      "-1.3488e-06\n",
      " 3.1121e-01\n",
      " 2.9108e-01\n",
      " 2.9899e-01\n",
      " 2.3082e-01\n",
      " 1.9598e-01\n",
      " 2.6191e-01\n",
      " 2.7187e-01\n",
      " 2.2963e-01\n",
      " 1.4143e-01\n",
      " 1.4752e-01\n",
      " 2.3622e-06\n",
      " 4.4442e-01\n",
      " 4.4354e-01\n",
      " 3.3252e-01\n",
      " 3.2728e-01\n",
      " 2.2405e-01\n",
      " 4.2871e-01\n",
      " 6.1500e-01\n",
      " 3.7076e-01\n",
      " 2.8651e-01\n",
      " 2.8718e-01\n",
      " 3.5565e-01\n",
      " 3.7415e-09\n",
      " 1.6530e-01\n",
      " 1.9170e-01\n",
      " 4.9050e-01\n",
      " 3.9886e-01\n",
      " 6.0868e-06\n",
      " 3.0507e-01\n",
      " 2.5082e-06\n",
      " 2.4893e-01\n",
      " 1.9287e-01\n",
      " 2.2351e-04\n",
      "-5.7647e-03\n",
      " 4.5144e-01\n",
      " 3.0863e-01\n",
      " 3.6260e-01\n",
      " 1.9885e-01\n",
      " 5.1314e-01\n",
      " 3.0106e-03\n",
      " 4.6276e-01\n",
      " 2.3452e-01\n",
      " 2.5091e-01\n",
      " 2.1134e-01\n",
      " 2.2740e-01\n",
      " 2.2448e-01\n",
      " 4.0467e-01\n",
      " 2.1540e-01\n",
      " 2.9179e-01\n",
      " 4.6918e-01\n",
      " 2.5318e-06\n",
      " 1.9792e-01\n",
      " 2.6697e-01\n",
      " 2.4671e-01\n",
      " 3.4885e-01\n",
      "-8.7444e-07\n",
      " 2.1735e-01\n",
      " 3.9791e-01\n",
      " 2.1584e-01\n",
      " 1.5625e-05\n",
      "-5.2853e-06\n",
      " 1.0196e-01\n",
      " 2.2527e-01\n",
      " 4.3080e-01\n",
      "-6.9320e-07\n",
      " 2.6463e-01\n",
      " 6.0619e-01\n",
      " 1.8988e-01\n",
      " 1.4211e-05\n",
      " 8.8276e-02\n",
      "-9.0882e-02\n",
      " 5.0708e-01\n",
      " 4.1961e-01\n",
      " 4.0101e-01\n",
      " 1.4362e-01\n",
      " 2.5917e-01\n",
      " 6.1042e-06\n",
      " 2.5609e-01\n",
      " 3.4374e-01\n",
      " 2.1773e-01\n",
      " 4.1411e-01\n",
      " 3.9232e-01\n",
      " 2.5215e-01\n",
      " 1.8053e-01\n",
      " 3.6857e-01\n",
      " 2.7072e-01\n",
      "-1.8765e-05\n",
      " 3.2016e-01\n",
      " 3.5700e-01\n",
      " 2.9844e-01\n",
      "-1.1489e-05\n",
      " 2.6040e-01\n",
      "-7.4752e-03\n",
      " 1.7565e-01\n",
      " 1.5956e-01\n",
      " 3.9392e-08\n",
      " 4.4305e-01\n",
      " 2.2700e-01\n",
      " 3.7794e-01\n",
      " 3.2736e-06\n",
      " 3.9085e-01\n",
      " 2.4329e-01\n",
      " 3.9741e-01\n",
      " 3.6095e-01\n",
      " 1.7665e-01\n",
      " 1.1249e-01\n",
      " 3.4060e-04\n",
      " 4.4315e-01\n",
      " 3.7515e-01\n",
      " 4.0162e-01\n",
      " 1.6203e-01\n",
      " 2.5353e-01\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      "-1.6783e-01\n",
      "-2.9352e-02\n",
      " 1.4682e-02\n",
      "-5.7444e-03\n",
      "-9.8041e-02\n",
      " 1.9740e-01\n",
      "-3.4789e-02\n",
      " 1.9480e-01\n",
      " 3.1217e-01\n",
      "-2.2841e-01\n",
      " 4.6363e-02\n",
      "-1.0571e-04\n",
      "-2.7509e-01\n",
      " 3.2549e-02\n",
      "-5.5123e-05\n",
      "-8.7175e-06\n",
      " 1.4331e-01\n",
      " 1.4114e-01\n",
      "-1.4204e-01\n",
      "-8.9239e-02\n",
      "-7.0118e-02\n",
      " 2.1178e-01\n",
      " 3.9743e-02\n",
      "-2.2071e-02\n",
      " 1.4192e-02\n",
      " 3.3850e-02\n",
      "-1.8646e-05\n",
      "-8.1130e-02\n",
      "-3.0594e-01\n",
      "-1.5143e-01\n",
      "-1.1430e-01\n",
      " 2.4554e-01\n",
      "-3.0658e-02\n",
      "-2.9465e-01\n",
      "-9.7674e-02\n",
      " 1.7417e-01\n",
      " 1.3771e-01\n",
      "-4.2206e-02\n",
      "-7.4147e-08\n",
      " 2.2182e-04\n",
      "-1.9738e-02\n",
      "-7.8429e-02\n",
      "-2.4935e-01\n",
      "-4.6600e-05\n",
      " 1.1811e-01\n",
      "-1.9487e-05\n",
      " 2.6518e-01\n",
      "-4.2089e-02\n",
      "-8.6214e-04\n",
      "-4.4022e-02\n",
      "-1.2481e-01\n",
      "-2.1262e-01\n",
      "-1.9435e-01\n",
      "-1.5215e-01\n",
      "-1.1536e-01\n",
      "-7.2456e-03\n",
      "-8.0735e-02\n",
      " 1.8415e-01\n",
      " 9.6662e-02\n",
      " 3.2861e-03\n",
      " 2.6174e-01\n",
      " 1.2225e-01\n",
      "-8.3158e-02\n",
      " 3.2406e-01\n",
      "-1.0495e-01\n",
      "-8.1584e-02\n",
      "-1.5266e-05\n",
      "-2.8705e-02\n",
      " 3.0495e-02\n",
      " 1.4506e-01\n",
      "-3.3710e-02\n",
      "-1.1330e-05\n",
      " 2.0698e-01\n",
      "-4.2866e-02\n",
      "-1.4307e-01\n",
      "-1.2705e-04\n",
      "-6.3263e-05\n",
      "-7.9506e-03\n",
      " 1.0331e-01\n",
      "-9.8214e-02\n",
      "-2.5302e-05\n",
      " 2.1105e-01\n",
      "-3.1540e-01\n",
      "-1.7072e-01\n",
      "-6.6865e-05\n",
      " 8.8716e-02\n",
      "-1.0292e-01\n",
      "-2.6878e-01\n",
      "-7.0940e-02\n",
      "-3.5010e-02\n",
      "-5.6708e-03\n",
      "-1.5982e-01\n",
      "-4.0699e-05\n",
      "-1.7312e-01\n",
      " 5.9688e-02\n",
      " 5.9107e-02\n",
      "-8.5664e-02\n",
      "-3.6508e-02\n",
      "-2.1011e-01\n",
      "-1.1894e-01\n",
      " 8.8905e-02\n",
      " 2.9686e-01\n",
      "-6.2864e-05\n",
      "-1.5736e-02\n",
      "-3.0118e-02\n",
      "-7.0681e-03\n",
      "-1.2049e-04\n",
      "-1.5004e-01\n",
      "-2.8960e-02\n",
      " 6.1251e-02\n",
      " 6.9138e-02\n",
      "-2.7762e-07\n",
      "-1.9771e-02\n",
      " 2.2929e-01\n",
      "-6.1670e-02\n",
      "-1.7419e-05\n",
      "-8.2710e-02\n",
      " 2.5161e-01\n",
      " 2.4825e-02\n",
      " 9.6452e-02\n",
      " 2.9024e-02\n",
      " 5.0837e-02\n",
      "-3.4891e-03\n",
      "-7.6470e-02\n",
      "-3.8854e-02\n",
      "-8.3315e-02\n",
      "-3.3910e-03\n",
      " 3.8018e-01\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -1.8873e-02 -4.4297e-03 -1.5467e-02\n",
      " -7.1413e-03  6.6069e-03  1.2069e-03\n",
      "  2.1076e-02  3.7473e-02  2.7034e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -4.7433e-02 -3.8847e-02 -4.7795e-02\n",
      " -4.0390e-02 -4.3613e-02 -4.5789e-02\n",
      " -3.5298e-02 -4.9051e-02 -3.9467e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  3.5087e-03  4.6049e-03  3.7498e-03\n",
      "  4.3110e-03  5.2080e-03  4.4487e-03\n",
      "  3.7480e-03  4.4963e-03  3.9133e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "  9.1840e-03  1.2716e-02 -1.1465e-02\n",
      " -1.6219e-02 -1.6585e-03 -2.3758e-02\n",
      " -1.0478e-02 -8.2540e-03 -9.3203e-03\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      " -3.6848e-02 -3.0346e-02 -3.2456e-02\n",
      " -3.2444e-02 -2.3372e-02 -2.8623e-02\n",
      " -2.6149e-02 -1.8996e-02 -2.2660e-02\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      " -6.3091e-03 -4.2517e-02  2.6029e-02\n",
      " -1.2990e-02  7.4808e-04 -1.7516e-02\n",
      " -8.6788e-03  1.1817e-03 -3.3894e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  1.1237e-02 -7.0737e-03 -2.2872e-02\n",
      "  1.3103e-02 -9.2888e-03 -2.8771e-02\n",
      "  3.7089e-02 -6.6387e-04 -2.5153e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  4.4798e-02  2.5831e-02  1.1738e-02\n",
      "  5.4090e-02  5.9002e-02  3.7150e-02\n",
      "  1.9908e-02  2.3023e-02  1.4909e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  8.5938e-04  3.6418e-04  3.3680e-04\n",
      "  7.0066e-04  1.0005e-04  8.8702e-05\n",
      "  7.1803e-04  1.2972e-04 -6.1254e-05\n",
      "    ... \n",
      "\n",
      "( 1 ,125,.,.) = \n",
      "  6.0177e-02  4.6260e-02  1.4917e-02\n",
      "  6.0906e-02  6.4227e-02  2.7640e-02\n",
      "  2.2716e-02  1.8080e-02 -1.5589e-02\n",
      "\n",
      "( 1 ,126,.,.) = \n",
      "  2.2623e-02  1.0834e-02 -8.7236e-03\n",
      "  3.2558e-02  2.1849e-02  5.5680e-03\n",
      "  3.7445e-02  1.8781e-02  5.2671e-03\n",
      "\n",
      "( 1 ,127,.,.) = \n",
      " -2.4911e-02  6.4511e-03 -1.0953e-03\n",
      " -2.4660e-02  1.8040e-02  1.2709e-02\n",
      "  5.9592e-03 -1.7427e-02  6.3173e-03\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  2.2756e-03  2.4016e-02 -3.1079e-03\n",
      " -1.2649e-02  1.9357e-02 -1.0561e-02\n",
      " -2.7325e-03 -1.1213e-03 -3.5399e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  1.6196e-02  1.4546e-02 -1.7344e-02\n",
      "  1.8989e-04 -1.2417e-02 -2.7331e-02\n",
      "  1.8612e-02 -4.9167e-03 -2.0888e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -4.6847e-04  9.3225e-05  7.4809e-04\n",
      " -1.1863e-03 -8.2066e-04 -1.2262e-04\n",
      " -1.8580e-03 -1.5272e-03 -1.1391e-03\n",
      "    ... \n",
      "\n",
      "( 2 ,125,.,.) = \n",
      "  8.6582e-03  2.6541e-02  1.4090e-02\n",
      "  2.6447e-02  2.9821e-02  3.0485e-02\n",
      " -6.3158e-03 -1.1088e-02 -1.5661e-04\n",
      "\n",
      "( 2 ,126,.,.) = \n",
      "  2.5673e-02 -1.0502e-02 -5.2309e-02\n",
      "  4.7157e-02  3.3761e-02 -2.0603e-02\n",
      "  5.6310e-02  4.8618e-02  1.6979e-02\n",
      "\n",
      "( 2 ,127,.,.) = \n",
      " -1.1442e-02 -1.6963e-03 -6.6134e-03\n",
      "  2.6365e-02  2.6824e-02 -2.8211e-02\n",
      " -3.8191e-03  1.8245e-02 -3.9013e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(125, 0 ,.,.) = \n",
      "  2.2144e-03  5.5448e-03  3.3890e-03\n",
      "  3.8322e-04 -1.2118e-02 -1.8522e-02\n",
      "  3.9593e-04 -1.5610e-02 -1.6386e-02\n",
      "\n",
      "(125, 1 ,.,.) = \n",
      "  2.8437e-02  5.2687e-02  5.0677e-03\n",
      "  1.8859e-02  2.4620e-02 -6.7895e-03\n",
      " -2.0315e-02  3.6607e-03  8.6925e-03\n",
      "\n",
      "(125, 2 ,.,.) = \n",
      " -2.4093e-03 -2.2225e-03 -4.8684e-04\n",
      " -9.5171e-04 -3.6794e-04  9.7417e-04\n",
      "  2.6999e-04  1.0043e-03  1.9033e-03\n",
      "    ... \n",
      "\n",
      "(125,125,.,.) = \n",
      "  5.6781e-02  6.0242e-02  1.2045e-02\n",
      "  3.6439e-02  5.5174e-02  3.6998e-03\n",
      "  1.1735e-02  2.6878e-02 -2.7009e-03\n",
      "\n",
      "(125,126,.,.) = \n",
      "  2.8545e-02  9.9830e-03 -1.1811e-02\n",
      "  2.3685e-02 -8.5873e-03 -3.3265e-02\n",
      "  7.3937e-03 -2.8123e-02 -4.4455e-02\n",
      "\n",
      "(125,127,.,.) = \n",
      "  2.4107e-02  1.1157e-02  5.6479e-03\n",
      "  2.0700e-02 -6.1454e-03 -1.0814e-02\n",
      "  1.9134e-03  3.4976e-03 -2.1219e-02\n",
      "      â‹®  \n",
      "\n",
      "(126, 0 ,.,.) = \n",
      " -1.8832e-02 -1.1944e-02 -7.4025e-03\n",
      " -1.3132e-02 -2.1600e-02 -2.8209e-02\n",
      "  1.3904e-03  1.8664e-03 -2.5416e-02\n",
      "\n",
      "(126, 1 ,.,.) = \n",
      "  7.9054e-03 -6.6045e-03 -1.7197e-03\n",
      "  3.5493e-02 -2.4661e-02 -1.1316e-02\n",
      "  5.1493e-03 -2.1428e-02  3.9082e-02\n",
      "\n",
      "(126, 2 ,.,.) = \n",
      " -1.0064e-03  8.0587e-04 -2.9440e-04\n",
      " -9.6539e-04  1.5782e-05 -3.4634e-04\n",
      " -4.2449e-04 -5.1886e-04 -1.0521e-03\n",
      "    ... \n",
      "\n",
      "(126,125,.,.) = \n",
      "  8.6319e-03  2.0422e-02  2.0204e-02\n",
      "  1.8999e-02 -4.1446e-02  2.4982e-02\n",
      "  4.3705e-03 -3.2072e-02 -1.0658e-02\n",
      "\n",
      "(126,126,.,.) = \n",
      " -5.2783e-03  1.5981e-02  2.6873e-02\n",
      "  1.1787e-03  2.2926e-02  1.7071e-02\n",
      "  2.8696e-03  7.7335e-03  1.6311e-03\n",
      "\n",
      "(126,127,.,.) = \n",
      " -1.6593e-02 -4.9452e-02 -3.1273e-02\n",
      "  1.6059e-02  4.1485e-02 -2.5255e-02\n",
      " -2.9783e-02  3.7819e-02 -3.4725e-04\n",
      "      â‹®  \n",
      "\n",
      "(127, 0 ,.,.) = \n",
      " -9.9152e-03 -1.2019e-02 -1.4598e-02\n",
      " -1.7166e-03 -1.4894e-03  1.9187e-03\n",
      "  1.1379e-02 -1.9035e-02  9.3802e-03\n",
      "\n",
      "(127, 1 ,.,.) = \n",
      "  3.5575e-02  2.6305e-02  2.3158e-02\n",
      "  3.7512e-02  4.2377e-02  1.9305e-02\n",
      " -1.8469e-03  1.3705e-02  8.1172e-03\n",
      "\n",
      "(127, 2 ,.,.) = \n",
      " -9.6805e-04 -4.8622e-04 -8.3853e-04\n",
      "  1.1625e-04  8.5144e-04  7.2750e-04\n",
      "  1.2782e-03  2.7458e-03  2.0550e-03\n",
      "    ... \n",
      "\n",
      "(127,125,.,.) = \n",
      " -1.2064e-02 -5.8487e-03 -3.0116e-02\n",
      "  4.0749e-02  1.0348e-02  1.5233e-04\n",
      "  2.3506e-02  2.2570e-02  7.2417e-03\n",
      "\n",
      "(127,126,.,.) = \n",
      "  1.3103e-02  5.2413e-04  1.9837e-03\n",
      "  3.6191e-03 -4.8241e-03 -2.1113e-03\n",
      "  1.1597e-02  1.0580e-02  1.2426e-02\n",
      "\n",
      "(127,127,.,.) = \n",
      "  2.1025e-03 -1.2759e-02  5.8132e-03\n",
      "  2.0138e-03 -8.1720e-03  5.3576e-02\n",
      " -9.1612e-03 -4.4193e-02 -8.3972e-03\n",
      "[torch.FloatTensor of size 128x128x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "-1.8294e-07\n",
      " 3.4537e-07\n",
      "-3.0373e-07\n",
      " 1.2800e-06\n",
      " 2.4887e-07\n",
      " 1.3976e-06\n",
      "-6.0563e-07\n",
      "-3.5959e-07\n",
      " 6.9051e-07\n",
      " 3.8033e-07\n",
      "-3.9146e-07\n",
      " 3.5494e-07\n",
      " 1.2085e-07\n",
      "-6.2713e-07\n",
      " 4.3247e-07\n",
      " 2.7122e-07\n",
      "-1.1495e-12\n",
      "-4.2857e-07\n",
      "-6.9261e-07\n",
      "-4.2634e-08\n",
      " 4.5255e-07\n",
      " 3.5997e-07\n",
      "-3.9588e-07\n",
      " 1.1835e-07\n",
      "-5.9755e-07\n",
      "-1.8713e-07\n",
      "-5.0099e-07\n",
      "-3.5451e-07\n",
      "-6.3401e-08\n",
      "-4.9677e-07\n",
      " 2.4598e-07\n",
      " 1.1742e-07\n",
      "-3.6479e-07\n",
      "-3.1929e-07\n",
      "-2.4782e-07\n",
      " 3.3023e-07\n",
      "-1.2695e-07\n",
      " 1.0265e-07\n",
      "-3.6173e-07\n",
      "-1.2558e-07\n",
      "-5.1195e-07\n",
      " 5.8304e-07\n",
      "-2.1916e-07\n",
      " 1.1750e-07\n",
      "-1.8439e-07\n",
      "-2.7054e-07\n",
      "-9.5370e-08\n",
      "-4.1920e-07\n",
      " 4.1307e-08\n",
      " 6.0893e-07\n",
      "-4.8254e-07\n",
      "-4.4868e-07\n",
      " 6.5435e-07\n",
      "-8.2637e-08\n",
      " 3.1365e-07\n",
      "-2.9091e-07\n",
      "-3.5942e-07\n",
      " 8.2118e-11\n",
      "-1.9016e-07\n",
      " 1.9337e-07\n",
      "-5.7973e-08\n",
      "-2.5732e-07\n",
      " 2.3608e-07\n",
      " 4.0925e-07\n",
      "-2.5570e-07\n",
      " 1.4430e-08\n",
      "-6.6178e-07\n",
      " 5.3171e-08\n",
      " 4.4947e-08\n",
      "-1.1832e-07\n",
      " 4.3711e-07\n",
      "-2.3532e-07\n",
      " 4.4630e-07\n",
      "-2.7029e-07\n",
      " 2.4307e-07\n",
      "-1.6881e-07\n",
      " 5.9064e-08\n",
      "-3.7712e-07\n",
      " 1.6668e-07\n",
      " 4.1248e-07\n",
      "-2.2260e-07\n",
      " 5.0940e-07\n",
      "-1.8343e-07\n",
      " 6.8250e-08\n",
      " 1.5425e-07\n",
      "-1.0089e-06\n",
      " 1.7339e-08\n",
      "-4.1648e-07\n",
      " 3.3367e-08\n",
      "-5.7528e-08\n",
      " 2.8490e-07\n",
      " 2.3166e-07\n",
      " 1.5777e-07\n",
      "-1.6364e-07\n",
      " 2.8782e-07\n",
      "-3.2194e-07\n",
      "-6.8538e-07\n",
      " 1.4365e-07\n",
      " 3.1701e-07\n",
      "-1.1389e-07\n",
      " 5.0698e-08\n",
      " 5.6677e-07\n",
      "-4.2909e-07\n",
      "-2.6086e-07\n",
      "-4.5247e-07\n",
      "-5.4835e-16\n",
      " 5.6095e-07\n",
      " 3.8542e-07\n",
      "-2.6654e-07\n",
      "-1.7913e-07\n",
      "-6.0098e-07\n",
      " 8.3170e-07\n",
      " 4.6265e-07\n",
      " 6.4348e-07\n",
      "-1.0361e-07\n",
      "-7.4248e-07\n",
      " 9.9469e-07\n",
      " 5.9328e-08\n",
      "-1.0659e-08\n",
      "-9.5011e-12\n",
      "-4.3437e-07\n",
      "-1.2132e-07\n",
      " 4.8845e-08\n",
      " 1.4007e-13\n",
      "-2.3356e-08\n",
      "-2.2353e-07\n",
      "-8.5303e-07\n",
      " 2.8906e-07\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      " 2.0906e-01\n",
      "-2.9177e-01\n",
      " 3.1751e-01\n",
      " 3.1838e-01\n",
      " 2.9385e-01\n",
      " 4.1527e-01\n",
      " 2.9391e-01\n",
      " 2.8751e-01\n",
      " 2.7667e-01\n",
      " 2.7676e-01\n",
      " 2.5683e-01\n",
      " 3.4663e-01\n",
      " 1.6107e-01\n",
      " 3.3218e-01\n",
      " 3.1161e-01\n",
      " 3.2931e-01\n",
      "-4.2960e-09\n",
      " 2.9499e-01\n",
      " 4.2003e-01\n",
      " 3.8381e-01\n",
      " 2.8108e-01\n",
      " 3.8000e-01\n",
      " 2.7337e-01\n",
      " 4.5987e-01\n",
      " 3.8888e-01\n",
      " 2.8957e-01\n",
      " 2.0737e-01\n",
      " 2.3514e-01\n",
      " 1.7624e-01\n",
      " 3.2717e-01\n",
      " 4.8013e-01\n",
      " 2.9394e-01\n",
      " 3.0171e-01\n",
      " 2.5671e-01\n",
      " 2.9962e-01\n",
      " 3.9049e-01\n",
      " 2.5693e-01\n",
      " 2.4934e-01\n",
      " 2.4387e-01\n",
      " 2.8101e-01\n",
      " 2.8725e-01\n",
      " 3.0734e-01\n",
      " 2.8882e-01\n",
      " 3.1433e-01\n",
      " 3.1143e-01\n",
      " 2.3197e-01\n",
      " 2.0764e-01\n",
      " 2.7515e-01\n",
      " 3.2672e-01\n",
      " 3.1688e-01\n",
      " 2.8818e-01\n",
      " 2.2101e-01\n",
      " 3.5532e-01\n",
      " 3.1271e-01\n",
      " 2.5801e-01\n",
      " 1.9546e-01\n",
      " 2.4388e-01\n",
      "-5.8016e-06\n",
      " 3.9375e-01\n",
      " 3.2999e-01\n",
      " 3.6367e-01\n",
      " 2.8448e-01\n",
      " 2.2981e-01\n",
      " 3.5136e-01\n",
      " 3.7008e-01\n",
      " 2.1928e-01\n",
      " 3.5260e-01\n",
      " 3.1161e-01\n",
      " 2.3601e-01\n",
      " 3.2994e-01\n",
      " 2.6795e-01\n",
      " 3.9881e-01\n",
      " 3.0537e-01\n",
      " 2.4712e-01\n",
      " 2.9097e-01\n",
      " 3.4060e-01\n",
      " 3.1166e-01\n",
      " 3.0757e-01\n",
      " 3.3158e-01\n",
      " 3.2035e-01\n",
      " 2.6065e-01\n",
      " 2.9700e-01\n",
      " 3.2832e-01\n",
      " 3.7282e-01\n",
      " 4.1130e-01\n",
      " 3.9063e-01\n",
      " 3.1573e-01\n",
      " 3.9294e-01\n",
      " 3.2683e-01\n",
      " 2.9343e-01\n",
      " 3.1344e-01\n",
      " 2.8561e-01\n",
      " 3.1212e-01\n",
      " 3.4229e-01\n",
      " 2.2644e-01\n",
      " 3.1841e-01\n",
      " 3.0261e-01\n",
      " 2.8502e-01\n",
      " 3.0059e-01\n",
      " 2.4962e-01\n",
      " 4.7394e-01\n",
      " 3.4556e-01\n",
      " 3.1512e-01\n",
      " 3.5739e-01\n",
      " 2.8784e-01\n",
      " 3.8358e-09\n",
      " 3.0297e-01\n",
      " 1.7179e-01\n",
      " 2.8589e-01\n",
      " 2.1688e-01\n",
      " 3.2335e-01\n",
      " 3.5298e-01\n",
      " 1.9909e-01\n",
      "-2.6759e-01\n",
      " 2.1727e-01\n",
      " 3.6319e-01\n",
      " 3.1461e-01\n",
      " 3.2137e-01\n",
      " 3.7752e-01\n",
      "-9.3226e-07\n",
      " 3.0150e-01\n",
      " 2.9847e-01\n",
      " 1.9637e-01\n",
      " 3.6328e-08\n",
      " 2.9522e-01\n",
      " 2.6955e-01\n",
      " 3.3095e-01\n",
      " 3.1685e-01\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      " 2.1681e-02\n",
      "-3.1721e-02\n",
      "-4.4874e-02\n",
      "-6.6171e-02\n",
      "-3.5444e-02\n",
      "-2.6425e-01\n",
      "-9.8146e-02\n",
      "-5.8350e-02\n",
      " 8.8789e-02\n",
      " 8.9035e-03\n",
      " 6.7118e-02\n",
      "-1.9150e-01\n",
      " 1.9949e-01\n",
      "-1.2080e-01\n",
      "-1.2747e-01\n",
      "-1.1186e-01\n",
      "-2.4341e-07\n",
      "-3.6434e-02\n",
      "-1.2726e-01\n",
      "-2.6579e-01\n",
      "-8.1704e-02\n",
      "-4.4807e-02\n",
      " 1.7246e-02\n",
      "-3.0520e-01\n",
      "-4.8642e-02\n",
      "-7.8183e-02\n",
      " 3.2063e-01\n",
      " 2.5489e-02\n",
      " 1.2304e-01\n",
      "-1.1368e-01\n",
      "-4.3808e-01\n",
      "-5.0221e-02\n",
      "-7.6679e-02\n",
      "-6.1794e-02\n",
      "-8.5505e-02\n",
      "-1.3860e-01\n",
      " 2.3487e-01\n",
      " 5.4284e-02\n",
      "-1.5954e-02\n",
      "-4.9981e-02\n",
      "-9.8860e-02\n",
      "-1.6710e-01\n",
      "-1.3594e-01\n",
      "-4.7858e-02\n",
      "-1.5370e-01\n",
      " 2.1528e-01\n",
      " 2.2232e-01\n",
      " 1.0859e-01\n",
      "-7.0542e-02\n",
      "-1.5307e-01\n",
      "-1.3854e-01\n",
      " 1.0165e-01\n",
      " 5.9747e-02\n",
      "-3.5456e-02\n",
      "-7.3147e-02\n",
      " 5.1427e-02\n",
      "-5.5321e-02\n",
      "-7.5347e-05\n",
      "-3.3640e-01\n",
      "-1.3251e-01\n",
      "-1.4658e-01\n",
      " 1.6523e-01\n",
      " 7.7728e-03\n",
      "-5.5525e-02\n",
      "-7.7661e-02\n",
      "-9.1253e-02\n",
      "-1.1324e-01\n",
      "-3.3572e-02\n",
      " 2.0060e-02\n",
      "-1.5880e-01\n",
      " 1.6165e-01\n",
      "-3.3195e-01\n",
      "-6.7754e-02\n",
      " 1.4615e-01\n",
      "-7.4614e-02\n",
      "-4.4786e-02\n",
      "-1.5052e-01\n",
      "-1.2604e-01\n",
      "-2.7074e-02\n",
      "-1.4464e-01\n",
      "-6.4618e-02\n",
      " 5.7270e-02\n",
      "-7.4613e-02\n",
      "-1.6458e-01\n",
      "-1.1446e-01\n",
      "-2.0008e-01\n",
      "-8.3671e-02\n",
      "-2.2238e-01\n",
      " 1.5339e-02\n",
      "-1.0271e-02\n",
      "-1.2457e-01\n",
      " 2.4270e-02\n",
      "-4.7623e-02\n",
      "-1.1358e-01\n",
      "-6.1413e-02\n",
      "-7.4182e-02\n",
      " 3.4094e-02\n",
      "-1.1264e-01\n",
      " 1.3035e-02\n",
      " 3.9355e-02\n",
      "-2.9382e-01\n",
      "-1.3672e-01\n",
      "-4.4234e-02\n",
      "-1.0882e-01\n",
      "-1.0781e-01\n",
      "-5.8734e-08\n",
      " 1.7031e-01\n",
      " 1.1327e-01\n",
      "-1.2375e-01\n",
      " 6.8173e-02\n",
      "-9.6255e-02\n",
      " 7.4535e-02\n",
      " 3.0900e-01\n",
      "-1.0785e-01\n",
      " 6.9795e-02\n",
      " 1.4832e-02\n",
      "-3.6437e-02\n",
      "-4.7123e-02\n",
      "-2.0794e-01\n",
      "-1.8465e-05\n",
      "-1.6940e-01\n",
      "-4.0999e-03\n",
      " 1.7748e-01\n",
      "-3.3973e-07\n",
      "-9.7568e-02\n",
      "-2.7169e-02\n",
      "-2.0851e-01\n",
      "-1.2777e-01\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -3.3292e-02 -1.4120e-02  8.3565e-03\n",
      " -6.7238e-02 -5.7516e-02 -1.1658e-02\n",
      " -8.2214e-02 -8.7232e-02 -3.1026e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -1.0118e-02 -1.5202e-02 -8.6281e-03\n",
      " -1.0842e-02 -9.6461e-03 -1.0042e-02\n",
      " -1.3196e-02  2.1129e-02  6.2836e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -2.9157e-02 -2.2547e-02  1.5920e-02\n",
      "  3.7332e-03 -4.7189e-03 -4.0475e-02\n",
      "  2.7836e-02  6.5394e-02  7.4702e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "  5.4412e-03  8.9491e-03  2.0623e-02\n",
      " -1.7586e-02  7.9693e-03  2.7480e-02\n",
      " -2.9222e-02  1.4645e-03  9.7201e-04\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      " -1.1360e-02 -1.2122e-02 -1.9012e-02\n",
      "  1.1839e-02  1.7251e-03  6.8400e-03\n",
      "  3.6929e-02  3.5146e-02  1.2953e-02\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      " -1.3572e-02 -3.9339e-03  2.1353e-03\n",
      " -1.4563e-02 -1.5826e-02 -1.6236e-02\n",
      "  3.0446e-02  2.8997e-02  3.1096e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  2.5136e-02  2.7275e-02  2.4689e-02\n",
      " -5.0868e-03  2.5971e-03  1.8866e-02\n",
      " -3.3776e-02 -4.0095e-02 -1.7018e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  1.5422e-02  3.6149e-02  1.3933e-02\n",
      " -5.9437e-03  1.5728e-03 -1.8514e-02\n",
      " -2.2743e-02 -3.2549e-02 -1.4691e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -9.3350e-03  4.1235e-02  2.7619e-02\n",
      " -1.9385e-02  3.5104e-02  3.5224e-02\n",
      "  1.3616e-02 -1.2942e-02  9.1668e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,125,.,.) = \n",
      " -3.4256e-02 -1.1028e-02  3.6637e-03\n",
      " -3.2105e-02 -6.3685e-04  1.6866e-02\n",
      " -5.8908e-03 -2.2383e-03 -8.0908e-03\n",
      "\n",
      "( 1 ,126,.,.) = \n",
      "  4.7292e-02  4.0057e-02 -3.8385e-02\n",
      " -3.3760e-02 -1.0417e-03  5.1417e-02\n",
      "  1.5004e-03 -4.5379e-02  1.4566e-02\n",
      "\n",
      "( 1 ,127,.,.) = \n",
      " -4.5232e-02 -1.1577e-02  9.4798e-03\n",
      " -2.8248e-02 -3.5298e-02 -8.8185e-04\n",
      " -2.1932e-02 -3.5078e-02 -2.9868e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  1.3083e-03 -1.0057e-02  8.8250e-03\n",
      " -2.5678e-03 -6.8183e-03  1.2107e-02\n",
      " -3.4392e-03  6.6777e-03  1.3454e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  1.4572e-02  1.6095e-02 -2.9959e-03\n",
      "  2.0722e-02  7.5078e-03 -9.7769e-03\n",
      "  2.9928e-02  8.7538e-03 -4.8254e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  5.6885e-03  1.2274e-02 -3.8922e-04\n",
      "  1.7489e-02 -3.0676e-03  1.3702e-02\n",
      "  1.0461e-02 -3.0776e-03  1.2371e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,125,.,.) = \n",
      " -1.7133e-02 -3.6864e-03  8.6395e-03\n",
      "  2.0908e-03  1.4898e-02  1.5828e-02\n",
      "  1.2588e-02  3.9641e-03 -3.2034e-03\n",
      "\n",
      "( 2 ,126,.,.) = \n",
      " -8.6805e-03 -2.9358e-03 -3.6998e-03\n",
      "  1.1274e-02  5.8865e-02  5.1263e-02\n",
      "  3.0182e-04 -1.2066e-02  2.1260e-02\n",
      "\n",
      "( 2 ,127,.,.) = \n",
      "  1.3602e-02  1.1379e-02  1.3146e-02\n",
      "  1.8500e-02  1.8260e-02  2.4223e-02\n",
      "  4.2752e-03  1.8285e-02  1.0011e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(253, 0 ,.,.) = \n",
      " -2.6799e-02 -3.2031e-02 -3.7717e-02\n",
      " -2.0279e-03 -1.4311e-02 -3.6742e-02\n",
      "  7.5970e-03 -2.5366e-04 -2.2291e-02\n",
      "\n",
      "(253, 1 ,.,.) = \n",
      "  7.5740e-03  1.9664e-02  1.8748e-02\n",
      "  2.0172e-02  4.2836e-02  2.9026e-02\n",
      " -2.2465e-02 -2.0116e-03  8.6480e-03\n",
      "\n",
      "(253, 2 ,.,.) = \n",
      "  1.0110e-02  1.0329e-02 -1.1140e-03\n",
      "  3.9388e-03 -7.2387e-03 -2.2457e-03\n",
      "  8.0991e-03 -1.1861e-02 -2.0403e-02\n",
      "    ... \n",
      "\n",
      "(253,125,.,.) = \n",
      "  1.3251e-02 -1.2242e-03 -1.0152e-02\n",
      "  1.8174e-02  3.0825e-02  6.4135e-03\n",
      "  1.0243e-02  1.0857e-02 -2.2477e-02\n",
      "\n",
      "(253,126,.,.) = \n",
      "  3.6677e-02  5.5840e-03  1.0275e-02\n",
      "  5.1312e-02  1.6869e-02  2.1923e-02\n",
      "  5.2454e-03  1.8504e-02  3.3272e-02\n",
      "\n",
      "(253,127,.,.) = \n",
      " -6.6584e-03  5.3909e-03 -1.7474e-02\n",
      " -3.8406e-03  5.8924e-02 -1.7974e-02\n",
      "  8.6339e-03  1.6764e-02 -2.3197e-02\n",
      "      â‹®  \n",
      "\n",
      "(254, 0 ,.,.) = \n",
      " -2.3646e-04  2.0513e-02  2.6586e-03\n",
      " -3.9167e-03  2.7257e-02  1.6286e-02\n",
      " -2.1510e-02  2.3596e-03 -8.7830e-03\n",
      "\n",
      "(254, 1 ,.,.) = \n",
      "  3.2650e-02  2.8117e-02  1.3729e-02\n",
      "  6.4524e-03  2.1824e-02  2.0685e-03\n",
      " -3.1572e-02 -1.4155e-02 -2.2925e-02\n",
      "\n",
      "(254, 2 ,.,.) = \n",
      " -2.0825e-02 -3.1246e-02 -7.3778e-03\n",
      " -1.0455e-02 -7.1736e-03 -7.4295e-03\n",
      " -6.9373e-03 -3.8425e-03 -7.1422e-03\n",
      "    ... \n",
      "\n",
      "(254,125,.,.) = \n",
      " -2.5166e-02  1.0409e-02  3.9899e-02\n",
      " -1.6235e-03 -2.0337e-02  3.0006e-02\n",
      "  3.5884e-02  2.9429e-02  3.1706e-02\n",
      "\n",
      "(254,126,.,.) = \n",
      "  2.9663e-02  1.5974e-03 -3.3333e-02\n",
      "  2.8826e-02  4.2230e-03 -6.1304e-03\n",
      "  3.5989e-02 -1.8676e-02 -8.8509e-03\n",
      "\n",
      "(254,127,.,.) = \n",
      "  4.5153e-03  1.3005e-03  2.1882e-02\n",
      "  2.2801e-02 -1.3339e-03  1.3643e-03\n",
      "  6.7874e-03  1.7647e-02  5.2987e-03\n",
      "      â‹®  \n",
      "\n",
      "(255, 0 ,.,.) = \n",
      "  7.6193e-04  1.8652e-03 -8.3484e-03\n",
      "  1.2829e-02  1.3175e-02  7.1008e-03\n",
      " -2.5555e-03 -5.0060e-03 -1.9965e-02\n",
      "\n",
      "(255, 1 ,.,.) = \n",
      "  2.0465e-02  1.7776e-03 -7.3782e-03\n",
      "  1.2082e-02  1.8087e-02 -9.8354e-04\n",
      " -1.6210e-02  1.5675e-03  3.7557e-03\n",
      "\n",
      "(255, 2 ,.,.) = \n",
      " -4.4830e-02 -1.0730e-02 -2.7946e-02\n",
      " -2.6339e-02  9.1421e-03 -6.3169e-03\n",
      "  3.6902e-03  3.7124e-03 -2.3903e-02\n",
      "    ... \n",
      "\n",
      "(255,125,.,.) = \n",
      "  1.9192e-02  1.0770e-03  2.7659e-02\n",
      "  8.4191e-03 -6.0575e-03  3.3279e-02\n",
      "  2.6040e-02  1.6104e-02  3.2223e-02\n",
      "\n",
      "(255,126,.,.) = \n",
      "  3.0836e-03  1.4127e-02 -1.8142e-03\n",
      " -1.2169e-02 -3.7365e-02 -4.2414e-03\n",
      "  5.7848e-04 -2.6638e-02  6.7427e-04\n",
      "\n",
      "(255,127,.,.) = \n",
      " -3.1190e-03 -3.1014e-02  2.8281e-02\n",
      "  2.1067e-03 -4.4556e-02 -7.7530e-03\n",
      "  1.3964e-02  1.4143e-02  8.1565e-03\n",
      "[torch.FloatTensor of size 256x128x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "-1.6105e-08\n",
      " 7.5244e-08\n",
      " 2.1525e-08\n",
      "-7.4830e-08\n",
      " 4.0028e-08\n",
      " 2.0125e-07\n",
      " 3.0836e-09\n",
      "-1.2907e-07\n",
      "-4.8796e-09\n",
      "-5.7191e-08\n",
      "-5.4578e-08\n",
      "-3.9656e-08\n",
      "-2.1347e-09\n",
      "-1.1581e-07\n",
      " 7.6258e-08\n",
      "-2.0476e-08\n",
      " 1.6960e-08\n",
      " 1.0894e-07\n",
      " 7.6891e-08\n",
      " 3.2939e-08\n",
      "-2.4267e-09\n",
      "-6.8908e-08\n",
      " 8.4935e-08\n",
      "-4.5520e-08\n",
      " 8.2543e-08\n",
      "-5.2748e-08\n",
      "-9.7704e-08\n",
      " 3.9908e-10\n",
      " 6.0350e-09\n",
      " 2.8966e-08\n",
      "-1.8077e-08\n",
      " 4.6512e-08\n",
      "-4.5208e-08\n",
      "-5.3004e-08\n",
      " 3.0517e-08\n",
      " 8.5009e-09\n",
      "-3.7000e-08\n",
      " 2.4454e-08\n",
      "-5.9350e-08\n",
      "-1.2664e-08\n",
      "-2.0602e-08\n",
      " 8.7701e-09\n",
      "-5.6419e-08\n",
      "-2.7916e-08\n",
      "-3.2290e-08\n",
      "-4.0166e-08\n",
      "-5.9390e-09\n",
      " 2.5727e-08\n",
      " 6.1466e-09\n",
      " 4.4333e-08\n",
      "-1.0324e-07\n",
      "-2.8709e-08\n",
      " 4.4208e-08\n",
      "-7.7599e-08\n",
      " 8.6090e-09\n",
      "-1.9044e-08\n",
      " 1.1488e-07\n",
      " 1.1980e-07\n",
      " 5.7397e-08\n",
      " 3.7494e-08\n",
      " 5.3562e-08\n",
      "-6.0933e-08\n",
      " 8.5125e-09\n",
      " 9.2560e-08\n",
      "-5.4153e-08\n",
      " 1.5346e-08\n",
      "-2.5016e-08\n",
      " 1.0584e-08\n",
      " 2.2897e-08\n",
      " 3.5590e-08\n",
      "-4.6841e-08\n",
      "-3.5798e-08\n",
      "-8.8869e-08\n",
      " 3.5842e-08\n",
      " 7.7629e-08\n",
      "-1.9674e-07\n",
      "-1.3551e-15\n",
      "-6.5669e-08\n",
      " 1.4480e-08\n",
      " 4.0900e-08\n",
      "-1.1627e-09\n",
      " 1.0102e-07\n",
      "-7.8441e-12\n",
      " 6.2371e-08\n",
      "-1.2025e-07\n",
      "-3.0720e-08\n",
      "-6.1022e-12\n",
      " 1.2731e-07\n",
      "-3.4654e-08\n",
      "-3.4712e-09\n",
      " 1.5406e-08\n",
      "-1.2044e-07\n",
      " 1.6723e-08\n",
      " 5.9396e-08\n",
      " 1.8150e-08\n",
      " 4.3155e-08\n",
      " 3.2605e-08\n",
      "-3.1232e-08\n",
      " 2.3361e-08\n",
      " 8.9578e-08\n",
      "-5.2490e-08\n",
      "-2.3679e-09\n",
      "-2.0753e-08\n",
      "-6.1659e-08\n",
      " 7.7862e-09\n",
      " 4.0278e-08\n",
      "-2.2868e-08\n",
      "-3.1027e-09\n",
      " 8.2134e-08\n",
      "-3.0716e-08\n",
      "-5.4082e-08\n",
      " 1.3781e-08\n",
      " 7.1853e-16\n",
      " 4.1113e-08\n",
      " 1.9483e-08\n",
      " 3.6233e-08\n",
      "-5.6497e-12\n",
      "-4.6571e-08\n",
      "-6.6946e-08\n",
      " 4.3285e-08\n",
      "-2.2384e-08\n",
      " 6.4347e-08\n",
      "-5.1729e-08\n",
      " 2.4435e-08\n",
      "-6.8093e-12\n",
      "-4.9810e-08\n",
      "-4.9386e-09\n",
      " 5.6959e-08\n",
      " 5.0658e-08\n",
      " 1.6271e-08\n",
      "-6.6254e-11\n",
      " 9.4196e-08\n",
      "-1.7466e-08\n",
      " 3.6590e-08\n",
      "-5.6445e-08\n",
      "-6.6841e-08\n",
      "-1.2518e-07\n",
      "-7.4940e-08\n",
      " 4.5525e-08\n",
      " 1.5000e-07\n",
      " 4.2658e-09\n",
      "-1.1203e-07\n",
      "-1.1004e-09\n",
      " 1.0238e-07\n",
      "-3.2099e-08\n",
      "-8.7770e-08\n",
      " 2.2881e-08\n",
      "-1.0424e-07\n",
      "-8.0070e-08\n",
      "-1.1193e-14\n",
      "-1.1517e-07\n",
      "-6.0828e-08\n",
      " 8.6582e-09\n",
      " 5.1161e-08\n",
      " 2.3871e-09\n",
      "-9.1417e-10\n",
      " 8.7783e-09\n",
      "-3.0924e-08\n",
      " 1.5785e-08\n",
      " 4.8668e-08\n",
      " 1.1152e-08\n",
      " 8.8657e-09\n",
      " 7.4873e-08\n",
      "-1.1168e-07\n",
      "-1.1200e-07\n",
      "-7.5096e-08\n",
      " 3.6865e-08\n",
      "-1.6151e-07\n",
      " 4.0007e-08\n",
      "-1.2262e-08\n",
      "-1.1625e-15\n",
      " 1.2581e-07\n",
      "-8.0809e-08\n",
      " 1.3334e-13\n",
      "-4.7398e-08\n",
      " 9.5623e-08\n",
      " 4.7422e-08\n",
      "-5.6211e-08\n",
      "-9.8316e-08\n",
      " 5.6926e-08\n",
      " 5.2950e-09\n",
      " 3.4255e-08\n",
      "-5.8186e-08\n",
      " 1.2180e-07\n",
      " 1.1282e-07\n",
      "-4.8720e-08\n",
      "-9.2188e-08\n",
      " 1.0529e-07\n",
      " 3.8813e-08\n",
      " 1.6462e-08\n",
      " 4.6585e-08\n",
      " 9.0092e-08\n",
      " 4.4649e-08\n",
      " 4.5501e-08\n",
      "-1.3349e-07\n",
      " 6.9873e-08\n",
      " 3.6231e-08\n",
      "-8.1092e-09\n",
      "-7.6084e-09\n",
      "-1.6821e-08\n",
      "-3.2038e-08\n",
      "-5.0040e-08\n",
      " 4.5679e-08\n",
      " 9.3107e-10\n",
      "-3.0348e-08\n",
      " 2.0401e-08\n",
      " 4.7125e-08\n",
      "-1.1247e-08\n",
      "-5.0328e-09\n",
      " 5.0238e-08\n",
      " 5.9268e-08\n",
      " 9.0911e-08\n",
      "-2.7551e-09\n",
      " 1.3587e-08\n",
      " 3.7782e-08\n",
      "-2.3760e-08\n",
      "-2.6377e-08\n",
      " 5.3217e-08\n",
      " 6.2271e-08\n",
      " 4.2268e-08\n",
      " 5.9398e-08\n",
      "-3.1747e-08\n",
      " 2.5965e-08\n",
      "-1.0891e-07\n",
      " 9.5627e-08\n",
      "-1.2207e-07\n",
      " 2.1686e-08\n",
      "-7.7764e-09\n",
      " 7.5804e-08\n",
      "-3.4641e-08\n",
      " 3.3070e-08\n",
      "-6.3374e-08\n",
      " 8.5352e-08\n",
      " 9.8338e-10\n",
      " 1.9279e-08\n",
      "-4.2185e-08\n",
      " 5.4268e-08\n",
      "-3.7386e-08\n",
      "-2.0471e-07\n",
      " 1.6075e-09\n",
      "-1.2334e-08\n",
      " 6.1424e-08\n",
      " 6.2942e-08\n",
      "-1.4589e-08\n",
      " 6.1067e-08\n",
      "-7.1101e-09\n",
      " 4.7055e-08\n",
      "-1.1256e-07\n",
      "-1.6538e-08\n",
      "-4.9533e-09\n",
      " 4.1261e-08\n",
      "-1.1176e-07\n",
      " 1.6614e-08\n",
      "-9.0417e-08\n",
      "-1.2749e-08\n",
      "-6.4238e-08\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      " 1.5381e-01\n",
      " 2.4843e-01\n",
      " 1.6499e-01\n",
      " 2.5485e-01\n",
      " 2.9675e-01\n",
      " 1.6516e-01\n",
      " 2.3095e-01\n",
      " 3.4821e-01\n",
      " 2.4264e-01\n",
      " 2.7542e-01\n",
      " 3.3902e-01\n",
      " 1.7514e-01\n",
      " 2.9489e-01\n",
      " 1.5281e-01\n",
      " 3.0838e-01\n",
      " 3.0846e-01\n",
      " 2.4518e-01\n",
      " 2.0111e-01\n",
      " 3.2144e-01\n",
      " 1.9253e-01\n",
      " 2.5037e-01\n",
      " 1.4993e-01\n",
      " 2.4803e-01\n",
      " 2.1473e-01\n",
      " 3.3516e-01\n",
      " 2.3036e-01\n",
      " 2.6265e-01\n",
      " 2.4056e-01\n",
      " 2.0782e-01\n",
      " 3.3270e-01\n",
      " 2.6697e-01\n",
      " 3.5397e-01\n",
      " 2.7892e-01\n",
      " 2.7259e-01\n",
      " 3.0527e-01\n",
      " 2.6109e-01\n",
      " 2.1672e-01\n",
      " 2.3001e-01\n",
      " 2.6815e-01\n",
      " 1.3503e-01\n",
      " 2.2117e-01\n",
      " 2.1904e-01\n",
      " 3.6422e-01\n",
      " 2.8474e-01\n",
      " 2.9774e-01\n",
      "-2.1366e-01\n",
      " 1.5553e-01\n",
      " 2.2181e-01\n",
      " 2.3450e-01\n",
      " 2.4078e-01\n",
      " 2.2661e-01\n",
      " 2.0416e-01\n",
      " 3.2715e-01\n",
      " 2.6258e-01\n",
      " 1.5178e-01\n",
      " 2.4108e-01\n",
      " 2.2313e-01\n",
      " 1.6414e-01\n",
      " 2.7850e-01\n",
      " 3.2168e-01\n",
      " 3.2635e-01\n",
      " 2.2067e-01\n",
      " 2.7087e-01\n",
      " 1.9429e-01\n",
      " 2.7940e-01\n",
      " 3.3111e-01\n",
      " 2.4740e-01\n",
      " 2.5614e-01\n",
      " 3.2907e-01\n",
      " 1.9329e-01\n",
      " 1.4882e-01\n",
      " 2.2364e-01\n",
      " 2.7950e-01\n",
      " 2.3670e-01\n",
      " 2.8451e-01\n",
      " 2.2018e-01\n",
      " 1.2490e-08\n",
      " 2.3182e-01\n",
      " 1.5146e-01\n",
      " 2.9657e-01\n",
      " 2.0926e-01\n",
      " 2.1625e-01\n",
      " 7.0960e-06\n",
      " 2.4215e-01\n",
      " 2.0476e-01\n",
      " 2.3104e-01\n",
      " 5.6582e-06\n",
      " 2.1019e-01\n",
      " 2.5668e-01\n",
      " 3.3831e-01\n",
      " 1.8386e-01\n",
      " 2.1769e-01\n",
      " 1.2685e-01\n",
      " 2.5508e-01\n",
      " 1.5350e-01\n",
      " 2.0882e-01\n",
      " 2.6131e-01\n",
      " 1.9648e-01\n",
      " 2.4915e-01\n",
      " 3.2035e-01\n",
      " 1.7687e-01\n",
      " 3.0441e-01\n",
      " 2.5733e-01\n",
      " 2.4956e-01\n",
      " 2.4351e-01\n",
      " 2.8937e-01\n",
      " 1.9947e-01\n",
      " 2.6034e-01\n",
      " 3.0989e-01\n",
      " 3.5402e-01\n",
      " 2.0052e-01\n",
      " 3.4263e-01\n",
      " 2.4125e-08\n",
      " 2.4139e-01\n",
      " 2.9071e-01\n",
      " 2.1835e-01\n",
      " 7.5835e-06\n",
      " 2.0479e-01\n",
      " 1.6108e-01\n",
      " 3.1610e-01\n",
      " 2.4142e-01\n",
      " 2.1115e-01\n",
      " 1.9390e-01\n",
      " 1.7359e-01\n",
      " 5.0802e-06\n",
      " 2.3070e-01\n",
      " 3.2996e-01\n",
      " 1.8853e-01\n",
      " 2.0970e-01\n",
      " 2.7876e-01\n",
      " 1.1246e-01\n",
      " 2.2013e-01\n",
      " 3.0970e-01\n",
      " 4.2552e-01\n",
      " 3.2559e-01\n",
      " 2.1561e-01\n",
      " 3.7535e-01\n",
      " 3.7743e-01\n",
      " 1.9710e-01\n",
      " 1.4912e-01\n",
      " 2.3295e-01\n",
      " 3.7963e-01\n",
      " 2.7770e-01\n",
      " 3.3947e-01\n",
      " 3.1800e-01\n",
      " 2.6776e-01\n",
      " 1.7902e-01\n",
      " 3.4680e-01\n",
      " 2.8481e-01\n",
      "-2.1142e-05\n",
      " 3.2816e-01\n",
      " 2.2254e-01\n",
      " 2.0913e-01\n",
      " 2.5558e-01\n",
      " 2.5665e-01\n",
      " 2.0345e-01\n",
      " 1.7132e-01\n",
      " 2.5206e-01\n",
      " 2.5534e-01\n",
      " 1.4996e-01\n",
      " 2.2928e-01\n",
      " 1.9661e-01\n",
      " 2.7032e-01\n",
      " 3.0892e-01\n",
      " 2.3233e-01\n",
      " 3.0455e-01\n",
      " 1.9556e-01\n",
      " 2.2978e-01\n",
      " 2.7632e-01\n",
      " 2.1907e-01\n",
      "-4.1058e-08\n",
      " 3.1899e-01\n",
      " 2.2000e-01\n",
      " 1.0785e-07\n",
      " 2.7913e-01\n",
      " 2.4109e-01\n",
      " 1.9525e-01\n",
      " 3.2743e-01\n",
      " 2.4172e-01\n",
      " 2.7641e-01\n",
      " 2.5752e-01\n",
      " 1.8059e-01\n",
      " 2.5963e-01\n",
      " 3.4674e-01\n",
      " 3.3588e-01\n",
      " 2.2344e-01\n",
      " 2.2282e-01\n",
      " 2.2711e-01\n",
      " 2.5740e-01\n",
      " 1.6036e-01\n",
      " 2.1692e-01\n",
      " 2.1334e-01\n",
      " 3.1077e-01\n",
      " 3.3701e-01\n",
      " 2.3040e-01\n",
      " 3.1880e-01\n",
      " 2.5285e-01\n",
      " 2.4050e-01\n",
      " 1.6645e-01\n",
      " 2.5605e-01\n",
      " 3.0512e-01\n",
      " 1.8273e-01\n",
      " 2.8588e-01\n",
      " 3.8495e-02\n",
      " 2.6479e-01\n",
      " 2.2711e-01\n",
      " 2.1107e-01\n",
      " 2.4551e-01\n",
      " 2.2552e-01\n",
      " 2.1466e-01\n",
      " 1.4494e-01\n",
      " 2.8680e-01\n",
      " 2.6789e-01\n",
      " 2.1688e-01\n",
      " 2.3949e-01\n",
      " 2.9615e-01\n",
      " 2.5208e-01\n",
      " 3.4117e-01\n",
      " 3.1979e-01\n",
      " 3.4775e-01\n",
      " 2.0075e-01\n",
      " 2.6976e-01\n",
      " 2.5833e-01\n",
      " 2.6680e-01\n",
      " 1.9384e-01\n",
      " 1.7509e-01\n",
      " 3.5335e-01\n",
      " 2.1326e-01\n",
      " 3.3251e-01\n",
      " 3.1653e-01\n",
      " 3.2765e-01\n",
      " 2.5969e-01\n",
      " 2.5415e-01\n",
      " 2.8321e-01\n",
      " 3.0590e-01\n",
      " 2.7445e-01\n",
      " 2.1580e-01\n",
      " 2.1431e-01\n",
      " 2.5537e-01\n",
      " 2.5362e-01\n",
      " 3.6893e-01\n",
      " 2.0768e-01\n",
      " 2.9512e-01\n",
      " 2.3469e-01\n",
      " 2.1615e-01\n",
      " 3.3473e-01\n",
      " 3.3768e-01\n",
      " 2.9671e-01\n",
      " 4.2744e-01\n",
      " 2.0083e-01\n",
      " 1.4640e-01\n",
      " 3.6264e-01\n",
      " 2.7198e-01\n",
      " 2.6462e-01\n",
      " 2.8660e-01\n",
      " 3.0883e-01\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      " 6.1194e-02\n",
      "-2.0338e-02\n",
      " 6.9390e-02\n",
      "-1.6764e-02\n",
      "-1.9737e-01\n",
      " 1.9742e-01\n",
      "-1.0016e-01\n",
      "-1.7291e-01\n",
      "-1.6403e-01\n",
      "-1.6123e-01\n",
      "-1.6141e-01\n",
      " 2.5101e-01\n",
      "-7.2010e-02\n",
      " 2.4077e-01\n",
      "-1.2702e-01\n",
      "-9.8436e-02\n",
      "-1.2555e-01\n",
      " 2.0554e-03\n",
      "-1.3933e-01\n",
      " 3.5958e-02\n",
      "-1.1799e-01\n",
      " 2.6867e-01\n",
      " 2.9129e-02\n",
      "-8.0782e-02\n",
      "-1.7855e-01\n",
      "-1.4094e-01\n",
      "-8.5141e-02\n",
      "-4.9177e-02\n",
      "-1.0380e-01\n",
      "-1.8253e-01\n",
      "-1.1369e-01\n",
      "-1.9760e-01\n",
      "-7.3630e-02\n",
      "-4.1572e-02\n",
      "-1.7843e-01\n",
      "-8.8795e-02\n",
      "-1.3544e-01\n",
      "-3.0692e-03\n",
      "-8.6868e-02\n",
      " 6.2045e-02\n",
      "-7.4820e-02\n",
      "-8.1671e-02\n",
      "-2.2117e-01\n",
      "-6.6426e-02\n",
      "-1.0768e-01\n",
      "-1.3577e-01\n",
      " 3.6913e-01\n",
      "-1.2038e-01\n",
      "-5.9190e-02\n",
      "-1.3874e-01\n",
      " 1.2675e-01\n",
      " 9.8803e-02\n",
      "-1.5123e-01\n",
      "-4.5077e-02\n",
      "-1.3915e-01\n",
      "-6.6811e-02\n",
      "-1.1065e-02\n",
      " 2.3155e-01\n",
      "-6.5496e-02\n",
      "-3.3038e-01\n",
      "-1.1347e-01\n",
      " 4.8300e-02\n",
      "-5.5851e-02\n",
      "-8.7971e-02\n",
      "-1.6467e-01\n",
      "-2.1017e-01\n",
      "-5.0401e-02\n",
      "-4.2781e-02\n",
      "-1.3426e-01\n",
      " 2.3260e-02\n",
      " 2.3616e-01\n",
      "-1.4558e-01\n",
      "-6.4265e-02\n",
      "-7.6430e-02\n",
      "-8.6056e-02\n",
      " 6.7299e-02\n",
      "-1.1208e-07\n",
      "-4.9723e-02\n",
      " 3.2373e-01\n",
      "-1.3629e-01\n",
      " 6.5470e-02\n",
      "-5.0537e-02\n",
      "-1.5661e-04\n",
      "-1.3729e-01\n",
      " 7.6918e-02\n",
      " 1.7881e-02\n",
      "-1.2237e-04\n",
      " 4.9199e-02\n",
      "-7.8283e-02\n",
      "-2.4185e-01\n",
      "-1.5850e-01\n",
      " 3.9180e-02\n",
      "-1.1396e-01\n",
      "-1.2839e-01\n",
      "-9.1294e-02\n",
      "-1.2040e-01\n",
      "-1.0744e-01\n",
      " 5.3072e-02\n",
      "-2.6217e-02\n",
      "-9.1573e-02\n",
      " 1.8093e-01\n",
      "-1.7810e-01\n",
      "-6.7461e-02\n",
      "-6.6528e-02\n",
      " 2.8011e-02\n",
      "-7.7115e-02\n",
      " 3.3222e-02\n",
      " 1.3640e-02\n",
      "-1.4268e-01\n",
      "-3.1734e-01\n",
      "-5.4343e-02\n",
      "-1.9594e-01\n",
      "-1.6361e-07\n",
      "-1.2124e-01\n",
      "-1.0731e-01\n",
      "-6.0919e-02\n",
      "-3.7238e-05\n",
      "-3.3409e-02\n",
      " 3.1796e-01\n",
      "-1.9243e-01\n",
      " 2.5114e-02\n",
      "-1.1275e-01\n",
      " 3.3689e-03\n",
      "-3.4666e-02\n",
      "-3.2464e-05\n",
      " 6.8454e-03\n",
      "-6.8970e-02\n",
      " 1.3608e-01\n",
      " 1.2048e-01\n",
      "-6.4688e-02\n",
      "-7.9617e-02\n",
      "-6.8480e-02\n",
      "-1.5520e-01\n",
      "-5.1682e-01\n",
      "-1.0324e-01\n",
      " 8.0512e-02\n",
      "-1.8260e-01\n",
      "-3.2701e-01\n",
      " 7.0966e-02\n",
      " 3.6087e-01\n",
      "-2.3968e-01\n",
      "-3.1417e-01\n",
      "-6.6939e-02\n",
      "-1.9331e-01\n",
      "-1.0986e-01\n",
      "-1.4698e-01\n",
      "-4.3750e-02\n",
      "-1.8895e-01\n",
      "-1.3143e-01\n",
      "-1.3055e-04\n",
      "-1.8254e-01\n",
      "-1.1961e-01\n",
      "-1.0711e-01\n",
      "-5.1134e-02\n",
      "-4.5345e-02\n",
      " 7.6741e-02\n",
      " 3.4139e-02\n",
      "-5.5245e-02\n",
      " 3.9769e-02\n",
      " 3.5372e-01\n",
      "-8.4940e-02\n",
      "-4.0808e-02\n",
      "-1.3762e-01\n",
      "-6.3934e-02\n",
      " 7.8114e-02\n",
      "-1.2821e-01\n",
      " 3.6118e-02\n",
      " 2.9715e-02\n",
      "-1.6970e-01\n",
      "-7.2314e-02\n",
      "-3.1128e-07\n",
      "-1.8396e-01\n",
      " 3.4730e-02\n",
      "-3.8360e-06\n",
      "-1.2167e-01\n",
      " 3.7122e-02\n",
      "-1.0194e-01\n",
      "-1.5307e-01\n",
      "-3.2042e-02\n",
      "-8.8893e-02\n",
      " 4.0284e-04\n",
      "-1.2093e-02\n",
      "-7.3759e-02\n",
      "-1.7527e-01\n",
      "-2.0603e-01\n",
      "-2.5026e-02\n",
      "-2.8962e-02\n",
      "-6.9405e-02\n",
      "-1.8230e-01\n",
      "-1.6716e-01\n",
      "-1.1859e-01\n",
      " 7.2731e-02\n",
      "-3.5440e-02\n",
      "-1.0419e-01\n",
      " 3.7469e-02\n",
      "-1.8593e-01\n",
      "-1.4709e-01\n",
      "-1.8496e-01\n",
      " 2.6744e-03\n",
      " 2.8728e-02\n",
      "-2.6323e-01\n",
      "-5.5893e-02\n",
      "-9.7249e-02\n",
      "-4.0118e-02\n",
      "-1.1255e-01\n",
      " 1.3047e-02\n",
      " 3.2656e-02\n",
      "-1.0364e-01\n",
      " 4.6739e-03\n",
      "-1.3257e-01\n",
      " 3.6673e-01\n",
      "-1.4873e-01\n",
      "-6.3823e-02\n",
      "-6.4041e-02\n",
      "-2.1785e-01\n",
      "-1.6057e-01\n",
      "-5.5695e-02\n",
      "-2.4023e-01\n",
      "-2.2040e-01\n",
      "-1.6880e-01\n",
      "-9.9183e-02\n",
      "-1.4885e-01\n",
      "-8.1371e-02\n",
      "-1.5586e-01\n",
      "-8.4445e-02\n",
      " 1.4985e-01\n",
      "-1.6860e-01\n",
      "-7.0004e-02\n",
      "-1.2017e-01\n",
      "-1.5527e-01\n",
      "-1.6140e-01\n",
      "-1.8751e-02\n",
      "-1.1364e-01\n",
      "-1.4418e-01\n",
      "-1.1894e-01\n",
      "-8.4666e-02\n",
      " 5.9056e-02\n",
      "-9.4679e-02\n",
      "-8.6236e-02\n",
      "-3.0550e-02\n",
      "-2.2582e-01\n",
      "-1.7862e-02\n",
      "-2.0602e-01\n",
      " 3.1158e-02\n",
      "-5.0545e-02\n",
      "-1.8904e-01\n",
      "-1.0406e-01\n",
      "-1.0358e-01\n",
      "-3.2140e-01\n",
      "-1.4882e-02\n",
      " 3.8386e-01\n",
      "-1.8194e-01\n",
      "-6.7050e-02\n",
      "-8.5238e-02\n",
      "-1.7742e-01\n",
      "-2.0226e-01\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  1.2505e-02  7.9107e-03  1.1970e-02\n",
      "  3.0148e-04 -2.4728e-03 -7.1444e-04\n",
      " -7.7601e-03  3.7900e-03  1.7035e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -9.1776e-03 -2.1741e-03  2.4527e-04\n",
      "  5.7286e-03 -2.2334e-02 -2.8165e-02\n",
      "  1.5033e-02  1.6996e-03 -2.6149e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -7.7734e-03 -2.5277e-03 -1.2466e-02\n",
      " -2.6672e-04 -1.0823e-02 -1.1640e-02\n",
      "  1.8392e-02  2.0205e-02  2.1632e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,253,.,.) = \n",
      " -1.8646e-03  2.0642e-02  1.8513e-02\n",
      "  1.6098e-04  2.6170e-02  1.8682e-02\n",
      " -7.5778e-03  1.0868e-03 -4.2816e-03\n",
      "\n",
      "( 0 ,254,.,.) = \n",
      " -1.0706e-02 -2.2162e-02 -2.4470e-02\n",
      "  1.0641e-02 -9.8604e-03 -2.1600e-02\n",
      "  1.5523e-02  4.7531e-03 -8.5181e-03\n",
      "\n",
      "( 0 ,255,.,.) = \n",
      " -1.0360e-02 -2.6568e-02 -1.7532e-02\n",
      "  5.6697e-03 -2.1333e-02 -1.9429e-02\n",
      "  1.3432e-02 -1.2397e-02 -1.6371e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -5.3482e-02 -5.5630e-02  1.5779e-02\n",
      " -3.4542e-02 -1.6393e-02  2.0218e-02\n",
      " -2.5866e-02 -3.4652e-03  1.4959e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  1.4931e-02  2.5884e-02  2.0029e-02\n",
      "  1.9960e-02  1.5160e-02 -2.3060e-03\n",
      "  3.9001e-03 -1.2546e-02 -1.2270e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  1.7053e-02  4.0946e-03  1.3693e-02\n",
      "  7.6720e-03  1.0501e-03 -7.8980e-03\n",
      "  4.0185e-03  1.1082e-02 -1.5241e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,253,.,.) = \n",
      "  1.4450e-02 -6.9438e-03 -7.1343e-03\n",
      "  3.5883e-02  1.2564e-02  5.7031e-03\n",
      "  1.1010e-02  3.1994e-03 -4.2265e-03\n",
      "\n",
      "( 1 ,254,.,.) = \n",
      "  6.0906e-03  5.5716e-03 -1.4777e-03\n",
      "  2.1531e-03 -3.3041e-03  9.0801e-03\n",
      "  2.6943e-03  1.2804e-02  2.2257e-02\n",
      "\n",
      "( 1 ,255,.,.) = \n",
      "  2.1636e-02 -1.0405e-02 -2.8505e-02\n",
      "  1.4905e-02 -5.0697e-03 -2.6206e-02\n",
      " -1.0102e-02 -1.0966e-02 -2.3589e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -3.0991e-03  5.5597e-03 -1.6376e-02\n",
      " -1.0858e-02 -1.7996e-02 -3.8245e-02\n",
      "  1.7603e-02  3.3346e-03 -1.2956e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  6.7481e-03  1.7991e-02  7.8287e-04\n",
      "  2.0972e-03  3.8234e-02 -1.2792e-02\n",
      "  8.9090e-03  7.5745e-04 -1.3295e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  1.5234e-02 -2.6580e-03  3.3076e-03\n",
      "  8.6489e-03 -3.2470e-02  4.9063e-03\n",
      " -9.9652e-03 -7.3268e-03  2.5742e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,253,.,.) = \n",
      "  4.3120e-02  5.3305e-02  3.2600e-02\n",
      "  3.9540e-02  6.1059e-02  2.6805e-02\n",
      "  7.3027e-03  2.0452e-02 -2.8610e-03\n",
      "\n",
      "( 2 ,254,.,.) = \n",
      "  9.3750e-03 -2.4107e-02 -4.2767e-02\n",
      "  1.9428e-02 -1.3794e-02 -4.9217e-02\n",
      "  1.5995e-02  6.8787e-03 -4.4238e-03\n",
      "\n",
      "( 2 ,255,.,.) = \n",
      "  2.7556e-02  1.5650e-02 -2.7977e-02\n",
      "  3.7310e-02  5.0936e-03 -3.8661e-02\n",
      "  2.0660e-02 -1.2493e-02 -3.2654e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(253, 0 ,.,.) = \n",
      " -5.2199e-03 -1.7422e-02 -4.7407e-03\n",
      "  5.6415e-03  7.0298e-03  9.5373e-03\n",
      "  1.4030e-03  7.2381e-03 -1.1937e-03\n",
      "\n",
      "(253, 1 ,.,.) = \n",
      " -1.2908e-02 -1.9190e-03  1.0433e-02\n",
      " -2.1696e-02 -1.5480e-02  4.6067e-03\n",
      " -6.2256e-03  1.9952e-02  1.3606e-02\n",
      "\n",
      "(253, 2 ,.,.) = \n",
      "  2.7269e-02  3.6913e-02 -6.7606e-03\n",
      "  3.1173e-02  2.0577e-02 -1.7031e-02\n",
      " -3.7607e-02 -3.6873e-02 -3.3322e-02\n",
      "    ... \n",
      "\n",
      "(253,253,.,.) = \n",
      "  1.2431e-03  1.1925e-02  7.5381e-03\n",
      "  8.2640e-03  1.0593e-02  6.8978e-03\n",
      " -1.5379e-04 -4.0857e-03  4.7171e-03\n",
      "\n",
      "(253,254,.,.) = \n",
      " -4.5796e-03 -1.2389e-03 -1.4682e-03\n",
      " -7.7669e-03 -2.6858e-03 -6.5985e-03\n",
      "  3.7848e-03  6.9213e-03  2.9898e-03\n",
      "\n",
      "(253,255,.,.) = \n",
      "  1.1998e-02  2.3160e-02  1.0107e-02\n",
      "  1.5310e-02  3.7596e-02  2.5305e-02\n",
      "  1.5440e-03  1.6342e-02  1.5667e-03\n",
      "      â‹®  \n",
      "\n",
      "(254, 0 ,.,.) = \n",
      "  3.4206e-03 -2.6214e-03  7.9876e-04\n",
      "  1.0396e-02  5.1976e-04 -3.9619e-03\n",
      "  1.3579e-02  1.0533e-02  1.6738e-03\n",
      "\n",
      "(254, 1 ,.,.) = \n",
      "  1.5683e-02  2.3881e-02  2.9922e-02\n",
      "  1.2990e-02  1.8859e-02  2.4833e-02\n",
      " -8.4480e-03  1.9939e-03 -1.0545e-02\n",
      "\n",
      "(254, 2 ,.,.) = \n",
      "  2.0791e-02  2.3573e-02 -7.0023e-03\n",
      "  2.3714e-02  1.6940e-02  1.2695e-03\n",
      "  2.4726e-02  8.9348e-03  7.8627e-03\n",
      "    ... \n",
      "\n",
      "(254,253,.,.) = \n",
      "  5.5776e-03  9.6667e-03  4.5073e-03\n",
      "  1.1197e-02  6.0557e-03 -4.2492e-03\n",
      " -5.9892e-03 -3.3064e-03 -1.0841e-02\n",
      "\n",
      "(254,254,.,.) = \n",
      "  7.1190e-03  3.3621e-03  3.1321e-03\n",
      " -1.2396e-04 -3.0591e-03 -1.1100e-02\n",
      " -6.5985e-03 -1.3980e-02 -1.9674e-02\n",
      "\n",
      "(254,255,.,.) = \n",
      "  2.4047e-02  3.4868e-02  3.4531e-02\n",
      "  2.8444e-03  2.8932e-03 -4.1821e-03\n",
      "  3.3091e-02  2.9753e-02  8.4767e-03\n",
      "      â‹®  \n",
      "\n",
      "(255, 0 ,.,.) = \n",
      " -1.1738e-02 -4.5313e-02 -1.1532e-02\n",
      "  1.0945e-02 -1.6695e-02  1.9661e-02\n",
      "  9.1157e-04 -6.6247e-03  6.3696e-03\n",
      "\n",
      "(255, 1 ,.,.) = \n",
      " -1.2436e-02 -2.7512e-02 -1.1284e-02\n",
      " -2.5097e-03 -2.2178e-02 -1.4738e-02\n",
      "  1.4338e-02  1.2489e-02  1.2360e-03\n",
      "\n",
      "(255, 2 ,.,.) = \n",
      " -1.8245e-02  1.8952e-03 -2.9768e-02\n",
      "  2.9582e-03  1.6733e-03 -7.6865e-02\n",
      " -4.2371e-02 -3.2234e-02 -4.3198e-02\n",
      "    ... \n",
      "\n",
      "(255,253,.,.) = \n",
      "  8.3927e-03  8.3685e-03 -9.5064e-03\n",
      "  1.7132e-02  2.3685e-02 -1.4457e-02\n",
      "  1.5506e-02  1.9853e-02 -8.6863e-03\n",
      "\n",
      "(255,254,.,.) = \n",
      " -6.9898e-03  2.0012e-04  6.8767e-03\n",
      " -1.1506e-02 -2.1149e-02  4.7425e-03\n",
      " -3.1301e-03 -6.0912e-03  3.6092e-02\n",
      "\n",
      "(255,255,.,.) = \n",
      "  1.6509e-02  3.5125e-03 -1.2745e-02\n",
      " -1.3199e-02 -2.3221e-02 -8.6577e-03\n",
      " -4.6032e-03 -2.6304e-03  2.3440e-02\n",
      "[torch.FloatTensor of size 256x256x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      "  1.3911\n",
      "  0.2375\n",
      " -0.6850\n",
      "  0.4713\n",
      "  0.1481\n",
      " -2.0585\n",
      " -0.1464\n",
      " -0.2714\n",
      " -0.4667\n",
      " -0.8170\n",
      "  0.3060\n",
      "  0.3034\n",
      "  0.4691\n",
      "  0.5464\n",
      "  0.3865\n",
      " -0.2626\n",
      " -0.2904\n",
      " -1.0693\n",
      " -0.1266\n",
      "  0.0602\n",
      " -0.2967\n",
      " -0.5492\n",
      " -0.8884\n",
      " -0.5661\n",
      " -1.3270\n",
      " -0.7383\n",
      " -0.6105\n",
      " -0.3453\n",
      "  0.3595\n",
      "  0.0205\n",
      " -0.7113\n",
      "  0.6785\n",
      "  0.2599\n",
      " -0.1284\n",
      " -0.0920\n",
      " -0.1842\n",
      "  0.9498\n",
      " -1.5615\n",
      " -1.0374\n",
      " -0.6780\n",
      " -0.5069\n",
      "  0.1909\n",
      " -0.0203\n",
      " -1.3107\n",
      "  1.0693\n",
      "  0.4710\n",
      " -0.1618\n",
      "  0.2564\n",
      "  0.0554\n",
      " -0.5437\n",
      " -0.3056\n",
      "  0.0148\n",
      "  0.6295\n",
      "  0.6670\n",
      " -0.0091\n",
      "  0.4281\n",
      "  1.0142\n",
      "  1.2990\n",
      " -0.9008\n",
      " -0.7560\n",
      " -0.5003\n",
      "  0.1604\n",
      "  0.0261\n",
      " -0.3099\n",
      " -0.1429\n",
      " -0.5659\n",
      " -0.5539\n",
      " -1.5906\n",
      " -0.4137\n",
      " -0.3283\n",
      "  1.1340\n",
      "  1.3104\n",
      " -0.1697\n",
      "  0.4818\n",
      " -0.8091\n",
      "  0.8221\n",
      " -0.5758\n",
      " -0.7009\n",
      " -0.4562\n",
      " -1.5151\n",
      "  0.1311\n",
      " -0.6317\n",
      " -0.0562\n",
      "  0.5898\n",
      " -0.7471\n",
      "  0.8547\n",
      "  0.2226\n",
      " -0.2451\n",
      "  0.9591\n",
      "  0.1968\n",
      " -0.0259\n",
      "  0.5925\n",
      " -0.1937\n",
      "  0.4912\n",
      " -0.2985\n",
      " -1.0145\n",
      " -0.4275\n",
      " -0.5786\n",
      "  1.2528\n",
      "  0.0871\n",
      "  0.1329\n",
      "  0.0467\n",
      " -0.4901\n",
      "  1.7511\n",
      " -0.5162\n",
      " -1.3300\n",
      "  1.4463\n",
      " -0.6887\n",
      " -0.5246\n",
      " -0.2035\n",
      "  0.1839\n",
      "  0.1726\n",
      "  0.9000\n",
      "  0.3244\n",
      " -0.7484\n",
      "  1.3335\n",
      "  0.7779\n",
      " -0.6711\n",
      " -0.4445\n",
      " -0.2030\n",
      " -0.4920\n",
      "  0.3440\n",
      "  0.1271\n",
      " -0.6228\n",
      "  0.4876\n",
      " -0.2866\n",
      "  0.6373\n",
      "  0.1009\n",
      "  0.1771\n",
      "  1.3632\n",
      "  0.0619\n",
      "  0.3218\n",
      "  0.5323\n",
      "  2.1286\n",
      "  0.7661\n",
      " -1.0959\n",
      " -0.3075\n",
      " -0.4526\n",
      "  0.6377\n",
      " -1.1092\n",
      "  0.2865\n",
      " -0.6532\n",
      " -0.4138\n",
      " -0.1252\n",
      " -0.0242\n",
      " -0.2759\n",
      "  0.0415\n",
      "  1.0812\n",
      " -0.2551\n",
      "  0.1148\n",
      " -0.7969\n",
      " -0.0114\n",
      " -0.9026\n",
      " -0.1972\n",
      "  0.4405\n",
      " -0.2398\n",
      "  0.1232\n",
      " -0.2257\n",
      "  0.3127\n",
      " -0.8418\n",
      "  0.8542\n",
      " -0.9761\n",
      "  0.4192\n",
      " -0.8153\n",
      " -0.1633\n",
      "  0.5189\n",
      "  0.2796\n",
      "  0.7563\n",
      "  0.6955\n",
      "  1.1804\n",
      " -1.0197\n",
      "  0.2006\n",
      " -0.3461\n",
      "  0.5623\n",
      "  0.4421\n",
      " -0.8445\n",
      "  0.4222\n",
      "  0.2279\n",
      " -0.4772\n",
      "  0.0516\n",
      " -1.2081\n",
      "  0.0306\n",
      " -1.2279\n",
      " -0.4683\n",
      " -0.0002\n",
      " -0.2383\n",
      "  0.0092\n",
      " -0.0283\n",
      " -0.0394\n",
      " -0.4279\n",
      "  0.6130\n",
      " -1.0832\n",
      " -0.0517\n",
      "  0.2522\n",
      "  0.7765\n",
      " -1.4693\n",
      " -0.4386\n",
      " -0.2152\n",
      " -0.6271\n",
      " -0.5685\n",
      " -1.7504\n",
      " -0.8335\n",
      " -0.0157\n",
      " -0.0174\n",
      " -1.0514\n",
      " -0.1205\n",
      " -0.4915\n",
      "  0.2710\n",
      "  0.6605\n",
      "  0.1892\n",
      "  1.6656\n",
      " -0.1960\n",
      "  0.7592\n",
      " -0.1856\n",
      " -0.8667\n",
      " -0.2538\n",
      "  1.5773\n",
      "  0.6442\n",
      "  1.4756\n",
      " -1.0859\n",
      " -0.4682\n",
      "  0.1814\n",
      " -0.6284\n",
      "  0.6061\n",
      "  0.4133\n",
      "  1.3512\n",
      "  0.2327\n",
      "  1.1345\n",
      " -0.1552\n",
      "  0.2978\n",
      "  0.4817\n",
      "  0.3445\n",
      "  1.0703\n",
      " -0.6218\n",
      " -1.2537\n",
      "  1.0444\n",
      "  0.2744\n",
      "  0.7779\n",
      "  0.8184\n",
      " -0.5567\n",
      " -0.2465\n",
      " -0.2771\n",
      " -0.5108\n",
      " -0.9447\n",
      " -0.0398\n",
      " -0.6899\n",
      " -0.2165\n",
      "  0.8613\n",
      "  0.6614\n",
      " -0.0339\n",
      " -0.1979\n",
      "  0.7917\n",
      " -0.0671\n",
      "  0.2609\n",
      "  0.4860\n",
      " -0.8549\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2229\n",
      " 0.2738\n",
      " 0.1467\n",
      " 0.2315\n",
      " 0.2662\n",
      " 0.1763\n",
      " 0.2645\n",
      " 0.2450\n",
      " 0.2701\n",
      " 0.2908\n",
      " 0.2148\n",
      " 0.2322\n",
      " 0.3159\n",
      " 0.3715\n",
      " 0.2376\n",
      " 0.2174\n",
      " 0.2908\n",
      " 0.1920\n",
      " 0.2372\n",
      " 0.1513\n",
      " 0.3215\n",
      " 0.1745\n",
      " 0.3390\n",
      " 0.2183\n",
      " 0.1550\n",
      " 0.2280\n",
      " 0.2504\n",
      " 0.2206\n",
      " 0.2970\n",
      " 0.2486\n",
      " 0.2460\n",
      " 0.2848\n",
      " 0.2230\n",
      " 0.2433\n",
      " 0.3511\n",
      " 0.2885\n",
      " 0.2157\n",
      " 0.2508\n",
      " 0.2721\n",
      " 0.2893\n",
      " 0.1819\n",
      " 0.2720\n",
      " 0.2446\n",
      " 0.2052\n",
      " 0.2491\n",
      " 0.1964\n",
      " 0.2870\n",
      " 0.2381\n",
      " 0.3451\n",
      " 0.2832\n",
      " 0.2473\n",
      " 0.2157\n",
      " 0.2209\n",
      " 0.2776\n",
      " 0.2672\n",
      " 0.2479\n",
      " 0.2791\n",
      " 0.2368\n",
      " 0.1541\n",
      " 0.2635\n",
      " 0.3062\n",
      " 0.2193\n",
      " 0.2467\n",
      " 0.2424\n",
      " 0.2331\n",
      " 0.2185\n",
      " 0.2492\n",
      " 0.3522\n",
      " 0.1903\n",
      " 0.2250\n",
      " 0.2598\n",
      " 0.2256\n",
      " 0.2910\n",
      " 0.3066\n",
      " 0.3529\n",
      " 0.1552\n",
      " 0.2588\n",
      " 0.1464\n",
      " 0.2869\n",
      " 0.2135\n",
      " 0.2597\n",
      " 0.1978\n",
      " 0.1780\n",
      " 0.3199\n",
      " 0.2063\n",
      " 0.2141\n",
      " 0.2614\n",
      " 0.2190\n",
      " 0.3255\n",
      " 0.2492\n",
      " 0.2554\n",
      " 0.2404\n",
      " 0.2757\n",
      " 0.1880\n",
      " 0.1527\n",
      " 0.2145\n",
      " 0.2961\n",
      " 0.2771\n",
      " 0.2331\n",
      " 0.2474\n",
      " 0.2400\n",
      " 0.1917\n",
      " 0.2399\n",
      " 0.2019\n",
      " 0.2841\n",
      " 0.2879\n",
      " 0.2912\n",
      " 0.2925\n",
      " 0.2788\n",
      " 0.1887\n",
      " 0.3470\n",
      " 0.2532\n",
      " 0.2690\n",
      " 0.2455\n",
      " 0.2422\n",
      " 0.2634\n",
      " 0.1502\n",
      " 0.3303\n",
      " 0.2775\n",
      " 0.2335\n",
      " 0.2571\n",
      " 0.3667\n",
      " 0.2629\n",
      " 0.2819\n",
      " 0.2864\n",
      " 0.3346\n",
      " 0.2744\n",
      " 0.2163\n",
      " 0.2944\n",
      " 0.2366\n",
      " 0.2277\n",
      " 0.1732\n",
      " 0.2495\n",
      " 0.2903\n",
      " 0.1838\n",
      " 0.3094\n",
      " 0.3107\n",
      " 0.2035\n",
      " 0.2828\n",
      " 0.2341\n",
      " 0.2299\n",
      " 0.3155\n",
      " 0.3242\n",
      " 0.2311\n",
      " 0.2249\n",
      " 0.2063\n",
      " 0.2394\n",
      " 0.2866\n",
      " 0.2227\n",
      " 0.2713\n",
      " 0.2429\n",
      " 0.2510\n",
      " 0.2213\n",
      " 0.2326\n",
      " 0.2420\n",
      " 0.2808\n",
      " 0.3386\n",
      " 0.2417\n",
      " 0.2603\n",
      " 0.2326\n",
      " 0.3090\n",
      " 0.2969\n",
      " 0.1975\n",
      " 0.2497\n",
      " 0.2697\n",
      " 0.2171\n",
      " 0.2619\n",
      " 0.3313\n",
      " 0.2781\n",
      " 0.2423\n",
      " 0.2602\n",
      " 0.2350\n",
      " 0.2605\n",
      " 0.2131\n",
      " 0.2549\n",
      " 0.2341\n",
      " 0.2354\n",
      " 0.2667\n",
      " 0.2855\n",
      " 0.1967\n",
      " 0.3020\n",
      " 0.3588\n",
      " 0.2151\n",
      " 0.2412\n",
      " 0.1589\n",
      " 0.2952\n",
      " 0.2195\n",
      " 0.2737\n",
      " 0.2540\n",
      " 0.1787\n",
      " 0.2494\n",
      " 0.3090\n",
      " 0.2515\n",
      " 0.2672\n",
      " 0.2742\n",
      " 0.2962\n",
      " 0.3388\n",
      " 0.2325\n",
      " 0.2331\n",
      " 0.2400\n",
      " 0.2465\n",
      " 0.2594\n",
      " 0.2905\n",
      " 0.2627\n",
      " 0.2722\n",
      " 0.3303\n",
      " 0.2299\n",
      " 0.3178\n",
      " 0.2366\n",
      " 0.3283\n",
      " 0.1879\n",
      " 0.2249\n",
      " 0.1838\n",
      " 0.1594\n",
      " 0.2806\n",
      " 0.2280\n",
      " 0.3382\n",
      " 0.2373\n",
      " 0.2076\n",
      " 0.2279\n",
      " 0.2110\n",
      " 0.2230\n",
      " 0.2914\n",
      " 0.2699\n",
      " 0.2564\n",
      " 0.2878\n",
      " 0.2732\n",
      " 0.2732\n",
      " 0.2163\n",
      " 0.2506\n",
      " 0.2964\n",
      " 0.3542\n",
      " 0.2986\n",
      " 0.3378\n",
      " 0.2201\n",
      " 0.2689\n",
      " 0.1621\n",
      " 0.2255\n",
      " 0.2586\n",
      " 0.1848\n",
      " 0.2151\n",
      " 0.2532\n",
      " 0.2953\n",
      " 0.2493\n",
      " 0.2671\n",
      " 0.2237\n",
      " 0.2076\n",
      " 0.2424\n",
      " 0.1699\n",
      " 0.2602\n",
      " 0.2159\n",
      " 0.1805\n",
      " 0.2358\n",
      " 0.2606\n",
      " 0.2844\n",
      " 0.3288\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      "-0.0756\n",
      "-0.1188\n",
      " 0.1430\n",
      "-0.1068\n",
      "-0.1976\n",
      " 0.0828\n",
      "-0.1658\n",
      "-0.0193\n",
      "-0.2198\n",
      "-0.0682\n",
      "-0.0927\n",
      "-0.1441\n",
      "-0.2894\n",
      "-0.3435\n",
      "-0.1136\n",
      "-0.1341\n",
      "-0.1171\n",
      " 0.1142\n",
      "-0.0833\n",
      "-0.4419\n",
      "-0.2834\n",
      " 0.1273\n",
      "-0.1591\n",
      "-0.1110\n",
      " 0.2037\n",
      "-0.1594\n",
      "-0.1124\n",
      "-0.0770\n",
      "-0.2739\n",
      "-0.1229\n",
      " 0.0073\n",
      "-0.1569\n",
      "-0.0105\n",
      " 0.0634\n",
      "-0.2540\n",
      "-0.1531\n",
      " 0.1376\n",
      "-0.0103\n",
      " 0.0118\n",
      "-0.1740\n",
      "-0.0221\n",
      "-0.2040\n",
      "-0.0767\n",
      " 0.0434\n",
      " 0.0450\n",
      "-0.0937\n",
      "-0.1340\n",
      "-0.2005\n",
      "-0.1538\n",
      "-0.1893\n",
      "-0.1434\n",
      " 0.0840\n",
      "-0.0358\n",
      "-0.3517\n",
      "-0.2102\n",
      "-0.1076\n",
      "-0.1199\n",
      "-0.0950\n",
      " 0.1943\n",
      "-0.3600\n",
      "-0.2097\n",
      "-0.1240\n",
      "-0.0610\n",
      "-0.1189\n",
      "-0.1135\n",
      " 0.0610\n",
      "-0.1288\n",
      "-0.3279\n",
      " 0.1003\n",
      "-0.1225\n",
      " 0.0058\n",
      "-0.0113\n",
      "-0.1618\n",
      "-0.0700\n",
      "-0.2566\n",
      " 0.1128\n",
      "-0.1394\n",
      " 0.2096\n",
      "-0.1563\n",
      " 0.0399\n",
      "-0.1627\n",
      " 0.1390\n",
      " 0.0148\n",
      "-0.0996\n",
      " 0.1645\n",
      "-0.1045\n",
      "-0.1872\n",
      "-0.0612\n",
      "-0.1875\n",
      "-0.1604\n",
      "-0.1541\n",
      "-0.1661\n",
      "-0.1295\n",
      " 0.0569\n",
      " 0.2278\n",
      " 0.0112\n",
      "-0.1760\n",
      "-0.1833\n",
      "-0.1024\n",
      "-0.1282\n",
      "-0.1591\n",
      " 0.0281\n",
      "-0.1005\n",
      "-0.0263\n",
      "-0.1328\n",
      "-0.1282\n",
      "-0.1693\n",
      "-0.2240\n",
      "-0.2350\n",
      " 0.0866\n",
      "-0.2250\n",
      "-0.0149\n",
      "-0.0890\n",
      "-0.0516\n",
      "-0.0254\n",
      " 0.0072\n",
      " 0.1673\n",
      "-0.0901\n",
      "-0.3003\n",
      " 0.0972\n",
      "-0.0221\n",
      "-0.4044\n",
      "-0.2100\n",
      "-0.1240\n",
      "-0.1903\n",
      "-0.2398\n",
      "-0.2028\n",
      "-0.1351\n",
      "-0.1766\n",
      "-0.0064\n",
      "-0.1814\n",
      " 0.0664\n",
      "-0.0433\n",
      "-0.1762\n",
      " 0.1272\n",
      "-0.2020\n",
      "-0.2996\n",
      "-0.0907\n",
      "-0.1864\n",
      " 0.1178\n",
      "-0.1487\n",
      "-0.2579\n",
      "-0.1898\n",
      " 0.0072\n",
      " 0.1103\n",
      " 0.0317\n",
      "-0.0645\n",
      "-0.0862\n",
      "-0.0163\n",
      "-0.0861\n",
      "-0.0153\n",
      "-0.1478\n",
      " 0.0285\n",
      "-0.0556\n",
      "-0.1560\n",
      "-0.2313\n",
      "-0.2477\n",
      "-0.2654\n",
      "-0.0874\n",
      "-0.1327\n",
      "-0.1113\n",
      "-0.1415\n",
      "-0.0929\n",
      "-0.0469\n",
      "-0.0750\n",
      " 0.0245\n",
      "-0.1579\n",
      "-0.2106\n",
      "-0.1541\n",
      "-0.0046\n",
      "-0.1085\n",
      "-0.0681\n",
      "-0.1182\n",
      "-0.0723\n",
      "-0.0552\n",
      "-0.0555\n",
      "-0.1324\n",
      "-0.2685\n",
      "-0.2049\n",
      " 0.1388\n",
      "-0.2128\n",
      "-0.1799\n",
      " 0.0468\n",
      "-0.0970\n",
      "-0.4013\n",
      "-0.2355\n",
      "-0.1790\n",
      "-0.1845\n",
      "-0.0457\n",
      " 0.0881\n",
      "-0.0940\n",
      "-0.1881\n",
      "-0.1795\n",
      "-0.1298\n",
      "-0.1429\n",
      "-0.0149\n",
      "-0.3016\n",
      "-0.0255\n",
      "-0.1518\n",
      "-0.1384\n",
      "-0.0710\n",
      "-0.0379\n",
      "-0.1759\n",
      "-0.2185\n",
      " 0.0022\n",
      "-0.2328\n",
      "-0.1080\n",
      "-0.3133\n",
      "-0.1013\n",
      "-0.1575\n",
      " 0.0362\n",
      "-0.0841\n",
      " 0.0409\n",
      " 0.1824\n",
      "-0.2400\n",
      " 0.1254\n",
      "-0.2421\n",
      "-0.1068\n",
      " 0.1050\n",
      " 0.0210\n",
      "-0.0246\n",
      "-0.0559\n",
      "-0.1403\n",
      "-0.1995\n",
      " 0.0270\n",
      "-0.2129\n",
      "-0.1770\n",
      "-0.0665\n",
      "-0.0760\n",
      "-0.0118\n",
      "-0.1489\n",
      "-0.3034\n",
      "-0.0468\n",
      "-0.2564\n",
      "-0.1102\n",
      "-0.0297\n",
      " 0.0459\n",
      " 0.0147\n",
      "-0.0996\n",
      " 0.0010\n",
      "-0.0664\n",
      "-0.1624\n",
      "-0.1685\n",
      "-0.0284\n",
      "-0.2145\n",
      "-0.1366\n",
      "-0.1274\n",
      "-0.0724\n",
      " 0.1352\n",
      "-0.1198\n",
      "-0.1462\n",
      " 0.1078\n",
      "-0.1496\n",
      "-0.1805\n",
      "-0.1250\n",
      "-0.1906\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  7.2159e-03  6.1938e-03  3.2553e-03\n",
      "  3.0948e-03  1.1226e-02  2.1357e-03\n",
      " -5.3846e-03 -2.3347e-02 -1.7436e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  1.9459e-03  1.4333e-02 -7.4505e-04\n",
      "  8.9369e-03  7.4770e-03 -4.1489e-03\n",
      " -3.2817e-03 -1.8606e-02 -4.3984e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -3.6933e-02 -1.1305e-02  2.2641e-02\n",
      " -2.0193e-02 -2.0657e-02 -2.4329e-02\n",
      " -2.2076e-02 -1.2302e-02 -3.2718e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,253,.,.) = \n",
      " -1.6885e-03 -9.4854e-03 -8.1665e-03\n",
      " -1.5497e-02 -1.4882e-02 -1.7082e-02\n",
      " -2.7297e-02 -2.8818e-02 -2.4445e-02\n",
      "\n",
      "( 0 ,254,.,.) = \n",
      "  4.8797e-03 -1.8855e-02 -2.4151e-02\n",
      " -4.4301e-03  6.3198e-03 -3.4529e-03\n",
      "  2.3307e-03  5.2074e-03 -4.4162e-03\n",
      "\n",
      "( 0 ,255,.,.) = \n",
      "  1.5616e-03  5.9050e-03  1.3789e-03\n",
      " -3.0185e-03 -1.7995e-02 -6.4016e-03\n",
      "  1.6362e-02 -8.8422e-03  3.2748e-03\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -1.8096e-02 -1.0237e-02 -7.4799e-03\n",
      " -2.9086e-02 -1.5863e-02 -1.0655e-02\n",
      " -2.4691e-02 -6.5049e-03 -1.1756e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -5.8814e-03  3.3029e-02  1.5653e-02\n",
      "  2.6060e-02  1.6884e-02 -1.2752e-02\n",
      "  2.5226e-02 -5.5701e-03  9.4834e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  3.0330e-02  2.4308e-02 -6.3917e-03\n",
      "  1.2350e-02 -4.4069e-02 -8.9386e-03\n",
      " -1.3506e-02 -2.1499e-02  4.1052e-04\n",
      "    ... \n",
      "\n",
      "( 1 ,253,.,.) = \n",
      " -3.0096e-03 -5.8111e-02 -5.9776e-02\n",
      " -5.5858e-02 -4.8334e-02 -4.1571e-02\n",
      " -3.7091e-02 -2.8072e-02 -2.0977e-02\n",
      "\n",
      "( 1 ,254,.,.) = \n",
      "  2.8268e-02  3.4291e-03 -8.2454e-03\n",
      "  2.9620e-03 -1.3807e-04  1.0199e-02\n",
      "  3.1019e-06  8.7606e-03  1.0806e-03\n",
      "\n",
      "( 1 ,255,.,.) = \n",
      "  1.8036e-02 -2.7587e-02 -1.7477e-02\n",
      " -1.7121e-02 -1.8120e-02 -5.1632e-02\n",
      " -2.0548e-02 -2.7128e-03 -2.5220e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  2.7722e-02 -5.0873e-03 -1.3880e-02\n",
      "  3.1475e-02 -1.9897e-02 -4.4753e-02\n",
      "  3.9381e-02 -2.1693e-02 -4.5605e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  2.9260e-04 -1.6511e-02 -1.2793e-02\n",
      " -1.5569e-02 -1.4027e-02 -4.2105e-03\n",
      " -2.2617e-02 -8.5597e-03  5.4857e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -4.3256e-02 -5.0761e-02 -1.2818e-02\n",
      " -7.8289e-02 -9.2330e-02 -4.2679e-02\n",
      " -6.3605e-02 -6.6820e-02 -1.8024e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,253,.,.) = \n",
      " -2.7215e-02 -3.1051e-03  3.7987e-02\n",
      " -5.2098e-02 -5.3895e-03  4.9848e-02\n",
      " -3.4538e-02 -4.7048e-03  4.3145e-02\n",
      "\n",
      "( 2 ,254,.,.) = \n",
      " -1.7511e-02 -6.5660e-03  1.1666e-02\n",
      " -3.8368e-02 -8.8878e-03  7.5903e-03\n",
      " -2.6674e-02 -7.6973e-03 -1.4697e-02\n",
      "\n",
      "( 2 ,255,.,.) = \n",
      " -1.2933e-02  9.0555e-03  1.0510e-02\n",
      " -2.9216e-02  7.8338e-03  1.0775e-02\n",
      " -2.0350e-02 -1.6260e-04  7.0924e-03\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(253, 0 ,.,.) = \n",
      "  1.7180e-02  2.1544e-02  2.2405e-02\n",
      "  1.4178e-02  1.5080e-02  1.0746e-02\n",
      "  9.0673e-03  1.7004e-02  1.7629e-02\n",
      "\n",
      "(253, 1 ,.,.) = \n",
      " -1.3079e-02 -2.1228e-02 -1.0561e-03\n",
      " -2.3574e-03 -1.0917e-02 -2.3914e-03\n",
      " -2.8845e-03 -1.0723e-02  1.2310e-02\n",
      "\n",
      "(253, 2 ,.,.) = \n",
      "  4.0852e-02  5.3646e-03  1.7732e-02\n",
      "  3.6805e-02  4.7679e-03  8.1881e-04\n",
      "  3.2882e-02 -1.4982e-02  1.9149e-03\n",
      "    ... \n",
      "\n",
      "(253,253,.,.) = \n",
      " -1.6188e-02 -4.2585e-03  9.0532e-03\n",
      " -2.6967e-03 -7.2355e-03  1.4060e-02\n",
      "  5.4425e-03  1.7314e-03  1.0242e-02\n",
      "\n",
      "(253,254,.,.) = \n",
      "  4.6637e-03  2.1685e-02  1.2009e-02\n",
      "  2.5506e-02  2.3127e-02  1.7302e-02\n",
      "  1.3551e-03  9.0372e-03  5.9444e-03\n",
      "\n",
      "(253,255,.,.) = \n",
      " -8.4297e-03  1.7806e-02  4.2301e-02\n",
      " -1.1025e-02  1.8231e-02  3.0108e-02\n",
      " -5.4335e-02 -1.8521e-03  8.2721e-03\n",
      "      â‹®  \n",
      "\n",
      "(254, 0 ,.,.) = \n",
      " -2.4175e-02 -1.3145e-02 -3.2330e-02\n",
      " -6.0321e-03 -4.1535e-03 -1.8430e-02\n",
      "  1.7790e-03  4.5927e-03 -1.9787e-02\n",
      "\n",
      "(254, 1 ,.,.) = \n",
      " -4.4042e-02 -1.1293e-02 -6.9378e-03\n",
      " -1.0734e-02 -2.0437e-02 -2.2934e-02\n",
      "  1.4587e-02 -4.2373e-02 -8.0364e-03\n",
      "\n",
      "(254, 2 ,.,.) = \n",
      "  3.6772e-03 -3.2922e-02  1.0060e-02\n",
      " -5.5952e-02 -1.1995e-02  3.7000e-02\n",
      " -4.2430e-02  5.6211e-02  1.7574e-02\n",
      "    ... \n",
      "\n",
      "(254,253,.,.) = \n",
      "  2.7303e-02  2.5257e-02  8.4235e-04\n",
      " -1.0613e-02 -2.8363e-02 -2.7571e-02\n",
      " -1.7892e-02 -3.5854e-02 -7.7791e-03\n",
      "\n",
      "(254,254,.,.) = \n",
      " -1.1118e-02 -8.9572e-03 -2.9971e-03\n",
      "  2.4592e-03 -1.6922e-03 -7.0871e-03\n",
      "  9.0442e-03 -8.6740e-03 -1.7927e-02\n",
      "\n",
      "(254,255,.,.) = \n",
      "  1.3977e-02 -3.3713e-03  1.3335e-02\n",
      "  1.2884e-02 -2.6743e-03  8.3689e-03\n",
      " -1.6016e-02 -1.2200e-02  6.7409e-03\n",
      "      â‹®  \n",
      "\n",
      "(255, 0 ,.,.) = \n",
      " -2.9914e-02 -1.4643e-02  2.2400e-02\n",
      " -7.6800e-03 -2.6185e-02 -6.6164e-03\n",
      "  1.7573e-02 -1.8487e-02 -2.4450e-02\n",
      "\n",
      "(255, 1 ,.,.) = \n",
      " -2.1340e-02 -1.4493e-02 -2.1073e-02\n",
      "  1.4740e-02  4.0651e-03 -1.3604e-03\n",
      "  2.1751e-02  3.0878e-02  5.1449e-02\n",
      "\n",
      "(255, 2 ,.,.) = \n",
      "  3.2325e-02  3.2633e-02  1.3462e-02\n",
      "  5.4463e-02  4.6127e-02 -4.7525e-03\n",
      "  1.9020e-02  9.5310e-03 -1.2283e-02\n",
      "    ... \n",
      "\n",
      "(255,253,.,.) = \n",
      " -2.4557e-03 -1.6110e-02 -2.6660e-02\n",
      " -1.5518e-02 -2.9606e-02 -3.3188e-02\n",
      " -1.0938e-02 -1.1202e-02 -1.3262e-02\n",
      "\n",
      "(255,254,.,.) = \n",
      " -3.7944e-02 -3.2402e-02 -3.0042e-02\n",
      " -4.9598e-02 -4.1434e-02 -3.5783e-02\n",
      " -3.2443e-02 -3.3293e-02 -4.8449e-02\n",
      "\n",
      "(255,255,.,.) = \n",
      " -1.0654e-02 -2.3267e-02 -3.4944e-02\n",
      " -4.1760e-02 -3.1650e-02 -6.2190e-03\n",
      "  3.8281e-03 -1.7281e-02 -1.2613e-02\n",
      "[torch.FloatTensor of size 256x256x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      "  0.2399\n",
      "  1.0717\n",
      " -0.1666\n",
      "  0.7960\n",
      " -2.6468\n",
      "  0.7113\n",
      "  0.6150\n",
      " -0.4295\n",
      " -0.5359\n",
      "  0.6283\n",
      "  0.5549\n",
      "  0.3660\n",
      "  0.8794\n",
      " -1.2987\n",
      " -0.3488\n",
      " -0.4516\n",
      "  0.3409\n",
      " -0.9388\n",
      "  0.3864\n",
      "  0.3305\n",
      " -0.8419\n",
      " -0.6958\n",
      "  0.5849\n",
      "  0.3250\n",
      "  0.0225\n",
      " -1.1576\n",
      "  0.5491\n",
      "  0.6181\n",
      " -0.5889\n",
      "  0.3564\n",
      "  0.7510\n",
      "  0.8347\n",
      "  0.8798\n",
      " -0.0891\n",
      " -0.2405\n",
      "  1.4774\n",
      " -0.6498\n",
      "  0.1387\n",
      "  0.0488\n",
      " -0.5935\n",
      "  0.2196\n",
      "  0.7246\n",
      "  0.1800\n",
      " -0.9101\n",
      "  0.5292\n",
      "  0.8826\n",
      "  0.7789\n",
      " -0.6167\n",
      " -0.3282\n",
      "  0.6659\n",
      " -0.5001\n",
      "  1.0237\n",
      " -0.1157\n",
      " -0.8047\n",
      "  0.3587\n",
      " -0.8954\n",
      " -0.4537\n",
      "  1.1864\n",
      "  1.4547\n",
      "  1.6725\n",
      "  0.5771\n",
      "  1.2638\n",
      " -0.0308\n",
      "  0.1029\n",
      " -1.6303\n",
      " -0.6170\n",
      " -1.1548\n",
      " -0.1929\n",
      "  0.8139\n",
      "  2.0942\n",
      "  0.0818\n",
      "  1.0214\n",
      "  1.2189\n",
      "  0.5783\n",
      " -0.4109\n",
      " -0.4104\n",
      " -0.8182\n",
      " -0.7710\n",
      " -0.4614\n",
      "  1.0474\n",
      "  0.8172\n",
      "  0.1365\n",
      " -0.7674\n",
      "  0.9938\n",
      " -2.0313\n",
      "  0.2862\n",
      " -0.7310\n",
      " -0.3548\n",
      "  1.0021\n",
      "  0.5918\n",
      " -0.5424\n",
      " -0.7957\n",
      " -0.4456\n",
      " -0.1729\n",
      "  0.8489\n",
      "  0.2075\n",
      "  0.0943\n",
      " -0.5716\n",
      " -0.9900\n",
      "  0.2858\n",
      "  0.2458\n",
      "  0.3779\n",
      " -0.4524\n",
      "  0.1446\n",
      " -0.1489\n",
      " -0.3785\n",
      "  0.4796\n",
      "  0.5732\n",
      " -1.0580\n",
      " -0.0723\n",
      " -0.6456\n",
      " -0.6585\n",
      "  0.5600\n",
      "  0.3673\n",
      "  0.5714\n",
      "  0.8968\n",
      " -1.4077\n",
      " -0.4691\n",
      "  1.3367\n",
      " -0.5407\n",
      " -0.8750\n",
      "  0.3743\n",
      " -0.1131\n",
      "  0.3403\n",
      "  0.1466\n",
      " -0.5479\n",
      " -0.9158\n",
      " -0.6641\n",
      "  0.9490\n",
      " -0.0010\n",
      "  0.3884\n",
      " -0.1518\n",
      "  0.5123\n",
      " -0.4118\n",
      "  0.0378\n",
      " -0.9974\n",
      "  0.2665\n",
      "  0.1375\n",
      " -0.7563\n",
      "  1.5434\n",
      "  0.4280\n",
      "  0.6585\n",
      " -0.4302\n",
      " -0.1885\n",
      " -0.1595\n",
      " -0.6369\n",
      "  1.0584\n",
      " -0.1198\n",
      " -0.7943\n",
      " -0.3685\n",
      "  1.4099\n",
      " -1.1663\n",
      " -0.7800\n",
      " -0.2741\n",
      "  0.0856\n",
      "  0.1759\n",
      "  0.9168\n",
      "  0.0432\n",
      "  0.2618\n",
      " -0.0024\n",
      " -0.2569\n",
      " -0.4330\n",
      "  1.4665\n",
      " -0.5327\n",
      "  0.4489\n",
      " -0.0844\n",
      "  0.7042\n",
      " -0.0997\n",
      " -1.1381\n",
      " -0.0694\n",
      " -0.3941\n",
      " -0.3290\n",
      "  0.7085\n",
      " -0.0435\n",
      " -0.9155\n",
      "  1.3462\n",
      " -2.0066\n",
      "  1.1438\n",
      "  0.0521\n",
      "  1.5267\n",
      "  0.1298\n",
      "  0.7055\n",
      "  0.3534\n",
      "  0.0089\n",
      "  0.0554\n",
      " -0.2249\n",
      "  0.6070\n",
      "  1.0922\n",
      "  0.0829\n",
      "  0.2351\n",
      "  0.6313\n",
      " -1.9959\n",
      " -0.6012\n",
      " -0.9164\n",
      "  0.5880\n",
      " -0.3723\n",
      "  1.6049\n",
      " -0.1998\n",
      " -0.6836\n",
      "  0.9962\n",
      "  1.1539\n",
      " -0.9077\n",
      " -1.1159\n",
      "  0.0930\n",
      "  0.1587\n",
      " -0.9692\n",
      " -0.3691\n",
      "  0.0319\n",
      "  1.2610\n",
      " -0.3103\n",
      " -1.0125\n",
      " -0.9766\n",
      " -1.6048\n",
      " -0.5113\n",
      "  0.1317\n",
      "  1.2863\n",
      " -1.9149\n",
      " -0.3328\n",
      "  0.7284\n",
      " -0.9314\n",
      "  1.4545\n",
      " -0.4536\n",
      " -0.2940\n",
      " -0.0384\n",
      " -0.0087\n",
      " -0.7544\n",
      " -0.1884\n",
      " -0.1508\n",
      "  0.3989\n",
      " -0.8760\n",
      "  1.0832\n",
      "  0.2557\n",
      " -0.0494\n",
      "  0.3379\n",
      " -0.0594\n",
      "  0.4053\n",
      "  0.3899\n",
      "  0.0826\n",
      "  0.3411\n",
      " -0.6824\n",
      "  0.6610\n",
      " -0.8532\n",
      " -0.2338\n",
      " -0.4055\n",
      " -1.2742\n",
      "  0.0214\n",
      "  0.4959\n",
      "  0.9646\n",
      " -0.0584\n",
      " -1.1896\n",
      " -0.0100\n",
      "  0.5554\n",
      " -0.1100\n",
      "  0.4981\n",
      "  1.7418\n",
      " -0.2513\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2939\n",
      " 0.2975\n",
      " 0.3333\n",
      " 0.2384\n",
      " 0.3895\n",
      " 0.3126\n",
      " 0.2674\n",
      " 0.2233\n",
      " 0.1884\n",
      " 0.2905\n",
      " 0.3059\n",
      " 0.3204\n",
      " 0.2759\n",
      " 0.3294\n",
      " 0.2702\n",
      " 0.2816\n",
      " 0.2091\n",
      " 0.2393\n",
      " 0.2475\n",
      " 0.2501\n",
      " 0.3364\n",
      " 0.3281\n",
      " 0.3028\n",
      " 0.2976\n",
      " 0.1909\n",
      " 0.2515\n",
      " 0.2955\n",
      " 0.2678\n",
      " 0.2481\n",
      " 0.1981\n",
      " 0.2638\n",
      " 0.3554\n",
      " 0.2666\n",
      " 0.2703\n",
      " 0.2613\n",
      " 0.3070\n",
      " 0.3594\n",
      " 0.2988\n",
      " 0.3051\n",
      " 0.2772\n",
      " 0.2353\n",
      " 0.2747\n",
      " 0.2729\n",
      " 0.2432\n",
      " 0.2814\n",
      " 0.3193\n",
      " 0.2377\n",
      " 0.3107\n",
      " 0.2704\n",
      " 0.2089\n",
      " 0.3451\n",
      " 0.3462\n",
      " 0.3001\n",
      " 0.2341\n",
      " 0.2558\n",
      " 0.2795\n",
      " 0.2979\n",
      " 0.3438\n",
      " 0.3341\n",
      " 0.2856\n",
      " 0.2386\n",
      " 0.3721\n",
      " 0.2331\n",
      " 0.2747\n",
      " 0.2823\n",
      " 0.2379\n",
      " 0.3157\n",
      " 0.2753\n",
      " 0.3306\n",
      " 0.3217\n",
      " 0.2554\n",
      " 0.2956\n",
      " 0.2555\n",
      " 0.2981\n",
      " 0.2665\n",
      " 0.1981\n",
      " 0.3738\n",
      " 0.3755\n",
      " 0.2887\n",
      " 0.2671\n",
      " 0.3335\n",
      " 0.2496\n",
      " 0.2839\n",
      " 0.2865\n",
      " 0.3082\n",
      " 0.3203\n",
      " 0.2586\n",
      " 0.2542\n",
      " 0.2745\n",
      " 0.2796\n",
      " 0.2448\n",
      " 0.2794\n",
      " 0.2592\n",
      " 0.3009\n",
      " 0.2923\n",
      " 0.3303\n",
      " 0.2302\n",
      " 0.2947\n",
      " 0.3246\n",
      " 0.3706\n",
      " 0.2949\n",
      " 0.3042\n",
      " 0.3016\n",
      " 0.2764\n",
      " 0.2633\n",
      " 0.2956\n",
      " 0.3028\n",
      " 0.2651\n",
      " 0.2564\n",
      " 0.2371\n",
      " 0.2790\n",
      " 0.3244\n",
      " 0.2542\n",
      " 0.3079\n",
      " 0.2736\n",
      " 0.3302\n",
      " 0.2833\n",
      " 0.3075\n",
      " 0.2978\n",
      " 0.2500\n",
      " 0.3416\n",
      " 0.3430\n",
      " 0.3291\n",
      " 0.3126\n",
      " 0.3445\n",
      " 0.3497\n",
      " 0.3292\n",
      " 0.2396\n",
      " 0.3021\n",
      " 0.2328\n",
      " 0.2842\n",
      " 0.3025\n",
      " 0.3462\n",
      " 0.2536\n",
      " 0.2670\n",
      " 0.2505\n",
      " 0.3202\n",
      " 0.2936\n",
      " 0.2919\n",
      " 0.3142\n",
      " 0.2978\n",
      " 0.3141\n",
      " 0.2609\n",
      " 0.2806\n",
      " 0.3125\n",
      " 0.2679\n",
      " 0.2309\n",
      " 0.2288\n",
      " 0.2683\n",
      " 0.2615\n",
      " 0.3095\n",
      " 0.3251\n",
      " 0.2715\n",
      " 0.3729\n",
      " 0.2566\n",
      " 0.2728\n",
      " 0.2703\n",
      " 0.2590\n",
      " 0.2472\n",
      " 0.2862\n",
      " 0.2572\n",
      " 0.2992\n",
      " 0.3002\n",
      " 0.2694\n",
      " 0.2736\n",
      " 0.2553\n",
      " 0.2301\n",
      " 0.2867\n",
      " 0.2802\n",
      " 0.2653\n",
      " 0.3558\n",
      " 0.2571\n",
      " 0.2652\n",
      " 0.2197\n",
      " 0.2465\n",
      " 0.2860\n",
      " 0.2323\n",
      " 0.2694\n",
      " 0.2547\n",
      " 0.1846\n",
      " 0.2595\n",
      " 0.2699\n",
      " 0.2533\n",
      " 0.3080\n",
      " 0.2769\n",
      " 0.2298\n",
      " 0.2484\n",
      " 0.3771\n",
      " 0.2599\n",
      " 0.3189\n",
      " 0.2469\n",
      " 0.2390\n",
      " 0.2155\n",
      " 0.2604\n",
      " 0.3073\n",
      " 0.1893\n",
      " 0.1989\n",
      " 0.2465\n",
      " 0.3070\n",
      " 0.2509\n",
      " 0.2974\n",
      " 0.2786\n",
      " 0.2842\n",
      " 0.2762\n",
      " 0.3058\n",
      " 0.2678\n",
      " 0.2988\n",
      " 0.1010\n",
      " 0.3066\n",
      " 0.3075\n",
      " 0.3133\n",
      " 0.2570\n",
      " 0.3048\n",
      " 0.3395\n",
      " 0.2111\n",
      " 0.2438\n",
      " 0.3000\n",
      " 0.2706\n",
      " 0.2454\n",
      " 0.3167\n",
      " 0.2774\n",
      " 0.2979\n",
      " 0.3047\n",
      " 0.2709\n",
      " 0.2353\n",
      " 0.2897\n",
      " 0.3081\n",
      " 0.2953\n",
      " 0.2676\n",
      " 0.3286\n",
      " 0.2565\n",
      " 0.2891\n",
      " 0.3492\n",
      " 0.2635\n",
      " 0.2840\n",
      " 0.2761\n",
      " 0.4059\n",
      " 0.3104\n",
      " 0.2791\n",
      " 0.1998\n",
      " 0.2635\n",
      " 0.3154\n",
      " 0.3183\n",
      " 0.2789\n",
      " 0.2826\n",
      " 0.2509\n",
      " 0.2621\n",
      " 0.2844\n",
      " 0.3657\n",
      " 0.3943\n",
      " 0.2641\n",
      " 0.3234\n",
      " 0.2520\n",
      " 0.3256\n",
      " 0.2666\n",
      " 0.2713\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      "-0.1746\n",
      "-0.2239\n",
      "-0.1652\n",
      "-0.0376\n",
      "-0.3423\n",
      "-0.2927\n",
      "-0.0803\n",
      "-0.1000\n",
      " 0.1877\n",
      "-0.2284\n",
      "-0.1705\n",
      "-0.2399\n",
      "-0.1736\n",
      "-0.2692\n",
      "-0.1144\n",
      "-0.1963\n",
      " 0.1072\n",
      "-0.1141\n",
      "-0.1096\n",
      "-0.1736\n",
      "-0.3239\n",
      "-0.2672\n",
      "-0.2373\n",
      "-0.1842\n",
      "-0.0482\n",
      "-0.1367\n",
      "-0.2098\n",
      "-0.0872\n",
      "-0.1436\n",
      " 0.0871\n",
      "-0.1213\n",
      "-0.2862\n",
      "-0.1867\n",
      "-0.0838\n",
      "-0.1094\n",
      "-0.2764\n",
      "-0.3850\n",
      "-0.0911\n",
      "-0.1308\n",
      "-0.2153\n",
      "-0.0680\n",
      "-0.1989\n",
      "-0.1403\n",
      "-0.0966\n",
      "-0.0878\n",
      "-0.2901\n",
      "-0.1308\n",
      "-0.2113\n",
      "-0.0981\n",
      "-0.1148\n",
      "-0.2434\n",
      "-0.4284\n",
      "-0.2814\n",
      "-0.0921\n",
      "-0.0974\n",
      "-0.2177\n",
      "-0.2497\n",
      "-0.3502\n",
      "-0.3274\n",
      "-0.2105\n",
      "-0.0448\n",
      "-0.3966\n",
      "-0.0856\n",
      "-0.1619\n",
      "-0.2296\n",
      "-0.0521\n",
      "-0.2547\n",
      "-0.1993\n",
      "-0.3355\n",
      "-0.2516\n",
      " 0.0033\n",
      "-0.1455\n",
      "-0.1259\n",
      "-0.1583\n",
      "-0.2398\n",
      " 0.0123\n",
      "-0.2691\n",
      "-0.3970\n",
      "-0.1819\n",
      "-0.0628\n",
      "-0.2922\n",
      "-0.1579\n",
      "-0.1684\n",
      "-0.1148\n",
      "-0.1623\n",
      "-0.2652\n",
      "-0.1366\n",
      "-0.1732\n",
      "-0.1502\n",
      "-0.1475\n",
      " 0.0652\n",
      "-0.1496\n",
      "-0.1356\n",
      "-0.2228\n",
      "-0.1446\n",
      "-0.3221\n",
      "-0.0716\n",
      "-0.1810\n",
      "-0.1745\n",
      "-0.3481\n",
      "-0.2030\n",
      "-0.1570\n",
      "-0.2721\n",
      "-0.1445\n",
      "-0.1312\n",
      "-0.2514\n",
      "-0.2241\n",
      "-0.1383\n",
      "-0.0695\n",
      "-0.0184\n",
      "-0.0595\n",
      "-0.2402\n",
      "-0.2118\n",
      "-0.1669\n",
      "-0.2490\n",
      "-0.3718\n",
      "-0.1588\n",
      "-0.1884\n",
      "-0.2316\n",
      "-0.1229\n",
      "-0.3227\n",
      "-0.3522\n",
      "-0.3592\n",
      "-0.1963\n",
      "-0.1511\n",
      "-0.3617\n",
      "-0.2752\n",
      "-0.1627\n",
      "-0.2743\n",
      "-0.1197\n",
      "-0.2480\n",
      "-0.1828\n",
      "-0.2263\n",
      "-0.1738\n",
      "-0.1027\n",
      "-0.0490\n",
      "-0.2389\n",
      "-0.1311\n",
      "-0.2339\n",
      "-0.2204\n",
      "-0.1312\n",
      "-0.1651\n",
      "-0.1196\n",
      "-0.1256\n",
      "-0.2218\n",
      "-0.1778\n",
      "-0.0608\n",
      " 0.1181\n",
      "-0.1543\n",
      "-0.2047\n",
      "-0.2017\n",
      "-0.3060\n",
      "-0.1151\n",
      "-0.4399\n",
      "-0.0945\n",
      "-0.1796\n",
      "-0.2177\n",
      "-0.1605\n",
      "-0.1359\n",
      "-0.1968\n",
      "-0.0493\n",
      "-0.1984\n",
      "-0.2501\n",
      "-0.1344\n",
      "-0.1837\n",
      "-0.0603\n",
      "-0.0561\n",
      "-0.2135\n",
      "-0.1857\n",
      "-0.0596\n",
      "-0.3573\n",
      "-0.0956\n",
      "-0.1644\n",
      " 0.0761\n",
      "-0.1190\n",
      "-0.1516\n",
      "-0.0858\n",
      "-0.1234\n",
      "-0.1015\n",
      " 0.1026\n",
      "-0.1920\n",
      "-0.2284\n",
      "-0.1343\n",
      "-0.1963\n",
      "-0.1438\n",
      "-0.1434\n",
      "-0.0864\n",
      "-0.3659\n",
      "-0.1085\n",
      "-0.1227\n",
      "-0.1387\n",
      "-0.0819\n",
      "-0.0029\n",
      "-0.1617\n",
      "-0.1263\n",
      "-0.0082\n",
      " 0.0833\n",
      "-0.1006\n",
      "-0.2265\n",
      "-0.1307\n",
      "-0.2105\n",
      "-0.1758\n",
      "-0.2623\n",
      "-0.1005\n",
      "-0.1820\n",
      "-0.1763\n",
      "-0.0680\n",
      "-0.5752\n",
      "-0.1366\n",
      "-0.2571\n",
      "-0.2027\n",
      "-0.0791\n",
      "-0.1342\n",
      "-0.3870\n",
      "-0.0610\n",
      "-0.1084\n",
      "-0.1871\n",
      "-0.0674\n",
      "-0.1271\n",
      "-0.2852\n",
      "-0.2239\n",
      "-0.2102\n",
      "-0.2809\n",
      "-0.1341\n",
      "-0.2127\n",
      "-0.1963\n",
      "-0.2756\n",
      "-0.2639\n",
      "-0.1968\n",
      "-0.2346\n",
      "-0.0721\n",
      "-0.2646\n",
      "-0.5532\n",
      "-0.1102\n",
      "-0.0225\n",
      "-0.3500\n",
      "-0.4820\n",
      "-0.2134\n",
      "-0.1615\n",
      " 0.1085\n",
      "-0.1147\n",
      "-0.1731\n",
      "-0.3279\n",
      "-0.1495\n",
      "-0.1258\n",
      "-0.0876\n",
      "-0.1858\n",
      "-0.2245\n",
      "-0.3960\n",
      "-0.5313\n",
      "-0.1372\n",
      "-0.2550\n",
      "-0.1342\n",
      "-0.2231\n",
      "-0.1532\n",
      "-0.0815\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  1.2287e-02  1.1908e-02  1.3045e-02\n",
      " -1.9937e-03 -1.2367e-02 -1.4888e-02\n",
      " -3.1666e-03  4.8769e-03 -3.7628e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  2.7373e-02  1.2015e-02  1.6725e-02\n",
      "  4.3480e-04 -7.7583e-03  6.9334e-03\n",
      "  3.6951e-02  1.1076e-02  3.2160e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -1.7949e-02 -9.3510e-05 -4.5484e-03\n",
      " -1.6758e-02  3.6146e-03  1.0314e-02\n",
      " -3.6442e-03  7.5933e-03  1.1759e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,253,.,.) = \n",
      "  2.9209e-04  1.8140e-03 -2.1347e-02\n",
      " -7.0248e-03 -2.4290e-03 -3.2098e-02\n",
      "  9.8172e-03  7.6558e-03 -9.7267e-03\n",
      "\n",
      "( 0 ,254,.,.) = \n",
      " -3.0247e-03 -1.8270e-02  4.9140e-03\n",
      "  1.2200e-02  1.6312e-02  2.0588e-02\n",
      "  2.7935e-02  1.8733e-02  2.2131e-02\n",
      "\n",
      "( 0 ,255,.,.) = \n",
      "  5.8050e-03  4.2033e-03  3.3931e-03\n",
      "  8.1246e-03  1.0123e-02 -2.7722e-03\n",
      " -3.9006e-04 -2.9341e-03 -1.2392e-03\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  1.2971e-02  1.8189e-02  1.1990e-02\n",
      " -3.7046e-02 -3.4575e-02 -1.1905e-02\n",
      " -2.2823e-02 -4.8159e-02 -3.0829e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -2.6285e-02 -2.9304e-02 -1.0795e-02\n",
      " -2.2999e-02 -2.9508e-02  4.7941e-03\n",
      " -1.1129e-02 -1.9919e-02 -9.0100e-04\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  7.3477e-03 -9.3022e-03 -1.0729e-02\n",
      "  2.1949e-02 -1.2987e-04  1.4212e-03\n",
      "  2.6406e-02  1.5993e-02 -1.3858e-02\n",
      "    ... \n",
      "\n",
      "( 1 ,253,.,.) = \n",
      " -2.3797e-02 -2.2549e-02 -3.0169e-02\n",
      " -4.2299e-04  3.7005e-03 -4.0712e-03\n",
      "  1.5557e-02  7.1492e-03  2.6787e-02\n",
      "\n",
      "( 1 ,254,.,.) = \n",
      " -2.6543e-03  6.9199e-03 -2.6590e-04\n",
      "  9.3766e-04 -1.9055e-03 -4.9624e-03\n",
      "  2.8171e-03  1.0593e-02 -3.3420e-03\n",
      "\n",
      "( 1 ,255,.,.) = \n",
      " -1.8517e-02 -2.3858e-02 -5.3981e-03\n",
      " -1.8408e-02 -1.8655e-02 -9.1651e-03\n",
      "  7.7142e-05 -2.4914e-03 -4.6179e-03\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  4.9650e-02  1.3717e-02 -4.3677e-03\n",
      "  2.9742e-02  2.1856e-03 -3.3056e-02\n",
      "  2.9993e-02 -1.0114e-03 -1.8461e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  3.1083e-02  1.8211e-02 -2.9622e-02\n",
      "  2.3332e-02 -1.8590e-02 -2.1762e-02\n",
      " -6.0865e-03 -1.1583e-02 -8.4651e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  9.5228e-03  1.7890e-02  2.8622e-02\n",
      "  2.5337e-02  4.2366e-02  2.3846e-02\n",
      "  5.4819e-02  2.2246e-02  1.6567e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,253,.,.) = \n",
      "  1.3428e-02  2.4713e-03 -1.7568e-02\n",
      "  2.0682e-02  5.8922e-04 -3.2121e-02\n",
      "  1.6092e-02  4.4423e-03 -3.4843e-02\n",
      "\n",
      "( 2 ,254,.,.) = \n",
      " -5.4904e-02 -7.1122e-03  4.1862e-02\n",
      " -6.7752e-02 -1.4765e-02  9.7862e-02\n",
      " -2.6654e-02  2.8740e-03  9.6284e-03\n",
      "\n",
      "( 2 ,255,.,.) = \n",
      "  1.7214e-02  9.8162e-03  1.0982e-03\n",
      "  1.0315e-02 -5.3256e-04 -3.2435e-02\n",
      " -1.1407e-02 -1.5998e-03 -4.6315e-03\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      "  2.0736e-03  3.9688e-02  2.4360e-02\n",
      " -1.5048e-02  3.7288e-02  4.1184e-02\n",
      " -3.9328e-02 -3.0355e-03  2.4406e-02\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      "  4.8905e-03  2.5661e-03 -1.2794e-03\n",
      "  7.7867e-03 -7.1434e-03 -1.3159e-02\n",
      "  3.6836e-03 -2.0501e-02 -8.9774e-04\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      "  1.5223e-02 -1.2466e-03 -8.9218e-04\n",
      "  1.1790e-02  2.6401e-03 -3.3433e-02\n",
      " -6.9927e-03  1.7374e-03 -2.0284e-02\n",
      "    ... \n",
      "\n",
      "(509,253,.,.) = \n",
      " -2.1079e-02  7.1344e-03 -2.3420e-02\n",
      " -1.5764e-02  1.7067e-02 -7.4579e-03\n",
      " -8.9243e-03  2.2018e-02  2.1253e-03\n",
      "\n",
      "(509,254,.,.) = \n",
      " -1.3978e-03  1.4757e-02  1.2003e-02\n",
      "  2.7978e-03  6.2250e-03  1.3900e-02\n",
      " -7.4448e-03 -3.1378e-03  4.2320e-03\n",
      "\n",
      "(509,255,.,.) = \n",
      "  4.2634e-03  1.3866e-03  6.2267e-03\n",
      "  1.5004e-02  4.5846e-03 -1.0999e-02\n",
      " -8.2180e-04  1.1310e-02 -1.6399e-02\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      "  1.7170e-02 -1.1932e-02 -1.9361e-02\n",
      " -1.5861e-03 -1.0631e-02 -1.7643e-02\n",
      " -5.9264e-03  6.7630e-04 -4.7628e-03\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      " -3.4297e-02 -9.8041e-03  1.5212e-03\n",
      "  1.9116e-03  8.4026e-03  2.5538e-02\n",
      "  1.5553e-02  2.8249e-02  5.3230e-02\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      "  2.0994e-02 -5.4173e-03 -4.6556e-03\n",
      "  6.0330e-03 -6.1080e-03  4.7463e-03\n",
      "  8.0152e-03 -1.0272e-02  7.1746e-03\n",
      "    ... \n",
      "\n",
      "(510,253,.,.) = \n",
      "  1.3215e-03  3.5769e-02 -6.1879e-03\n",
      "  2.7654e-03  5.3588e-02  1.8354e-03\n",
      "  1.0127e-02  2.6624e-02  1.0735e-02\n",
      "\n",
      "(510,254,.,.) = \n",
      "  1.4529e-03 -7.8042e-03 -1.4792e-03\n",
      "  1.5566e-02  9.3049e-03 -3.3432e-03\n",
      "  1.7075e-02  1.2043e-02  2.2823e-03\n",
      "\n",
      "(510,255,.,.) = \n",
      " -7.7122e-03 -1.6641e-02 -5.2766e-03\n",
      "  6.2606e-03  1.3361e-02  1.0674e-02\n",
      "  9.3765e-03 -1.8740e-03  1.3640e-03\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      " -1.4367e-02 -2.2412e-02 -1.2694e-02\n",
      "  7.4393e-03 -1.7655e-02 -2.5830e-02\n",
      "  3.3679e-02 -7.2136e-03 -2.6410e-02\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      "  4.5678e-03 -8.5253e-03  5.5597e-03\n",
      "  9.4693e-03 -5.0691e-03  1.2225e-02\n",
      "  1.9510e-02  2.3597e-02 -3.9467e-03\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      " -1.7497e-02 -2.3840e-02  1.4181e-02\n",
      "  2.7820e-03 -5.4014e-04  2.5816e-02\n",
      " -1.9683e-02 -1.5668e-02  1.8177e-02\n",
      "    ... \n",
      "\n",
      "(511,253,.,.) = \n",
      "  1.6082e-02 -3.8364e-02 -3.7479e-02\n",
      "  1.1011e-02 -2.9598e-02 -2.5181e-02\n",
      "  2.1228e-02 -2.3804e-02 -2.5693e-02\n",
      "\n",
      "(511,254,.,.) = \n",
      "  1.8112e-02 -3.0497e-03 -8.2922e-03\n",
      "  1.0254e-02  1.2914e-02 -1.3912e-02\n",
      "  8.6725e-03 -4.1542e-03  7.2695e-03\n",
      "\n",
      "(511,255,.,.) = \n",
      "  1.8216e-02  1.9249e-02  2.5204e-02\n",
      " -2.5937e-02  2.6007e-03  1.6339e-02\n",
      " -2.0245e-02  5.5722e-04  1.2079e-02\n",
      "[torch.FloatTensor of size 512x256x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      " -0.1157\n",
      "  0.3509\n",
      "  0.1765\n",
      " -0.0897\n",
      " -0.0235\n",
      " -0.0582\n",
      "  0.0433\n",
      "  0.0505\n",
      " -0.0587\n",
      "  0.0484\n",
      "  0.7893\n",
      " -0.0874\n",
      "  0.1863\n",
      "  0.1051\n",
      "  0.0201\n",
      "  0.0152\n",
      "  0.4417\n",
      "  0.0431\n",
      " -0.2495\n",
      " -0.1087\n",
      " -0.2216\n",
      "  0.0070\n",
      "  0.2318\n",
      "  0.3967\n",
      " -0.4421\n",
      " -0.1997\n",
      "  0.2560\n",
      " -0.1533\n",
      "  0.0986\n",
      "  0.2985\n",
      "  0.0567\n",
      "  0.3481\n",
      " -0.4787\n",
      "  0.4273\n",
      " -0.2823\n",
      " -0.2839\n",
      "  0.0348\n",
      "  0.2432\n",
      " -0.2308\n",
      "  0.2736\n",
      "  0.5814\n",
      " -0.8289\n",
      " -0.2975\n",
      "  0.0843\n",
      "  0.7140\n",
      " -0.0307\n",
      " -0.1597\n",
      " -0.2439\n",
      " -0.5954\n",
      "  0.3960\n",
      " -0.0321\n",
      "  0.1170\n",
      " -0.5713\n",
      " -0.1782\n",
      "  0.2814\n",
      "  0.1808\n",
      " -0.1691\n",
      " -0.2317\n",
      "  0.1381\n",
      " -0.6846\n",
      " -0.0522\n",
      "  0.1513\n",
      " -0.0648\n",
      " -0.1174\n",
      " -0.0685\n",
      " -0.2813\n",
      "  0.2488\n",
      " -0.1550\n",
      "  0.2278\n",
      " -0.3935\n",
      "  0.3326\n",
      "  0.2117\n",
      " -0.1658\n",
      " -0.1120\n",
      " -0.0844\n",
      " -0.0447\n",
      "  0.2179\n",
      " -0.1099\n",
      "  0.1555\n",
      " -0.0714\n",
      " -0.1037\n",
      " -0.4125\n",
      " -0.0087\n",
      "  0.5200\n",
      " -0.3671\n",
      " -0.0741\n",
      "  0.4395\n",
      " -0.0847\n",
      " -0.4831\n",
      " -0.0241\n",
      " -0.6033\n",
      "  0.4542\n",
      "  0.3490\n",
      " -0.3365\n",
      " -0.5677\n",
      "  0.3807\n",
      " -0.0263\n",
      "  0.2874\n",
      "  0.0640\n",
      "  0.2208\n",
      " -0.5143\n",
      " -0.0727\n",
      " -0.0434\n",
      " -0.5795\n",
      "  0.0234\n",
      "  0.4603\n",
      " -0.1108\n",
      "  0.0853\n",
      "  0.1716\n",
      " -0.8156\n",
      "  0.1400\n",
      " -0.1327\n",
      "  0.4962\n",
      " -0.2982\n",
      "  0.0078\n",
      " -0.3260\n",
      "  0.2338\n",
      "  0.0106\n",
      " -0.5205\n",
      " -0.3629\n",
      " -0.3572\n",
      "  0.1446\n",
      " -0.0756\n",
      "  0.3866\n",
      "  0.4721\n",
      " -0.3860\n",
      " -0.0110\n",
      "  0.1946\n",
      " -0.3330\n",
      " -0.0301\n",
      "  0.0240\n",
      "  0.3078\n",
      "  0.3961\n",
      " -0.1483\n",
      " -0.1136\n",
      " -0.1557\n",
      "  0.2498\n",
      "  0.0039\n",
      "  0.6174\n",
      " -0.1720\n",
      " -0.2390\n",
      " -0.1554\n",
      " -0.0582\n",
      "  0.1957\n",
      " -0.1051\n",
      "  0.2523\n",
      " -0.1225\n",
      "  0.0124\n",
      " -0.4196\n",
      "  0.0236\n",
      "  0.2416\n",
      " -0.2261\n",
      "  0.2128\n",
      " -0.0317\n",
      " -0.2636\n",
      "  0.3843\n",
      "  0.2128\n",
      " -0.1659\n",
      " -0.0646\n",
      " -0.0100\n",
      " -0.0967\n",
      " -0.1251\n",
      " -0.3315\n",
      " -0.2061\n",
      " -0.6294\n",
      " -0.0084\n",
      "  0.1486\n",
      " -0.3941\n",
      "  0.3219\n",
      " -0.5064\n",
      " -0.2158\n",
      "  0.0116\n",
      " -0.4743\n",
      " -0.2889\n",
      " -0.1521\n",
      " -0.0203\n",
      "  0.6604\n",
      "  0.7803\n",
      " -0.1935\n",
      " -0.2454\n",
      "  0.0097\n",
      " -0.2935\n",
      "  0.4578\n",
      " -0.3595\n",
      "  0.2390\n",
      " -0.0287\n",
      " -0.0018\n",
      " -0.1344\n",
      " -0.0125\n",
      "  0.2369\n",
      "  0.0969\n",
      "  0.0947\n",
      "  0.3317\n",
      " -0.1670\n",
      " -0.1663\n",
      "  0.3036\n",
      " -0.3240\n",
      " -0.0496\n",
      "  0.4648\n",
      " -0.2504\n",
      "  0.2102\n",
      "  0.0821\n",
      " -0.1686\n",
      "  0.2502\n",
      " -0.2950\n",
      "  0.3044\n",
      " -0.1609\n",
      "  0.2473\n",
      "  0.2161\n",
      "  0.1317\n",
      "  0.2637\n",
      "  0.2354\n",
      " -0.0836\n",
      " -0.0122\n",
      "  0.4430\n",
      " -0.6266\n",
      " -0.3588\n",
      " -0.0105\n",
      "  0.3572\n",
      " -0.3410\n",
      " -0.4403\n",
      "  0.0477\n",
      "  0.1918\n",
      " -0.0628\n",
      " -0.2749\n",
      "  0.5179\n",
      "  0.2389\n",
      " -0.1691\n",
      "  0.2998\n",
      " -0.0776\n",
      " -0.0714\n",
      " -0.3470\n",
      " -0.1920\n",
      " -0.0043\n",
      " -0.0204\n",
      " -0.4568\n",
      "  0.2241\n",
      " -0.1757\n",
      "  0.1654\n",
      "  0.4057\n",
      "  0.0382\n",
      " -0.3039\n",
      "  0.2635\n",
      "  0.0742\n",
      "  0.5262\n",
      "  0.1935\n",
      " -0.2071\n",
      "  0.2531\n",
      "  0.1740\n",
      " -0.1319\n",
      "  0.3645\n",
      "  0.0533\n",
      " -0.3460\n",
      "  0.1961\n",
      "  0.2510\n",
      "  0.0092\n",
      " -0.1591\n",
      " -0.0320\n",
      " -0.1078\n",
      "  0.0895\n",
      "  0.2207\n",
      " -0.0400\n",
      "  0.7150\n",
      " -0.1600\n",
      "  0.0024\n",
      " -0.4994\n",
      "  0.0047\n",
      "  0.1653\n",
      "  0.0585\n",
      "  0.0305\n",
      "  0.4838\n",
      "  0.2641\n",
      " -0.3277\n",
      " -0.1260\n",
      "  0.4865\n",
      " -0.1629\n",
      " -0.3972\n",
      "  0.4768\n",
      "  0.1482\n",
      " -0.2170\n",
      "  0.0798\n",
      " -0.0021\n",
      "  0.3557\n",
      " -0.3649\n",
      " -0.0004\n",
      "  0.0431\n",
      " -0.0873\n",
      " -0.4167\n",
      " -0.6725\n",
      " -0.2464\n",
      "  0.4343\n",
      "  0.0924\n",
      " -0.0129\n",
      "  0.3987\n",
      " -0.6326\n",
      "  0.2214\n",
      " -0.1828\n",
      " -0.1255\n",
      " -0.2214\n",
      " -0.2937\n",
      "  0.0751\n",
      "  0.2821\n",
      " -0.1472\n",
      "  0.2459\n",
      " -0.3444\n",
      "  0.1196\n",
      "  0.0212\n",
      " -0.4072\n",
      "  0.1067\n",
      "  0.1366\n",
      "  0.0275\n",
      "  0.1351\n",
      " -0.0747\n",
      " -0.3803\n",
      "  0.1664\n",
      "  0.0888\n",
      " -0.0680\n",
      " -0.1241\n",
      "  0.9847\n",
      " -0.6386\n",
      "  0.2568\n",
      "  0.1655\n",
      "  0.4402\n",
      " -0.9433\n",
      " -0.0597\n",
      "  0.0542\n",
      " -0.1713\n",
      " -0.1642\n",
      "  0.4499\n",
      "  0.0551\n",
      " -0.0670\n",
      "  0.0402\n",
      " -0.2755\n",
      " -0.0608\n",
      " -0.2846\n",
      " -0.1156\n",
      " -0.0065\n",
      "  0.3806\n",
      "  0.0780\n",
      " -0.3489\n",
      "  0.0058\n",
      " -0.1753\n",
      " -0.2956\n",
      "  0.0727\n",
      " -0.2222\n",
      " -0.3060\n",
      "  0.0783\n",
      " -0.1527\n",
      " -0.2327\n",
      "  0.1711\n",
      "  0.2087\n",
      " -0.2376\n",
      "  0.2666\n",
      "  0.3543\n",
      "  0.0094\n",
      " -0.4867\n",
      " -0.3280\n",
      " -0.3147\n",
      "  0.0062\n",
      " -0.1671\n",
      " -0.1240\n",
      " -0.3966\n",
      " -0.4362\n",
      "  0.2937\n",
      "  0.0563\n",
      "  0.1600\n",
      "  0.1234\n",
      "  0.1710\n",
      " -0.3017\n",
      "  0.1805\n",
      " -0.2873\n",
      " -0.0021\n",
      "  0.3284\n",
      " -0.2615\n",
      "  0.2153\n",
      "  0.0632\n",
      " -0.1954\n",
      " -0.0829\n",
      "  0.0344\n",
      " -0.4417\n",
      "  0.2167\n",
      "  0.1686\n",
      "  0.0039\n",
      " -0.0725\n",
      "  0.4125\n",
      " -0.0863\n",
      " -0.1117\n",
      " -0.6626\n",
      "  0.2083\n",
      " -0.0457\n",
      "  0.2677\n",
      " -0.1907\n",
      " -0.0656\n",
      "  0.6122\n",
      " -0.0778\n",
      " -0.2316\n",
      "  0.2392\n",
      " -0.0944\n",
      "  0.0442\n",
      "  0.0675\n",
      " -0.3569\n",
      " -0.7481\n",
      " -0.0759\n",
      " -0.1945\n",
      " -0.1314\n",
      "  0.0840\n",
      " -0.3187\n",
      " -0.3307\n",
      " -0.2940\n",
      " -0.2552\n",
      " -0.2928\n",
      " -0.3008\n",
      "  0.1272\n",
      " -0.5181\n",
      " -0.3433\n",
      "  0.2211\n",
      " -0.0647\n",
      "  0.2669\n",
      " -0.0522\n",
      " -0.0631\n",
      " -0.4066\n",
      " -0.0036\n",
      " -0.2741\n",
      " -0.4825\n",
      "  0.4247\n",
      "  0.2256\n",
      " -0.0641\n",
      " -0.0157\n",
      " -0.2438\n",
      " -0.1611\n",
      " -0.4160\n",
      " -0.1065\n",
      "  0.0231\n",
      " -0.2882\n",
      "  0.2048\n",
      "  0.2614\n",
      " -0.2769\n",
      "  0.2046\n",
      " -0.0201\n",
      "  0.1527\n",
      "  0.5095\n",
      " -0.3074\n",
      " -0.5394\n",
      " -0.4066\n",
      "  0.5066\n",
      "  0.0164\n",
      " -0.1438\n",
      "  0.0542\n",
      "  0.3258\n",
      " -0.2976\n",
      " -0.1499\n",
      " -0.0901\n",
      " -0.5755\n",
      "  0.2429\n",
      " -0.3440\n",
      "  0.4643\n",
      "  0.1923\n",
      "  0.0774\n",
      "  0.4870\n",
      "  0.1632\n",
      "  0.5078\n",
      "  0.0135\n",
      " -0.3251\n",
      " -0.0982\n",
      " -0.0887\n",
      " -0.2820\n",
      " -0.3471\n",
      "  0.2826\n",
      "  0.0527\n",
      "  0.0714\n",
      " -0.2226\n",
      "  0.0615\n",
      " -0.2064\n",
      "  0.1246\n",
      "  0.2207\n",
      "  0.2063\n",
      "  0.2182\n",
      "  0.1755\n",
      " -0.0924\n",
      "  0.1001\n",
      " -0.1668\n",
      " -0.2759\n",
      " -0.1770\n",
      " -0.2411\n",
      "  0.7005\n",
      "  0.0423\n",
      "  0.2196\n",
      " -0.4275\n",
      " -0.2967\n",
      "  0.4580\n",
      " -0.1105\n",
      " -0.2517\n",
      " -0.2654\n",
      " -0.2503\n",
      " -0.0648\n",
      " -0.0812\n",
      "  0.3858\n",
      "  0.5528\n",
      " -0.3487\n",
      " -0.2151\n",
      " -0.0958\n",
      " -0.2162\n",
      "  0.0713\n",
      " -0.0199\n",
      " -0.0955\n",
      " -1.0745\n",
      " -0.3215\n",
      "  0.0037\n",
      "  0.0480\n",
      " -0.5188\n",
      "  0.2431\n",
      " -0.1769\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2935\n",
      " 0.2028\n",
      " 0.2278\n",
      " 0.2812\n",
      " 0.2415\n",
      " 0.2887\n",
      " 0.2899\n",
      " 0.2003\n",
      " 0.2941\n",
      " 0.2560\n",
      " 0.3168\n",
      " 0.2435\n",
      " 0.3322\n",
      " 0.3298\n",
      " 0.2133\n",
      " 0.2591\n",
      " 0.2522\n",
      " 0.1981\n",
      " 0.2415\n",
      " 0.2533\n",
      " 0.2305\n",
      " 0.2925\n",
      " 0.2994\n",
      " 0.2615\n",
      " 0.2720\n",
      " 0.2227\n",
      " 0.2926\n",
      " 0.2413\n",
      " 0.1946\n",
      " 0.2358\n",
      " 0.2498\n",
      " 0.2444\n",
      " 0.2506\n",
      " 0.2319\n",
      " 0.1841\n",
      " 0.3238\n",
      " 0.2379\n",
      " 0.2455\n",
      " 0.2290\n",
      " 0.3200\n",
      " 0.2885\n",
      " 0.2446\n",
      " 0.3003\n",
      " 0.3444\n",
      " 0.2188\n",
      " 0.2233\n",
      " 0.2232\n",
      " 0.2345\n",
      " 0.2844\n",
      " 0.2483\n",
      " 0.3061\n",
      " 0.2620\n",
      " 0.2760\n",
      " 0.1997\n",
      " 0.2439\n",
      " 0.2314\n",
      " 0.2240\n",
      " 0.2164\n",
      " 0.2549\n",
      " 0.2742\n",
      " 0.3209\n",
      " 0.2169\n",
      " 0.2534\n",
      " 0.2281\n",
      " 0.1678\n",
      " 0.2688\n",
      " 0.2606\n",
      " 0.2355\n",
      " 0.3214\n",
      " 0.1893\n",
      " 0.2665\n",
      " 0.2737\n",
      " 0.2830\n",
      " 0.2774\n",
      " 0.3148\n",
      " 0.3267\n",
      " 0.1472\n",
      " 0.2263\n",
      " 0.2715\n",
      " 0.2272\n",
      " 0.2736\n",
      " 0.2581\n",
      " 0.1928\n",
      " 0.2443\n",
      " 0.2618\n",
      " 0.2663\n",
      " 0.2470\n",
      " 0.2491\n",
      " 0.2880\n",
      " 0.2165\n",
      " 0.2620\n",
      " 0.2152\n",
      " 0.2795\n",
      " 0.2302\n",
      " 0.2643\n",
      " 0.2239\n",
      " 0.2631\n",
      " 0.2446\n",
      " 0.2493\n",
      " 0.2614\n",
      " 0.2863\n",
      " 0.2512\n",
      " 0.2699\n",
      " 0.2520\n",
      " 0.2049\n",
      " 0.0903\n",
      " 0.2853\n",
      " 0.2506\n",
      " 0.3423\n",
      " 0.3115\n",
      " 0.2416\n",
      " 0.2206\n",
      " 0.1536\n",
      " 0.2597\n",
      " 0.2027\n",
      " 0.3966\n",
      " 0.2579\n",
      " 0.2648\n",
      " 0.2934\n",
      " 0.2667\n",
      " 0.2490\n",
      " 0.2327\n",
      " 0.2851\n",
      " 0.3215\n",
      " 0.2747\n",
      " 0.2811\n",
      " 0.1928\n",
      " 0.2831\n",
      " 0.2555\n",
      " 0.2252\n",
      " 0.2712\n",
      " 0.2570\n",
      " 0.2377\n",
      " 0.2286\n",
      " 0.2900\n",
      " 0.3105\n",
      " 0.2531\n",
      " 0.2710\n",
      " 0.2691\n",
      " 0.1972\n",
      " 0.2907\n",
      " 0.3450\n",
      " 0.2628\n",
      " 0.2636\n",
      " 0.2562\n",
      " 0.2652\n",
      " 0.1720\n",
      " 0.3016\n",
      " 0.2648\n",
      " 0.2919\n",
      " 0.1815\n",
      " 0.2222\n",
      " 0.3121\n",
      " 0.2182\n",
      " 0.2977\n",
      " 0.3117\n",
      " 0.2161\n",
      " 0.1924\n",
      " 0.2618\n",
      " 0.2397\n",
      " 0.2427\n",
      " 0.1777\n",
      " 0.2439\n",
      " 0.2597\n",
      " 0.2890\n",
      " 0.2495\n",
      " 0.3104\n",
      " 0.2422\n",
      " 0.2271\n",
      " 0.2127\n",
      " 0.3259\n",
      " 0.2356\n",
      " 0.2528\n",
      " 0.2607\n",
      " 0.2369\n",
      " 0.2602\n",
      " 0.2264\n",
      " 0.2924\n",
      " 0.3664\n",
      " 0.2400\n",
      " 0.2120\n",
      " 0.2252\n",
      " 0.2436\n",
      " 0.2337\n",
      " 0.2992\n",
      " 0.1672\n",
      " 0.2346\n",
      " 0.3146\n",
      " 0.2717\n",
      " 0.2809\n",
      " 0.3133\n",
      " 0.2728\n",
      " 0.2481\n",
      " 0.3072\n",
      " 0.2681\n",
      " 0.2257\n",
      " 0.2484\n",
      " 0.2445\n",
      " 0.3284\n",
      " 0.2402\n",
      " 0.2508\n",
      " 0.2052\n",
      " 0.1999\n",
      " 0.2470\n",
      " 0.2638\n",
      " 0.3551\n",
      " 0.2171\n",
      " 0.2229\n",
      " 0.2170\n",
      " 0.2478\n",
      " 0.2351\n",
      " 0.2150\n",
      " 0.2265\n",
      " 0.2277\n",
      " 0.2698\n",
      " 0.3139\n",
      " 0.2630\n",
      " 0.3044\n",
      " 0.2664\n",
      " 0.2453\n",
      " 0.2101\n",
      " 0.2202\n",
      " 0.2405\n",
      " 0.2817\n",
      " 0.3539\n",
      " 0.2860\n",
      " 0.2632\n",
      " 0.3015\n",
      " 0.2557\n",
      " 0.2898\n",
      " 0.2751\n",
      " 0.2805\n",
      " 0.2559\n",
      " 0.2454\n",
      " 0.2576\n",
      " 0.2211\n",
      " 0.2135\n",
      " 0.2770\n",
      " 0.3123\n",
      " 0.2355\n",
      " 0.2800\n",
      " 0.2254\n",
      " 0.2132\n",
      " 0.2342\n",
      " 0.2917\n",
      " 0.2121\n",
      " 0.2888\n",
      " 0.2575\n",
      " 0.3028\n",
      " 0.2040\n",
      " 0.3062\n",
      " 0.2721\n",
      " 0.2471\n",
      " 0.2505\n",
      " 0.2358\n",
      " 0.2873\n",
      " 0.2565\n",
      " 0.2689\n",
      " 0.2950\n",
      " 0.2596\n",
      " 0.2557\n",
      " 0.3261\n",
      " 0.3167\n",
      " 0.2360\n",
      " 0.2809\n",
      " 0.2609\n",
      " 0.2198\n",
      " 0.2816\n",
      " 0.3736\n",
      " 0.2266\n",
      " 0.2418\n",
      " 0.2452\n",
      " 0.2777\n",
      " 0.2472\n",
      " 0.2720\n",
      " 0.2255\n",
      " 0.2842\n",
      " 0.3202\n",
      " 0.2702\n",
      " 0.3200\n",
      " 0.2659\n",
      " 0.2319\n",
      " 0.2494\n",
      " 0.1990\n",
      " 0.2009\n",
      " 0.3130\n",
      " 0.3181\n",
      " 0.2848\n",
      " 0.3188\n",
      " 0.2962\n",
      " 0.3027\n",
      " 0.2627\n",
      " 0.2732\n",
      " 0.2390\n",
      " 0.2189\n",
      " 0.2569\n",
      " 0.2562\n",
      " 0.2380\n",
      " 0.2314\n",
      " 0.3397\n",
      " 0.2579\n",
      " 0.2258\n",
      " 0.2404\n",
      " 0.2165\n",
      " 0.2207\n",
      " 0.2305\n",
      " 0.2318\n",
      " 0.2000\n",
      " 0.3088\n",
      " 0.2576\n",
      " 0.2392\n",
      " 0.2475\n",
      " 0.2062\n",
      " 0.2195\n",
      " 0.3039\n",
      " 0.2327\n",
      " 0.2496\n",
      " 0.2482\n",
      " 0.3296\n",
      " 0.2858\n",
      " 0.2631\n",
      " 0.2466\n",
      " 0.2709\n",
      " 0.2267\n",
      " 0.2167\n",
      " 0.2375\n",
      " 0.2372\n",
      " 0.2728\n",
      " 0.2179\n",
      " 0.2471\n",
      " 0.2395\n",
      " 0.3063\n",
      " 0.3196\n",
      " 0.2262\n",
      " 0.2573\n",
      " 0.3017\n",
      " 0.2310\n",
      " 0.2291\n",
      " 0.2914\n",
      " 0.2536\n",
      " 0.2069\n",
      " 0.2086\n",
      " 0.1923\n",
      " 0.3070\n",
      " 0.2781\n",
      " 0.2851\n",
      " 0.2382\n",
      " 0.2310\n",
      " 0.2902\n",
      " 0.2398\n",
      " 0.2222\n",
      " 0.2724\n",
      " 0.2774\n",
      " 0.3075\n",
      " 0.2434\n",
      " 0.2436\n",
      " 0.2291\n",
      " 0.3327\n",
      " 0.3304\n",
      " 0.2718\n",
      " 0.2324\n",
      " 0.2552\n",
      " 0.2931\n",
      " 0.3827\n",
      " 0.2520\n",
      " 0.2683\n",
      " 0.2471\n",
      " 0.3061\n",
      " 0.2844\n",
      " 0.2820\n",
      " 0.2590\n",
      " 0.2220\n",
      " 0.2309\n",
      " 0.2913\n",
      " 0.2480\n",
      " 0.2789\n",
      " 0.2362\n",
      " 0.2275\n",
      " 0.2644\n",
      " 0.3197\n",
      " 0.2803\n",
      " 0.2242\n",
      " 0.2590\n",
      " 0.2697\n",
      " 0.3678\n",
      " 0.2881\n",
      " 0.2664\n",
      " 0.2272\n",
      " 0.3057\n",
      " 0.2623\n",
      " 0.2645\n",
      " 0.2752\n",
      " 0.2480\n",
      " 0.2557\n",
      " 0.2167\n",
      " 0.3402\n",
      " 0.2250\n",
      " 0.2703\n",
      " 0.2323\n",
      " 0.2457\n",
      " 0.3254\n",
      " 0.3498\n",
      " 0.2760\n",
      " 0.2699\n",
      " 0.2132\n",
      " 0.2419\n",
      " 0.2441\n",
      " 0.2184\n",
      " 0.2320\n",
      " 0.2396\n",
      " 0.3098\n",
      " 0.2097\n",
      " 0.2658\n",
      " 0.2290\n",
      " 0.2425\n",
      " 0.2152\n",
      " 0.2781\n",
      " 0.2774\n",
      " 0.3339\n",
      " 0.3267\n",
      " 0.2213\n",
      " 0.2100\n",
      " 0.3125\n",
      " 0.2686\n",
      " 0.3158\n",
      " 0.2495\n",
      " 0.2720\n",
      " 0.2354\n",
      " 0.2561\n",
      " 0.2915\n",
      " 0.2712\n",
      " 0.1850\n",
      " 0.2498\n",
      " 0.2521\n",
      " 0.2725\n",
      " 0.2633\n",
      " 0.3210\n",
      " 0.2331\n",
      " 0.2613\n",
      " 0.2893\n",
      " 0.2683\n",
      " 0.2636\n",
      " 0.2686\n",
      " 0.3836\n",
      " 0.3405\n",
      " 0.2510\n",
      " 0.2826\n",
      " 0.2502\n",
      " 0.2513\n",
      " 0.2609\n",
      " 0.2655\n",
      " 0.2090\n",
      " 0.3560\n",
      " 0.3054\n",
      " 0.2241\n",
      " 0.2874\n",
      " 0.2739\n",
      " 0.2514\n",
      " 0.2393\n",
      " 0.3530\n",
      " 0.3612\n",
      " 0.2407\n",
      " 0.2607\n",
      " 0.2463\n",
      " 0.2760\n",
      " 0.2119\n",
      " 0.2630\n",
      " 0.2583\n",
      " 0.2783\n",
      " 0.2081\n",
      " 0.2928\n",
      " 0.2731\n",
      " 0.2535\n",
      " 0.2704\n",
      " 0.2604\n",
      " 0.3331\n",
      " 0.2266\n",
      " 0.2618\n",
      " 0.2730\n",
      " 0.3891\n",
      " 0.2959\n",
      " 0.2005\n",
      " 0.2661\n",
      " 0.3045\n",
      " 0.3104\n",
      " 0.1992\n",
      " 0.2291\n",
      " 0.2715\n",
      " 0.2274\n",
      " 0.2272\n",
      " 0.2403\n",
      " 0.2406\n",
      " 0.2180\n",
      " 0.2234\n",
      " 0.2591\n",
      " 0.2275\n",
      " 0.1971\n",
      " 0.2432\n",
      " 0.3067\n",
      " 0.2163\n",
      " 0.2640\n",
      " 0.3167\n",
      " 0.2437\n",
      " 0.3146\n",
      " 0.2267\n",
      " 0.3213\n",
      " 0.2869\n",
      " 0.2717\n",
      " 0.2564\n",
      " 0.2375\n",
      " 0.2420\n",
      " 0.2523\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2222\n",
      "-0.0907\n",
      "-0.0867\n",
      "-0.1520\n",
      "-0.1912\n",
      "-0.4742\n",
      "-0.2794\n",
      "-0.0711\n",
      "-0.4119\n",
      "-0.1597\n",
      "-0.1058\n",
      "-0.1341\n",
      "-0.2795\n",
      "-0.3213\n",
      "-0.1070\n",
      "-0.1883\n",
      "-0.1450\n",
      "-0.0434\n",
      "-0.1450\n",
      "-0.2270\n",
      "-0.1207\n",
      "-0.1805\n",
      "-0.1407\n",
      "-0.1891\n",
      "-0.2122\n",
      "-0.1039\n",
      "-0.2294\n",
      "-0.1309\n",
      "-0.0941\n",
      "-0.1663\n",
      "-0.1058\n",
      "-0.1812\n",
      "-0.1719\n",
      "-0.1372\n",
      "-0.0105\n",
      "-0.3542\n",
      "-0.1404\n",
      "-0.1563\n",
      "-0.1083\n",
      "-0.3321\n",
      "-0.2834\n",
      "-0.1452\n",
      "-0.1499\n",
      "-0.3349\n",
      " 0.0480\n",
      "-0.0753\n",
      "-0.1404\n",
      "-0.1831\n",
      "-0.2717\n",
      "-0.2108\n",
      "-0.2197\n",
      "-0.1559\n",
      "-0.1820\n",
      "-0.0825\n",
      "-0.1950\n",
      "-0.1168\n",
      "-0.0951\n",
      "-0.1476\n",
      "-0.0738\n",
      "-0.2202\n",
      "-0.2676\n",
      "-0.1693\n",
      "-0.2344\n",
      "-0.0647\n",
      "-0.0119\n",
      "-0.2641\n",
      "-0.1889\n",
      "-0.1909\n",
      "-0.1442\n",
      "-0.0783\n",
      "-0.2219\n",
      "-0.2239\n",
      "-0.2119\n",
      "-0.1620\n",
      "-0.1537\n",
      "-0.2780\n",
      "-0.0029\n",
      "-0.1054\n",
      "-0.2102\n",
      "-0.1272\n",
      "-0.1452\n",
      "-0.1630\n",
      "-0.0550\n",
      "-0.2114\n",
      "-0.1870\n",
      "-0.1935\n",
      "-0.1589\n",
      "-0.1173\n",
      "-0.2039\n",
      "-0.1113\n",
      "-0.1188\n",
      "-0.1082\n",
      "-0.1145\n",
      "-0.1186\n",
      "-0.1231\n",
      "-0.1383\n",
      "-0.1117\n",
      "-0.1278\n",
      "-0.1596\n",
      "-0.1525\n",
      "-0.1331\n",
      "-0.1699\n",
      "-0.1557\n",
      "-0.1595\n",
      "-0.0558\n",
      "-0.2779\n",
      "-0.2439\n",
      "-0.2325\n",
      "-0.3178\n",
      "-0.2792\n",
      "-0.1847\n",
      "-0.1068\n",
      " 0.1015\n",
      "-0.2397\n",
      "-0.0978\n",
      "-0.4052\n",
      "-0.1727\n",
      "-0.0788\n",
      "-0.3033\n",
      "-0.3243\n",
      "-0.1806\n",
      "-0.1204\n",
      "-0.2480\n",
      "-0.1766\n",
      "-0.2235\n",
      "-0.1813\n",
      "-0.2788\n",
      "-0.1802\n",
      "-0.1104\n",
      "-0.1946\n",
      "-0.2053\n",
      "-0.1481\n",
      "-0.1137\n",
      "-0.1215\n",
      "-0.2966\n",
      "-0.3616\n",
      "-0.1033\n",
      "-0.2082\n",
      "-0.1733\n",
      "-0.0791\n",
      "-0.2518\n",
      "-0.4476\n",
      "-0.1152\n",
      "-0.2981\n",
      "-0.1919\n",
      "-0.2371\n",
      "-0.0288\n",
      "-0.2072\n",
      "-0.1587\n",
      "-0.3970\n",
      " 0.0007\n",
      "-0.1239\n",
      "-0.3061\n",
      "-0.0694\n",
      "-0.2350\n",
      "-0.2458\n",
      "-0.1190\n",
      "-0.0644\n",
      "-0.1148\n",
      "-0.1850\n",
      "-0.1058\n",
      "-0.0617\n",
      "-0.1326\n",
      "-0.2015\n",
      "-0.1987\n",
      "-0.1585\n",
      "-0.2366\n",
      "-0.1471\n",
      "-0.1356\n",
      "-0.0594\n",
      "-0.1708\n",
      "-0.1254\n",
      "-0.1518\n",
      "-0.0967\n",
      "-0.1093\n",
      "-0.1462\n",
      "-0.3272\n",
      "-0.2392\n",
      "-0.3752\n",
      "-0.1264\n",
      "-0.1257\n",
      "-0.1313\n",
      "-0.2177\n",
      "-0.1355\n",
      "-0.1665\n",
      " 0.0324\n",
      "-0.1354\n",
      "-0.3070\n",
      "-0.3041\n",
      "-0.1315\n",
      "-0.2330\n",
      "-0.2238\n",
      "-0.1775\n",
      "-0.2857\n",
      "-0.2498\n",
      "-0.1427\n",
      "-0.1678\n",
      "-0.1449\n",
      "-0.2835\n",
      "-0.1531\n",
      "-0.1911\n",
      "-0.0679\n",
      "-0.0745\n",
      "-0.1403\n",
      "-0.2646\n",
      "-0.3393\n",
      "-0.0831\n",
      "-0.1087\n",
      "-0.1092\n",
      "-0.1657\n",
      "-0.1520\n",
      "-0.0487\n",
      "-0.1010\n",
      "-0.1078\n",
      "-0.1757\n",
      "-0.1687\n",
      "-0.1086\n",
      "-0.2144\n",
      "-0.1450\n",
      "-0.1492\n",
      "-0.1397\n",
      "-0.0882\n",
      "-0.1491\n",
      "-0.2511\n",
      "-0.3784\n",
      "-0.1495\n",
      "-0.1666\n",
      "-0.1743\n",
      "-0.2504\n",
      "-0.2115\n",
      "-0.1557\n",
      "-0.2095\n",
      "-0.2006\n",
      "-0.2132\n",
      "-0.1628\n",
      "-0.1113\n",
      "-0.1227\n",
      "-0.2052\n",
      "-0.2225\n",
      "-0.1709\n",
      "-0.1522\n",
      "-0.1353\n",
      "-0.0995\n",
      "-0.1458\n",
      "-0.2153\n",
      "-0.0424\n",
      "-0.1810\n",
      "-0.2114\n",
      "-0.2705\n",
      "-0.0906\n",
      "-0.1709\n",
      "-0.1218\n",
      "-0.1192\n",
      "-0.2006\n",
      "-0.2307\n",
      "-0.2560\n",
      "-0.2537\n",
      "-0.2050\n",
      "-0.2974\n",
      "-0.1609\n",
      "-0.1696\n",
      "-0.2607\n",
      "-0.1874\n",
      "-0.1312\n",
      "-0.2632\n",
      "-0.0859\n",
      "-0.1036\n",
      "-0.1730\n",
      "-0.3297\n",
      "-0.1816\n",
      "-0.1621\n",
      "-0.1195\n",
      "-0.1847\n",
      "-0.1456\n",
      "-0.1693\n",
      "-0.0859\n",
      "-0.2254\n",
      "-0.2499\n",
      "-0.1708\n",
      "-0.2703\n",
      "-0.2019\n",
      "-0.0514\n",
      "-0.1916\n",
      " 0.0086\n",
      "-0.0731\n",
      "-0.3054\n",
      "-0.2823\n",
      "-0.1915\n",
      "-0.2419\n",
      "-0.1427\n",
      "-0.2656\n",
      "-0.2537\n",
      "-0.2459\n",
      "-0.1917\n",
      "-0.0721\n",
      "-0.1862\n",
      "-0.1312\n",
      "-0.1550\n",
      "-0.1435\n",
      "-0.2517\n",
      "-0.1830\n",
      "-0.1123\n",
      "-0.0999\n",
      "-0.0714\n",
      "-0.0886\n",
      "-0.1362\n",
      "-0.1187\n",
      "-0.0688\n",
      "-0.1988\n",
      "-0.1354\n",
      "-0.1459\n",
      "-0.1520\n",
      "-0.0792\n",
      "-0.0923\n",
      "-0.2204\n",
      "-0.1773\n",
      "-0.2462\n",
      "-0.2140\n",
      "-0.2875\n",
      "-0.2091\n",
      "-0.1479\n",
      "-0.1683\n",
      "-0.1853\n",
      "-0.1397\n",
      "-0.1136\n",
      "-0.1450\n",
      "-0.1753\n",
      "-0.2714\n",
      "-0.1929\n",
      "-0.1719\n",
      "-0.1028\n",
      "-0.2321\n",
      "-0.2031\n",
      "-0.0900\n",
      "-0.1671\n",
      "-0.1961\n",
      "-0.1188\n",
      "-0.1261\n",
      "-0.2136\n",
      "-0.1447\n",
      "-0.1361\n",
      "-0.0600\n",
      "-0.0518\n",
      "-0.3202\n",
      "-0.3191\n",
      "-0.2370\n",
      "-0.1120\n",
      "-0.1755\n",
      "-0.2388\n",
      "-0.1795\n",
      "-0.0645\n",
      "-0.2413\n",
      "-0.1923\n",
      "-0.2135\n",
      "-0.1583\n",
      "-0.1780\n",
      "-0.2351\n",
      "-0.2409\n",
      "-0.2615\n",
      "-0.2056\n",
      "-0.1535\n",
      "-0.1166\n",
      "-0.2760\n",
      "-0.5200\n",
      "-0.1218\n",
      "-0.2584\n",
      "-0.2758\n",
      "-0.2903\n",
      "-0.2078\n",
      "-0.0732\n",
      "-0.1948\n",
      "-0.1764\n",
      "-0.0539\n",
      "-0.1414\n",
      "-0.1159\n",
      "-0.2606\n",
      "-0.1760\n",
      "-0.1438\n",
      "-0.1731\n",
      "-0.2489\n",
      "-0.2322\n",
      "-0.1152\n",
      "-0.2207\n",
      "-0.1305\n",
      "-0.2895\n",
      "-0.3661\n",
      "-0.1468\n",
      "-0.1079\n",
      "-0.2102\n",
      "-0.1635\n",
      "-0.1956\n",
      "-0.1542\n",
      "-0.1668\n",
      "-0.0906\n",
      "-0.1655\n",
      "-0.2935\n",
      "-0.1327\n",
      "-0.1908\n",
      "-0.1586\n",
      "-0.1856\n",
      "-0.3094\n",
      "-0.3646\n",
      "-0.1644\n",
      "-0.1857\n",
      "-0.1343\n",
      "-0.1151\n",
      "-0.1090\n",
      "-0.1239\n",
      "-0.1479\n",
      "-0.1110\n",
      "-0.2937\n",
      "-0.0993\n",
      "-0.2014\n",
      "-0.1273\n",
      "-0.1639\n",
      "-0.0703\n",
      "-0.2701\n",
      "-0.3268\n",
      "-0.2916\n",
      "-0.3074\n",
      "-0.1539\n",
      "-0.0883\n",
      "-0.2706\n",
      "-0.1826\n",
      "-0.3841\n",
      "-0.1346\n",
      "-0.1199\n",
      "-0.1860\n",
      "-0.1322\n",
      "-0.2534\n",
      "-0.1707\n",
      "-0.0332\n",
      "-0.1544\n",
      "-0.1506\n",
      "-0.1636\n",
      "-0.0944\n",
      "-0.2562\n",
      "-0.0516\n",
      "-0.1335\n",
      "-0.1486\n",
      "-0.1747\n",
      "-0.1853\n",
      "-0.1405\n",
      "-0.4886\n",
      "-0.1884\n",
      "-0.1539\n",
      "-0.2644\n",
      "-0.1523\n",
      "-0.1538\n",
      "-0.1868\n",
      "-0.1000\n",
      "-0.0556\n",
      "-0.3637\n",
      "-0.3999\n",
      "-0.1300\n",
      "-0.2291\n",
      "-0.1625\n",
      "-0.2210\n",
      "-0.1161\n",
      "-0.4373\n",
      "-0.2935\n",
      "-0.1444\n",
      "-0.1594\n",
      "-0.2060\n",
      "-0.1935\n",
      "-0.1217\n",
      "-0.1915\n",
      "-0.1864\n",
      "-0.4108\n",
      "-0.0595\n",
      "-0.1845\n",
      "-0.2338\n",
      "-0.1271\n",
      "-0.1672\n",
      "-0.1981\n",
      "-0.3165\n",
      "-0.1274\n",
      "-0.1283\n",
      "-0.2459\n",
      "-0.4393\n",
      "-0.2624\n",
      "-0.0380\n",
      "-0.1818\n",
      "-0.3468\n",
      "-0.1634\n",
      "-0.1018\n",
      "-0.1967\n",
      "-0.1228\n",
      "-0.1439\n",
      "-0.1319\n",
      "-0.0933\n",
      "-0.1320\n",
      "-0.1054\n",
      "-0.1759\n",
      "-0.1494\n",
      "-0.2049\n",
      "-0.0873\n",
      "-0.2006\n",
      "-0.1437\n",
      "-0.1344\n",
      "-0.2087\n",
      "-0.2625\n",
      "-0.2086\n",
      "-0.2655\n",
      "-0.0626\n",
      "-0.3659\n",
      "-0.2525\n",
      "-0.2148\n",
      "-0.1698\n",
      "-0.1706\n",
      "-0.1361\n",
      "-0.0639\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  2.0101e-02  3.5779e-02 -1.0511e-02\n",
      "  1.7103e-02  5.4512e-02  4.5345e-04\n",
      " -7.2330e-03  2.7815e-02 -1.7693e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  7.5410e-03  8.2219e-03  1.1119e-03\n",
      "  4.5033e-03  9.4504e-03  1.2389e-02\n",
      "  9.6322e-03  6.0494e-03  4.1823e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  1.2604e-02  6.4746e-03  1.1472e-02\n",
      "  7.3976e-03 -5.5376e-03  5.7332e-03\n",
      "  5.6122e-03 -4.4162e-03  3.4283e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,509,.,.) = \n",
      "  1.6630e-02  3.2122e-02  1.9508e-02\n",
      "  9.7642e-03  1.7368e-02  1.3763e-02\n",
      "  1.2484e-03  3.9263e-03 -1.1305e-02\n",
      "\n",
      "( 0 ,510,.,.) = \n",
      "  1.8581e-02  1.4877e-02  2.0691e-02\n",
      "  2.2556e-02  3.0739e-02  3.1269e-02\n",
      "  4.2550e-03  1.0475e-02  1.2410e-02\n",
      "\n",
      "( 0 ,511,.,.) = \n",
      " -2.9350e-02 -1.5643e-02 -2.2234e-03\n",
      " -1.2511e-02 -6.5020e-03 -7.6790e-03\n",
      " -2.5598e-02 -8.0623e-03 -7.3783e-03\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -1.7796e-02 -2.9172e-02 -6.7727e-03\n",
      " -1.8538e-02 -1.7893e-02 -7.5968e-03\n",
      " -1.9090e-02 -2.2834e-02 -1.1370e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -2.1613e-03  3.4971e-03  6.8169e-03\n",
      "  3.4600e-03  1.0893e-02  9.8161e-03\n",
      "  8.1440e-03  6.2050e-03 -4.9259e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -2.3813e-03 -2.9798e-03  8.3061e-03\n",
      " -5.0201e-03  2.6249e-03  6.4156e-03\n",
      "  1.1500e-02 -2.3969e-03 -7.4685e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,509,.,.) = \n",
      "  1.4182e-02  2.0880e-03  1.5499e-02\n",
      "  7.6753e-03 -3.4637e-03 -9.3563e-03\n",
      "  8.7186e-04  1.4127e-02 -1.3704e-02\n",
      "\n",
      "( 1 ,510,.,.) = \n",
      " -7.8051e-03 -9.0256e-03  1.1620e-02\n",
      "  4.2150e-03 -1.0665e-02  2.7806e-03\n",
      "  1.3070e-02 -3.9675e-03  1.6594e-03\n",
      "\n",
      "( 1 ,511,.,.) = \n",
      " -7.4461e-03  7.6692e-03 -9.6226e-05\n",
      " -1.1271e-02 -9.3795e-04  1.3828e-02\n",
      " -2.3338e-02 -3.7073e-03  2.0755e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -4.5768e-03 -4.5367e-03 -6.0424e-03\n",
      " -1.5872e-02 -7.5559e-03 -1.0763e-02\n",
      " -1.2209e-02 -1.6867e-03 -2.2571e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  7.4801e-03  1.3080e-02  3.4820e-04\n",
      " -5.4277e-03 -1.2411e-03 -1.2049e-02\n",
      " -1.7910e-02 -2.8446e-02 -3.3902e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -2.7154e-02  8.0334e-03  2.0728e-02\n",
      " -5.8774e-03  1.9720e-02  2.0990e-02\n",
      " -2.3417e-02  1.0154e-02  7.3425e-03\n",
      "    ... \n",
      "\n",
      "( 2 ,509,.,.) = \n",
      " -2.2703e-02 -5.1804e-03  4.4770e-03\n",
      " -1.4206e-02 -6.1257e-03  1.2107e-02\n",
      " -9.7779e-03 -1.9056e-02 -7.2144e-03\n",
      "\n",
      "( 2 ,510,.,.) = \n",
      "  1.0864e-02  6.8579e-03  1.0318e-02\n",
      " -1.6214e-03 -9.8189e-03 -2.1734e-03\n",
      "  2.9362e-03 -5.4421e-03 -1.5190e-02\n",
      "\n",
      "( 2 ,511,.,.) = \n",
      "  1.9294e-02  3.7816e-03 -2.7482e-03\n",
      "  1.1900e-03 -8.4978e-04 -1.1181e-02\n",
      " -1.4015e-02 -1.4229e-02 -1.3642e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      " -2.1579e-03 -1.1423e-02  4.5947e-03\n",
      "  9.3773e-03  1.1606e-02  1.6373e-02\n",
      "  2.7028e-04  6.1070e-03  9.4222e-03\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      " -1.0909e-03 -6.5402e-03  1.9698e-03\n",
      " -4.3472e-03 -4.2700e-03  8.6332e-04\n",
      "  6.8045e-03  1.0836e-02  1.1216e-02\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      " -2.6086e-03 -1.0056e-02  1.3181e-03\n",
      "  1.3099e-03 -1.4717e-02  9.7689e-05\n",
      " -7.2343e-03 -2.2868e-02 -3.3736e-03\n",
      "    ... \n",
      "\n",
      "(509,509,.,.) = \n",
      "  1.2331e-02  7.4124e-03  1.2749e-02\n",
      " -8.8850e-04 -4.5861e-03  3.7479e-03\n",
      " -7.1749e-03 -6.7384e-03 -3.3736e-03\n",
      "\n",
      "(509,510,.,.) = \n",
      "  3.8277e-03  5.8917e-03 -8.5206e-03\n",
      " -3.5528e-02 -4.1981e-02 -4.2986e-02\n",
      " -2.4858e-02 -1.7028e-02 -9.4561e-03\n",
      "\n",
      "(509,511,.,.) = \n",
      "  1.8104e-02 -4.0711e-03 -5.2728e-03\n",
      "  2.4094e-03  6.0447e-03 -3.6141e-06\n",
      " -1.4044e-02 -5.3809e-03 -5.4912e-03\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -6.4042e-03 -1.1728e-02 -1.2212e-02\n",
      " -1.4843e-02 -1.2769e-02 -1.8943e-02\n",
      " -6.7984e-03 -7.0257e-03 -1.1770e-02\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      " -2.4349e-02 -3.1116e-02 -1.1133e-02\n",
      " -2.3577e-02 -2.7449e-02 -6.6376e-03\n",
      " -1.6168e-02 -2.2754e-02 -1.1713e-02\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      "  2.2752e-03  1.0183e-02  8.4564e-03\n",
      "  7.4659e-03 -3.9440e-03  3.5635e-03\n",
      "  1.4348e-02 -3.1717e-03 -9.8797e-03\n",
      "    ... \n",
      "\n",
      "(510,509,.,.) = \n",
      " -2.7519e-03 -2.2519e-02 -1.8585e-02\n",
      "  1.2431e-02 -1.7788e-02 -2.2280e-02\n",
      "  3.0251e-02 -8.2980e-03 -1.2030e-02\n",
      "\n",
      "(510,510,.,.) = \n",
      " -1.2296e-02 -2.2815e-02 -1.2514e-02\n",
      " -2.3781e-02 -2.4428e-02 -1.4318e-02\n",
      " -2.0756e-02 -1.4121e-02  2.7448e-03\n",
      "\n",
      "(510,511,.,.) = \n",
      "  8.4558e-03  6.3274e-03 -9.6884e-03\n",
      "  9.1066e-03  1.1711e-03 -2.1234e-02\n",
      "  1.1942e-02  1.2645e-04 -1.3511e-02\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      "  1.9637e-03  2.3357e-02  4.8063e-03\n",
      "  3.3267e-03  2.7266e-02  1.2812e-02\n",
      "  6.5395e-03  1.9035e-02  7.0538e-03\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      "  1.2300e-02  1.4830e-02  2.3591e-02\n",
      "  1.3727e-02  1.9088e-02  1.5915e-02\n",
      " -1.6019e-03  6.1617e-03 -1.4637e-04\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      "  1.9550e-02 -1.7466e-02 -1.1364e-02\n",
      "  5.2216e-03 -2.7643e-02 -1.6557e-02\n",
      "  8.0360e-03 -1.8231e-02 -2.6932e-02\n",
      "    ... \n",
      "\n",
      "(511,509,.,.) = \n",
      "  1.0374e-02  1.5462e-02 -1.3095e-03\n",
      "  3.8727e-03  3.2918e-02  1.2951e-02\n",
      "  8.1074e-04  1.2805e-02  9.5454e-04\n",
      "\n",
      "(511,510,.,.) = \n",
      " -3.0461e-03 -2.4155e-03 -1.2607e-02\n",
      "  1.0073e-03 -4.2310e-03 -1.3901e-02\n",
      " -4.0841e-03 -1.4759e-02 -1.8056e-02\n",
      "\n",
      "(511,511,.,.) = \n",
      " -6.2155e-03  1.8488e-02  4.8508e-03\n",
      " -6.7020e-03  1.8798e-02  7.4688e-03\n",
      " -1.0357e-02  8.4025e-03  2.3861e-03\n",
      "[torch.FloatTensor of size 512x512x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      " -0.0762\n",
      "  0.5109\n",
      "  0.2586\n",
      "  0.0110\n",
      " -0.0173\n",
      "  0.6643\n",
      " -0.5842\n",
      " -0.8995\n",
      "  0.4180\n",
      "  0.4773\n",
      "  0.0821\n",
      "  0.2321\n",
      " -0.1979\n",
      " -0.1215\n",
      " -0.1786\n",
      " -0.2529\n",
      "  0.0015\n",
      "  0.2674\n",
      " -0.5173\n",
      " -0.0863\n",
      "  0.0789\n",
      " -0.5168\n",
      " -0.3142\n",
      "  0.2681\n",
      " -0.0653\n",
      " -0.1456\n",
      "  0.1036\n",
      " -0.3054\n",
      " -0.2668\n",
      "  0.1090\n",
      "  0.5451\n",
      "  0.1152\n",
      " -0.1029\n",
      " -0.3496\n",
      " -0.4548\n",
      " -0.3532\n",
      " -0.0419\n",
      "  0.4644\n",
      "  0.4037\n",
      " -0.0355\n",
      " -0.2728\n",
      " -0.3435\n",
      " -0.2289\n",
      "  0.5919\n",
      " -0.4665\n",
      " -0.3133\n",
      " -0.9245\n",
      " -0.1498\n",
      " -0.1613\n",
      " -0.7727\n",
      "  0.1366\n",
      " -0.0963\n",
      "  0.0824\n",
      "  0.3597\n",
      " -0.1228\n",
      " -0.5980\n",
      "  0.1128\n",
      " -0.4060\n",
      "  0.2309\n",
      "  0.2247\n",
      "  0.2951\n",
      " -0.6363\n",
      " -0.0892\n",
      "  0.2370\n",
      "  0.2950\n",
      " -0.2962\n",
      "  0.0151\n",
      "  0.0089\n",
      " -0.1763\n",
      " -0.4404\n",
      " -0.1555\n",
      " -0.3260\n",
      " -0.1617\n",
      " -0.4749\n",
      "  0.4901\n",
      " -0.0749\n",
      "  0.1459\n",
      "  0.1645\n",
      "  0.3070\n",
      "  0.1394\n",
      " -0.8240\n",
      "  0.0432\n",
      " -0.0414\n",
      "  0.3542\n",
      " -0.7887\n",
      " -0.3352\n",
      " -0.0455\n",
      " -0.1216\n",
      " -0.2457\n",
      " -0.7815\n",
      "  0.7356\n",
      " -0.4165\n",
      "  0.5501\n",
      "  0.2699\n",
      " -0.2039\n",
      " -0.4530\n",
      " -0.9786\n",
      " -0.0284\n",
      "  0.5921\n",
      " -0.2143\n",
      "  0.3034\n",
      " -0.3778\n",
      "  0.2712\n",
      " -0.5554\n",
      "  0.0814\n",
      "  0.3502\n",
      " -0.0913\n",
      "  0.2269\n",
      " -0.3565\n",
      "  0.2794\n",
      "  0.1430\n",
      " -0.4844\n",
      " -0.5025\n",
      " -0.0429\n",
      "  0.0021\n",
      "  0.1722\n",
      "  0.0440\n",
      " -0.6529\n",
      "  0.1806\n",
      " -0.1742\n",
      " -0.6057\n",
      "  0.4364\n",
      "  0.0347\n",
      " -0.9033\n",
      "  0.0483\n",
      " -0.1006\n",
      " -0.3572\n",
      "  0.0975\n",
      " -0.4307\n",
      " -0.1657\n",
      "  0.0311\n",
      "  0.0732\n",
      " -0.4817\n",
      " -0.6926\n",
      " -0.8409\n",
      "  0.0303\n",
      " -0.0181\n",
      " -0.0154\n",
      " -0.4989\n",
      "  0.6572\n",
      "  0.4487\n",
      "  0.3596\n",
      "  0.6671\n",
      "  0.0126\n",
      " -0.4922\n",
      " -0.1505\n",
      "  0.1671\n",
      " -0.1517\n",
      " -0.1117\n",
      " -0.2137\n",
      " -0.5537\n",
      " -0.1951\n",
      " -0.4312\n",
      " -0.0552\n",
      " -0.0906\n",
      "  0.2068\n",
      " -0.4563\n",
      " -0.3031\n",
      "  0.0387\n",
      " -0.4313\n",
      "  0.3916\n",
      "  0.3918\n",
      " -0.5229\n",
      " -0.5930\n",
      " -0.2558\n",
      "  0.1324\n",
      " -0.2703\n",
      " -0.7383\n",
      "  0.0886\n",
      " -0.4471\n",
      "  0.1969\n",
      "  0.2180\n",
      "  0.3209\n",
      " -0.5520\n",
      "  0.0394\n",
      "  0.3410\n",
      " -0.3034\n",
      "  1.1262\n",
      " -0.1424\n",
      "  0.2302\n",
      " -0.1715\n",
      "  0.2853\n",
      "  0.0394\n",
      " -0.2037\n",
      "  0.2619\n",
      " -0.4813\n",
      "  0.2736\n",
      "  0.0463\n",
      "  0.0477\n",
      "  0.5246\n",
      "  0.2510\n",
      "  0.6168\n",
      "  0.1549\n",
      " -0.7560\n",
      " -0.6665\n",
      " -0.1904\n",
      " -0.3516\n",
      "  0.1902\n",
      " -0.6540\n",
      " -0.1264\n",
      " -0.4433\n",
      " -0.1634\n",
      " -0.2144\n",
      " -0.9065\n",
      " -0.0368\n",
      "  0.2224\n",
      " -0.3847\n",
      " -0.5501\n",
      "  0.3788\n",
      " -0.4928\n",
      " -0.0743\n",
      "  0.0416\n",
      "  0.2226\n",
      " -0.0804\n",
      " -0.3266\n",
      " -0.4029\n",
      " -0.3762\n",
      "  0.3124\n",
      " -0.3303\n",
      " -0.5435\n",
      " -0.3804\n",
      "  0.2694\n",
      "  0.2407\n",
      " -0.1345\n",
      "  0.1076\n",
      " -0.0941\n",
      " -0.1040\n",
      " -0.3861\n",
      " -0.2934\n",
      " -0.2648\n",
      " -0.4418\n",
      "  0.4108\n",
      " -0.3656\n",
      "  0.1743\n",
      "  0.1336\n",
      "  0.7127\n",
      " -0.4045\n",
      " -0.4888\n",
      " -0.1571\n",
      " -0.4584\n",
      " -0.0957\n",
      "  1.0008\n",
      " -0.2351\n",
      " -0.1802\n",
      "  0.3529\n",
      " -0.0616\n",
      " -0.4525\n",
      " -0.0208\n",
      " -0.4412\n",
      " -0.1252\n",
      " -0.1152\n",
      " -0.0290\n",
      " -0.4382\n",
      " -0.3148\n",
      " -0.4248\n",
      " -0.1481\n",
      " -0.0393\n",
      "  0.5182\n",
      " -0.1154\n",
      "  0.1226\n",
      " -0.2396\n",
      "  0.2695\n",
      "  0.2372\n",
      " -0.0034\n",
      " -0.5958\n",
      "  0.0020\n",
      " -0.2569\n",
      "  0.4340\n",
      "  0.8538\n",
      " -0.0702\n",
      "  1.0735\n",
      " -0.4201\n",
      "  0.0288\n",
      " -0.5426\n",
      "  0.2029\n",
      " -0.2734\n",
      " -0.3228\n",
      " -0.4487\n",
      " -0.0948\n",
      "  0.2625\n",
      " -0.8400\n",
      " -0.2274\n",
      "  0.0852\n",
      " -1.2213\n",
      " -0.1093\n",
      "  0.3401\n",
      " -0.1847\n",
      "  0.3571\n",
      "  0.0123\n",
      " -0.0024\n",
      " -0.1373\n",
      " -0.1477\n",
      "  0.0034\n",
      "  0.0648\n",
      "  0.3166\n",
      "  0.2516\n",
      " -0.0810\n",
      "  0.2176\n",
      "  0.1406\n",
      " -0.1514\n",
      " -0.1047\n",
      " -0.0213\n",
      "  0.0035\n",
      " -0.3769\n",
      " -0.6076\n",
      " -0.3275\n",
      "  0.3723\n",
      " -0.5551\n",
      " -0.3542\n",
      " -0.0611\n",
      " -0.8105\n",
      " -0.0763\n",
      "  0.1517\n",
      " -0.0071\n",
      " -0.0700\n",
      " -0.2682\n",
      "  0.8272\n",
      " -0.3915\n",
      "  0.1293\n",
      " -0.4768\n",
      " -0.0799\n",
      "  0.1940\n",
      " -0.6365\n",
      " -1.1688\n",
      " -0.5271\n",
      "  0.4807\n",
      "  0.0592\n",
      "  0.3258\n",
      "  0.1363\n",
      "  0.1557\n",
      "  0.0219\n",
      "  0.1152\n",
      " -0.5446\n",
      " -0.6016\n",
      "  0.0735\n",
      " -0.3608\n",
      " -0.2741\n",
      " -0.1179\n",
      " -0.3742\n",
      "  0.0396\n",
      "  0.2414\n",
      " -0.6141\n",
      " -0.5970\n",
      " -0.2694\n",
      "  0.6704\n",
      " -0.5656\n",
      " -0.5588\n",
      " -0.0665\n",
      " -0.6990\n",
      "  0.5152\n",
      " -0.0173\n",
      " -0.5098\n",
      " -0.6899\n",
      "  0.2285\n",
      " -0.4553\n",
      " -0.7710\n",
      " -0.2187\n",
      "  0.4629\n",
      " -0.0502\n",
      "  0.0608\n",
      " -0.1762\n",
      " -0.2772\n",
      " -0.0674\n",
      "  0.2076\n",
      "  0.1527\n",
      "  0.1354\n",
      " -0.2663\n",
      " -0.4040\n",
      " -0.1012\n",
      " -0.6540\n",
      "  0.3548\n",
      " -0.4514\n",
      " -0.2888\n",
      " -0.2530\n",
      "  0.4926\n",
      " -0.4250\n",
      " -0.9585\n",
      "  0.0255\n",
      " -0.0083\n",
      " -0.5987\n",
      "  0.8744\n",
      " -0.1496\n",
      " -0.1189\n",
      "  0.4445\n",
      "  0.1025\n",
      " -0.0500\n",
      " -0.2008\n",
      " -0.6338\n",
      " -0.3913\n",
      " -0.4049\n",
      "  0.4332\n",
      " -0.5939\n",
      "  0.3001\n",
      "  0.1120\n",
      "  0.1797\n",
      " -0.4814\n",
      "  0.0535\n",
      "  0.3464\n",
      "  0.1434\n",
      " -0.1411\n",
      "  0.4872\n",
      " -0.4571\n",
      "  0.2558\n",
      " -0.7047\n",
      " -0.6741\n",
      " -0.2782\n",
      " -0.1910\n",
      " -0.4456\n",
      " -0.0932\n",
      "  0.0194\n",
      "  0.2357\n",
      " -0.3036\n",
      "  0.5567\n",
      "  0.0601\n",
      "  0.2192\n",
      " -0.3199\n",
      "  0.1718\n",
      "  0.0652\n",
      " -0.5315\n",
      "  0.0983\n",
      " -0.5363\n",
      "  0.5597\n",
      "  0.5828\n",
      "  0.1114\n",
      "  0.3116\n",
      " -0.3686\n",
      "  0.1712\n",
      "  1.3308\n",
      " -0.3288\n",
      "  0.1429\n",
      " -0.0025\n",
      " -0.2501\n",
      "  0.4785\n",
      "  0.1768\n",
      " -0.2769\n",
      " -0.1687\n",
      " -0.5241\n",
      "  0.6276\n",
      " -0.3024\n",
      "  0.1216\n",
      " -0.0545\n",
      "  0.0791\n",
      "  0.4223\n",
      " -0.5299\n",
      " -0.6074\n",
      "  0.3703\n",
      " -0.2105\n",
      " -0.1640\n",
      " -0.0067\n",
      "  0.4353\n",
      " -0.0511\n",
      "  0.3444\n",
      " -0.2210\n",
      " -0.2627\n",
      " -0.0454\n",
      " -0.1737\n",
      " -1.0629\n",
      " -0.3264\n",
      "  0.3437\n",
      "  0.1063\n",
      "  0.1769\n",
      " -0.1382\n",
      " -1.1655\n",
      " -0.4384\n",
      "  0.1628\n",
      " -0.1244\n",
      " -0.7684\n",
      " -0.7676\n",
      " -0.3386\n",
      "  0.1552\n",
      " -1.1846\n",
      "  0.1250\n",
      "  0.0336\n",
      "  0.6815\n",
      " -0.0568\n",
      " -0.1021\n",
      " -0.0601\n",
      "  0.1684\n",
      "  0.2091\n",
      " -0.2598\n",
      "  0.6453\n",
      " -0.5077\n",
      "  0.1255\n",
      "  0.3294\n",
      "  0.8595\n",
      "  0.3682\n",
      " -0.1804\n",
      " -0.7372\n",
      "  0.6267\n",
      " -0.7059\n",
      " -0.7760\n",
      "  0.1862\n",
      "  0.0207\n",
      " -0.1840\n",
      " -0.1801\n",
      "  0.1120\n",
      " -0.4349\n",
      "  0.2399\n",
      " -0.6996\n",
      " -0.2298\n",
      "  0.4339\n",
      "  0.4565\n",
      "  0.6231\n",
      " -0.1108\n",
      " -0.0812\n",
      "  0.0402\n",
      " -0.5740\n",
      " -0.2846\n",
      " -0.2810\n",
      " -0.0712\n",
      " -0.0484\n",
      " -0.2895\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2834\n",
      " 0.2865\n",
      " 0.2266\n",
      " 0.2758\n",
      " 0.2761\n",
      " 0.2799\n",
      " 0.2220\n",
      " 0.2888\n",
      " 0.2480\n",
      " 0.2305\n",
      " 0.2034\n",
      " 0.2520\n",
      " 0.2714\n",
      " 0.2792\n",
      " 0.2909\n",
      " 0.2105\n",
      " 0.3065\n",
      " 0.2132\n",
      " 0.2925\n",
      " 0.2544\n",
      " 0.2554\n",
      " 0.2572\n",
      " 0.2951\n",
      " 0.3001\n",
      " 0.3085\n",
      " 0.1151\n",
      " 0.2249\n",
      " 0.2450\n",
      " 0.2226\n",
      " 0.2515\n",
      " 0.3180\n",
      " 0.2632\n",
      " 0.2552\n",
      " 0.3036\n",
      " 0.2004\n",
      " 0.2339\n",
      " 0.1808\n",
      " 0.2693\n",
      " 0.2167\n",
      " 0.0609\n",
      " 0.2723\n",
      " 0.2348\n",
      " 0.2221\n",
      " 0.2423\n",
      " 0.2277\n",
      " 0.2967\n",
      " 0.2609\n",
      " 0.2994\n",
      " 0.3455\n",
      " 0.2283\n",
      " 0.2745\n",
      " 0.2607\n",
      " 0.2654\n",
      " 0.2418\n",
      " 0.1920\n",
      " 0.2456\n",
      " 0.2767\n",
      " 0.2752\n",
      " 0.2910\n",
      " 0.2921\n",
      " 0.2181\n",
      " 0.2048\n",
      " 0.1641\n",
      " 0.2756\n",
      " 0.2633\n",
      " 0.2673\n",
      " 0.2325\n",
      " 0.2698\n",
      " 0.2613\n",
      " 0.2369\n",
      " 0.2291\n",
      " 0.2737\n",
      " 0.2548\n",
      " 0.2890\n",
      " 0.2721\n",
      " 0.2574\n",
      " 0.2758\n",
      " 0.1711\n",
      " 0.2100\n",
      " 0.3295\n",
      " 0.3257\n",
      " 0.2390\n",
      " 0.2683\n",
      " 0.2742\n",
      " 0.2758\n",
      " 0.2176\n",
      " 0.2503\n",
      " 0.2889\n",
      " 0.2104\n",
      " 0.2877\n",
      " 0.2664\n",
      " 0.2953\n",
      " 0.2159\n",
      " 0.2425\n",
      " 0.2390\n",
      " 0.2352\n",
      " 0.1549\n",
      " 0.2596\n",
      " 0.2456\n",
      " 0.2576\n",
      " 0.2942\n",
      " 0.2892\n",
      " 0.2228\n",
      " 0.2560\n",
      " 0.2238\n",
      " 0.2149\n",
      " 0.1205\n",
      " 0.2660\n",
      " 0.2048\n",
      " 0.2296\n",
      " 0.2203\n",
      " 0.2830\n",
      " 0.2388\n",
      " 0.2513\n",
      " 0.2441\n",
      " 0.3173\n",
      " 0.3022\n",
      " 0.3176\n",
      " 0.2563\n",
      " 0.2474\n",
      " 0.2742\n",
      " 0.2828\n",
      " 0.3489\n",
      " 0.1696\n",
      " 0.2809\n",
      " 0.2948\n",
      " 0.2204\n",
      " 0.2686\n",
      " 0.2265\n",
      " 0.2255\n",
      " 0.2105\n",
      " 0.2365\n",
      " 0.3052\n",
      " 0.2979\n",
      " 0.2948\n",
      " 0.3772\n",
      " 0.3438\n",
      " 0.2381\n",
      " 0.2732\n",
      " 0.1777\n",
      " 0.2421\n",
      " 0.2512\n",
      " 0.2604\n",
      " 0.2273\n",
      " 0.1724\n",
      " 0.3358\n",
      " 0.2350\n",
      " 0.2393\n",
      " 0.2789\n",
      " 0.2161\n",
      " 0.2249\n",
      " 0.2510\n",
      " 0.1308\n",
      " 0.2438\n",
      " 0.2213\n",
      " 0.2462\n",
      " 0.2322\n",
      " 0.2615\n",
      " 0.3051\n",
      " 0.2451\n",
      " 0.2583\n",
      " 0.2535\n",
      " 0.2219\n",
      " 0.2050\n",
      " 0.3035\n",
      " 0.2610\n",
      " 0.2213\n",
      " 0.2882\n",
      " 0.2973\n",
      " 0.3043\n",
      " 0.2419\n",
      " 0.2856\n",
      " 0.2513\n",
      " 0.2751\n",
      " 0.2681\n",
      " 0.2424\n",
      " 0.2976\n",
      " 0.2882\n",
      " 0.3318\n",
      " 0.2598\n",
      " 0.1852\n",
      " 0.2431\n",
      " 0.3050\n",
      " 0.2277\n",
      " 0.2166\n",
      " 0.2424\n",
      " 0.2348\n",
      " 0.2691\n",
      " 0.2626\n",
      " 0.2333\n",
      " 0.2302\n",
      " 0.2130\n",
      " 0.2987\n",
      " 0.2788\n",
      " 0.3569\n",
      " 0.2676\n",
      " 0.2662\n",
      " 0.2594\n",
      " 0.2962\n",
      " 0.2034\n",
      " 0.2325\n",
      " 0.2253\n",
      " 0.1992\n",
      " 0.3122\n",
      " 0.2485\n",
      " 0.3025\n",
      " 0.2144\n",
      " 0.2441\n",
      " 0.4273\n",
      " 0.2450\n",
      " 0.0189\n",
      " 0.2352\n",
      " 0.3054\n",
      " 0.2691\n",
      " 0.2895\n",
      " 0.2364\n",
      " 0.1847\n",
      " 0.2808\n",
      " 0.2467\n",
      " 0.2524\n",
      " 0.0880\n",
      " 0.2575\n",
      " 0.2524\n",
      " 0.2254\n",
      " 0.2661\n",
      " 0.3184\n",
      " 0.2588\n",
      " 0.2476\n",
      " 0.2858\n",
      " 0.2930\n",
      " 0.2513\n",
      " 0.2654\n",
      " 0.1716\n",
      " 0.2711\n",
      " 0.2427\n",
      " 0.3020\n",
      " 0.2578\n",
      " 0.3017\n",
      " 0.2406\n",
      " 0.3300\n",
      " 0.2897\n",
      " 0.2817\n",
      " 0.3296\n",
      " 0.2404\n",
      " 0.2418\n",
      " 0.2486\n",
      " 0.2789\n",
      " 0.1714\n",
      " 0.2489\n",
      " 0.2724\n",
      " 0.3071\n",
      " 0.2506\n",
      " 0.3286\n",
      " 0.2238\n",
      " 0.2484\n",
      " 0.2448\n",
      " 0.2002\n",
      " 0.2755\n",
      " 0.2506\n",
      " 0.2580\n",
      " 0.2432\n",
      " 0.2659\n",
      " 0.2527\n",
      " 0.2378\n",
      " 0.3124\n",
      " 0.3319\n",
      " 0.2070\n",
      " 0.2392\n",
      " 0.1236\n",
      " 0.2399\n",
      " 0.3137\n",
      " 0.3911\n",
      " 0.2657\n",
      " 0.2276\n",
      " 0.2297\n",
      " 0.3031\n",
      " 0.2652\n",
      " 0.2698\n",
      " 0.2703\n",
      " 0.2768\n",
      " 0.2471\n",
      " 0.2322\n",
      " 0.2011\n",
      " 0.2949\n",
      " 0.2023\n",
      " 0.2714\n",
      " 0.2762\n",
      " 0.2872\n",
      " 0.3212\n",
      " 0.2332\n",
      " 0.1823\n",
      " 0.2798\n",
      " 0.2466\n",
      " 0.2910\n",
      " 0.2469\n",
      " 0.2522\n",
      " 0.2262\n",
      " 0.2344\n",
      " 0.2169\n",
      " 0.2560\n",
      " 0.3010\n",
      " 0.3175\n",
      " 0.2478\n",
      " 0.1703\n",
      " 0.2731\n",
      " 0.2623\n",
      " 0.2958\n",
      " 0.3244\n",
      " 0.2685\n",
      " 0.2562\n",
      " 0.2530\n",
      " 0.3214\n",
      " 0.2651\n",
      " 0.3027\n",
      " 0.2427\n",
      " 0.2721\n",
      " 0.2459\n",
      " 0.3093\n",
      " 0.2110\n",
      " 0.2381\n",
      " 0.2921\n",
      " 0.2669\n",
      " 0.3138\n",
      " 0.4304\n",
      " 0.2114\n",
      " 0.2229\n",
      " 0.2363\n",
      " 0.2398\n",
      " 0.2430\n",
      " 0.2503\n",
      " 0.3220\n",
      " 0.2802\n",
      " 0.2879\n",
      " 0.2694\n",
      " 0.2055\n",
      " 0.2299\n",
      " 0.3187\n",
      " 0.2028\n",
      " 0.2435\n",
      " 0.3244\n",
      " 0.2581\n",
      " 0.2237\n",
      " 0.2251\n",
      " 0.2570\n",
      " 0.2118\n",
      " 0.1396\n",
      " 0.2232\n",
      " 0.2397\n",
      " 0.1945\n",
      " 0.2415\n",
      " 0.2024\n",
      " 0.1775\n",
      " 0.2944\n",
      " 0.3224\n",
      " 0.3072\n",
      " 0.2504\n",
      " 0.2567\n",
      " 0.3023\n",
      " 0.2251\n",
      " 0.2693\n",
      " 0.1904\n",
      " 0.2517\n",
      " 0.3834\n",
      " 0.3034\n",
      " 0.2038\n",
      " 0.2394\n",
      " 0.2463\n",
      " 0.2616\n",
      " 0.2957\n",
      " 0.2633\n",
      " 0.1756\n",
      " 0.1958\n",
      " 0.1824\n",
      " 0.4348\n",
      " 0.2756\n",
      " 0.2291\n",
      " 0.2939\n",
      " 0.2433\n",
      " 0.2581\n",
      " 0.2072\n",
      " 0.2826\n",
      " 0.2369\n",
      " 0.2655\n",
      " 0.2162\n",
      " 0.2539\n",
      " 0.1796\n",
      " 0.2786\n",
      " 0.1788\n",
      " 0.2819\n",
      " 0.2474\n",
      " 0.2855\n",
      " 0.2569\n",
      " 0.2688\n",
      " 0.2910\n",
      " 0.4027\n",
      " 0.3600\n",
      " 0.2427\n",
      " 0.3194\n",
      " 0.2634\n",
      " 0.2826\n",
      " 0.1310\n",
      " 0.2759\n",
      " 0.3253\n",
      " 0.2589\n",
      " 0.2663\n",
      " 0.2957\n",
      " 0.4180\n",
      " 0.2611\n",
      " 0.2313\n",
      " 0.2576\n",
      " 0.2145\n",
      " 0.2646\n",
      " 0.1876\n",
      " 0.3542\n",
      " 0.2626\n",
      " 0.2473\n",
      " 0.2889\n",
      " 0.2631\n",
      " 0.2407\n",
      " 0.2650\n",
      " 0.3066\n",
      " 0.3446\n",
      " 0.2395\n",
      " 0.2675\n",
      " 0.2486\n",
      " 0.2535\n",
      " 0.2793\n",
      " 0.1893\n",
      " 0.2447\n",
      " 0.2655\n",
      " 0.2980\n",
      " 0.2688\n",
      " 0.2454\n",
      " 0.2246\n",
      " 0.2442\n",
      " 0.2616\n",
      " 0.2595\n",
      " 0.2261\n",
      " 0.3279\n",
      " 0.2503\n",
      " 0.2558\n",
      " 0.2190\n",
      " 0.2197\n",
      " 0.3079\n",
      " 0.2482\n",
      " 0.3258\n",
      " 0.2502\n",
      " 0.2605\n",
      " 0.2231\n",
      " 0.2659\n",
      " 0.2659\n",
      " 0.2715\n",
      " 0.2784\n",
      " 0.2947\n",
      " 0.2156\n",
      " 0.3235\n",
      " 0.3190\n",
      " 0.2584\n",
      " 0.2886\n",
      " 0.2467\n",
      " 0.2880\n",
      " 0.2639\n",
      " 0.2552\n",
      " 0.2611\n",
      " 0.2552\n",
      " 0.1970\n",
      " 0.2994\n",
      " 0.3410\n",
      " 0.2845\n",
      " 0.2719\n",
      " 0.2504\n",
      " 0.2443\n",
      " 0.2510\n",
      " 0.2559\n",
      " 0.2325\n",
      " 0.3853\n",
      " 0.2912\n",
      " 0.2292\n",
      " 0.2221\n",
      " 0.2684\n",
      " 0.2310\n",
      " 0.2559\n",
      " 0.2431\n",
      " 0.1724\n",
      " 0.2905\n",
      " 0.2461\n",
      " 0.2983\n",
      " 0.2696\n",
      " 0.2585\n",
      " 0.2887\n",
      " 0.3772\n",
      " 0.2508\n",
      " 0.2469\n",
      " 0.1874\n",
      " 0.1697\n",
      " 0.2626\n",
      " 0.2298\n",
      " 0.1790\n",
      " 0.2526\n",
      " 0.2120\n",
      " 0.2829\n",
      " 0.2833\n",
      " 0.2871\n",
      " 0.1892\n",
      " 0.2237\n",
      " 0.2153\n",
      " 0.2621\n",
      " 0.2181\n",
      " 0.2947\n",
      " 0.2882\n",
      " 0.2249\n",
      " 0.2392\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-0.1895\n",
      "-0.2356\n",
      "-0.1479\n",
      "-0.2443\n",
      "-0.1634\n",
      "-0.2426\n",
      "-0.1247\n",
      "-0.3133\n",
      "-0.1755\n",
      "-0.1215\n",
      "-0.0677\n",
      "-0.1584\n",
      "-0.1815\n",
      "-0.1799\n",
      "-0.2639\n",
      "-0.1503\n",
      "-0.2805\n",
      "-0.1178\n",
      "-0.3342\n",
      "-0.2346\n",
      "-0.1498\n",
      "-0.2070\n",
      "-0.1845\n",
      "-0.2825\n",
      "-0.3172\n",
      "-0.4037\n",
      "-0.1128\n",
      "-0.1609\n",
      "-0.1653\n",
      "-0.1175\n",
      "-0.3373\n",
      "-0.3139\n",
      "-0.1667\n",
      "-0.3396\n",
      " 0.0108\n",
      "-0.0797\n",
      "-0.0088\n",
      "-0.2209\n",
      "-0.0439\n",
      "-0.1631\n",
      "-0.2837\n",
      "-0.1531\n",
      "-0.1655\n",
      "-0.1474\n",
      "-0.1360\n",
      "-0.2359\n",
      "-0.2194\n",
      "-0.3555\n",
      "-0.3544\n",
      "-0.1596\n",
      "-0.2410\n",
      "-0.2414\n",
      "-0.1795\n",
      "-0.1238\n",
      "-0.0930\n",
      "-0.1862\n",
      "-0.2598\n",
      "-0.2659\n",
      "-0.2243\n",
      "-0.2061\n",
      "-0.1252\n",
      "-0.1080\n",
      " 0.0817\n",
      "-0.2393\n",
      "-0.1550\n",
      "-0.2480\n",
      "-0.1559\n",
      "-0.2668\n",
      "-0.2061\n",
      "-0.1810\n",
      "-0.1699\n",
      "-0.1967\n",
      "-0.1822\n",
      "-0.1837\n",
      "-0.2052\n",
      "-0.2214\n",
      "-0.2921\n",
      "-0.0338\n",
      "-0.1209\n",
      "-0.3167\n",
      "-0.4325\n",
      "-0.1578\n",
      "-0.1891\n",
      "-0.2692\n",
      "-0.2117\n",
      "-0.1406\n",
      "-0.2088\n",
      "-0.2846\n",
      "-0.1510\n",
      "-0.2743\n",
      "-0.2153\n",
      "-0.2186\n",
      "-0.1549\n",
      "-0.2275\n",
      "-0.1494\n",
      "-0.1317\n",
      " 0.0199\n",
      "-0.2267\n",
      "-0.1841\n",
      "-0.2153\n",
      "-0.2331\n",
      "-0.2235\n",
      "-0.1589\n",
      "-0.1659\n",
      "-0.1674\n",
      "-0.1117\n",
      "-0.3484\n",
      "-0.2584\n",
      "-0.1343\n",
      "-0.1492\n",
      "-0.1406\n",
      "-0.2745\n",
      "-0.1551\n",
      "-0.1537\n",
      "-0.1651\n",
      "-0.3303\n",
      "-0.2667\n",
      "-0.2260\n",
      "-0.1960\n",
      "-0.2058\n",
      "-0.2410\n",
      "-0.3179\n",
      "-0.4273\n",
      "-0.0207\n",
      "-0.2912\n",
      "-0.2257\n",
      "-0.1786\n",
      "-0.2293\n",
      "-0.1377\n",
      "-0.1896\n",
      "-0.1168\n",
      "-0.1357\n",
      "-0.3006\n",
      "-0.3350\n",
      "-0.2273\n",
      "-0.5351\n",
      "-0.3384\n",
      "-0.1117\n",
      "-0.3034\n",
      "-0.0246\n",
      "-0.1268\n",
      "-0.1386\n",
      "-0.1886\n",
      "-0.1900\n",
      " 0.0227\n",
      "-0.3529\n",
      "-0.1907\n",
      "-0.1657\n",
      "-0.2718\n",
      "-0.1090\n",
      "-0.1641\n",
      "-0.1710\n",
      " 0.0881\n",
      "-0.1378\n",
      "-0.1451\n",
      "-0.1627\n",
      "-0.1509\n",
      "-0.2428\n",
      "-0.3206\n",
      "-0.2244\n",
      "-0.2413\n",
      "-0.1802\n",
      "-0.0996\n",
      "-0.0542\n",
      "-0.2178\n",
      "-0.2039\n",
      "-0.1194\n",
      "-0.3055\n",
      "-0.3552\n",
      "-0.3032\n",
      "-0.1034\n",
      "-0.1515\n",
      "-0.2241\n",
      "-0.1993\n",
      "-0.2218\n",
      "-0.1011\n",
      "-0.2634\n",
      "-0.2376\n",
      "-0.2967\n",
      "-0.1507\n",
      "-0.0507\n",
      "-0.1738\n",
      "-0.1886\n",
      "-0.1740\n",
      "-0.1539\n",
      "-0.1424\n",
      "-0.1625\n",
      "-0.2200\n",
      "-0.2392\n",
      "-0.1326\n",
      "-0.1439\n",
      "-0.0901\n",
      "-0.2398\n",
      "-0.2466\n",
      "-0.3580\n",
      "-0.2316\n",
      "-0.1879\n",
      "-0.1638\n",
      "-0.2539\n",
      "-0.1185\n",
      "-0.1728\n",
      "-0.1702\n",
      "-0.0992\n",
      "-0.2299\n",
      "-0.2485\n",
      "-0.2648\n",
      "-0.0609\n",
      "-0.2258\n",
      "-0.6712\n",
      "-0.1639\n",
      "-0.0729\n",
      "-0.1465\n",
      "-0.3793\n",
      "-0.1623\n",
      "-0.2315\n",
      "-0.1609\n",
      "-0.0566\n",
      "-0.2450\n",
      "-0.1003\n",
      "-0.2571\n",
      "-0.4071\n",
      "-0.1926\n",
      "-0.1268\n",
      "-0.0981\n",
      "-0.2344\n",
      "-0.3058\n",
      "-0.2054\n",
      "-0.1356\n",
      "-0.2231\n",
      "-0.2495\n",
      "-0.2226\n",
      "-0.2179\n",
      "-0.2730\n",
      "-0.2382\n",
      "-0.1792\n",
      "-0.2906\n",
      "-0.1857\n",
      "-0.3402\n",
      "-0.1554\n",
      "-0.2832\n",
      "-0.2653\n",
      "-0.2368\n",
      "-0.2949\n",
      "-0.1435\n",
      "-0.1803\n",
      "-0.1130\n",
      "-0.2397\n",
      "-0.4170\n",
      "-0.1118\n",
      "-0.1727\n",
      "-0.2905\n",
      "-0.1721\n",
      "-0.3674\n",
      "-0.0807\n",
      "-0.1541\n",
      "-0.1443\n",
      "-0.1144\n",
      "-0.1989\n",
      "-0.2332\n",
      "-0.2352\n",
      "-0.2208\n",
      "-0.1485\n",
      "-0.2128\n",
      "-0.1862\n",
      "-0.2574\n",
      "-0.3158\n",
      "-0.1135\n",
      "-0.1799\n",
      " 0.0595\n",
      "-0.1845\n",
      "-0.3313\n",
      "-0.3586\n",
      "-0.2319\n",
      "-0.0520\n",
      "-0.1573\n",
      "-0.2038\n",
      "-0.1975\n",
      "-0.2911\n",
      "-0.1991\n",
      "-0.2552\n",
      "-0.2514\n",
      "-0.1306\n",
      "-0.0615\n",
      "-0.2355\n",
      " 0.0870\n",
      "-0.1509\n",
      "-0.2605\n",
      "-0.3550\n",
      "-0.3220\n",
      "-0.1698\n",
      "-0.0853\n",
      "-0.3295\n",
      "-0.1029\n",
      "-0.2776\n",
      "-0.2169\n",
      "-0.2613\n",
      "-0.1462\n",
      "-0.1502\n",
      "-0.1409\n",
      "-0.2465\n",
      "-0.2448\n",
      "-0.3181\n",
      "-0.2804\n",
      "-0.0078\n",
      "-0.1807\n",
      "-0.2110\n",
      "-0.1942\n",
      "-0.5219\n",
      "-0.1936\n",
      "-0.1416\n",
      "-0.1447\n",
      "-0.3670\n",
      "-0.2052\n",
      "-0.2887\n",
      "-0.2168\n",
      "-0.2099\n",
      "-0.1588\n",
      "-0.2984\n",
      "-0.0837\n",
      "-0.1626\n",
      "-0.2172\n",
      "-0.1501\n",
      "-0.2560\n",
      "-0.4005\n",
      "-0.1258\n",
      "-0.1652\n",
      "-0.2029\n",
      "-0.1219\n",
      "-0.1829\n",
      "-0.1700\n",
      "-0.3256\n",
      "-0.2074\n",
      "-0.1932\n",
      "-0.1506\n",
      "-0.0772\n",
      "-0.1832\n",
      "-0.3265\n",
      "-0.0250\n",
      "-0.1800\n",
      "-0.2816\n",
      "-0.1674\n",
      "-0.1685\n",
      "-0.1428\n",
      "-0.2226\n",
      "-0.0855\n",
      " 0.0844\n",
      "-0.0881\n",
      "-0.0183\n",
      "-0.0457\n",
      "-0.1654\n",
      "-0.0765\n",
      " 0.0041\n",
      "-0.2420\n",
      "-0.3736\n",
      "-0.3152\n",
      "-0.2267\n",
      "-0.2550\n",
      "-0.2999\n",
      "-0.0820\n",
      "-0.2563\n",
      "-0.0042\n",
      "-0.3186\n",
      "-0.3853\n",
      "-0.2662\n",
      "-0.0624\n",
      "-0.1777\n",
      "-0.2259\n",
      "-0.1751\n",
      "-0.2691\n",
      "-0.2343\n",
      "-0.0362\n",
      "-0.0727\n",
      "-0.0437\n",
      "-0.3953\n",
      "-0.2984\n",
      "-0.1231\n",
      "-0.2749\n",
      "-0.2097\n",
      "-0.2529\n",
      "-0.0954\n",
      "-0.1369\n",
      "-0.1905\n",
      "-0.2339\n",
      "-0.1122\n",
      "-0.2074\n",
      "-0.0573\n",
      "-0.1883\n",
      "-0.0692\n",
      "-0.2592\n",
      "-0.1724\n",
      "-0.2315\n",
      "-0.2041\n",
      "-0.2230\n",
      "-0.2575\n",
      "-0.3699\n",
      "-0.3708\n",
      "-0.1694\n",
      "-0.3146\n",
      "-0.2166\n",
      "-0.3223\n",
      " 0.1095\n",
      "-0.2861\n",
      "-0.3284\n",
      "-0.1868\n",
      "-0.2769\n",
      "-0.2837\n",
      "-0.3847\n",
      "-0.1658\n",
      "-0.1911\n",
      "-0.0886\n",
      "-0.1246\n",
      "-0.2049\n",
      "-0.0319\n",
      "-0.4059\n",
      "-0.2159\n",
      "-0.1752\n",
      "-0.2412\n",
      "-0.1369\n",
      "-0.1033\n",
      "-0.2333\n",
      "-0.2979\n",
      "-0.4244\n",
      "-0.1606\n",
      "-0.1920\n",
      "-0.1587\n",
      "-0.1578\n",
      "-0.2768\n",
      " 0.0070\n",
      "-0.1573\n",
      "-0.2456\n",
      "-0.2848\n",
      "-0.2011\n",
      "-0.2047\n",
      "-0.1761\n",
      "-0.1913\n",
      "-0.2660\n",
      "-0.2819\n",
      "-0.1521\n",
      "-0.3473\n",
      "-0.1697\n",
      "-0.1307\n",
      "-0.1431\n",
      "-0.1436\n",
      "-0.2912\n",
      "-0.2183\n",
      "-0.3380\n",
      "-0.1434\n",
      "-0.1719\n",
      "-0.1221\n",
      "-0.2791\n",
      "-0.2050\n",
      "-0.2006\n",
      "-0.1786\n",
      "-0.1744\n",
      "-0.1578\n",
      "-0.2936\n",
      "-0.2432\n",
      "-0.2169\n",
      "-0.2926\n",
      "-0.2177\n",
      "-0.2739\n",
      "-0.1840\n",
      "-0.2206\n",
      "-0.1902\n",
      "-0.1558\n",
      "-0.0962\n",
      "-0.3012\n",
      "-0.4694\n",
      "-0.2417\n",
      "-0.1870\n",
      "-0.1164\n",
      "-0.1983\n",
      "-0.1674\n",
      "-0.1666\n",
      "-0.1526\n",
      "-0.4123\n",
      "-0.2784\n",
      "-0.1295\n",
      "-0.0810\n",
      "-0.1206\n",
      "-0.1632\n",
      "-0.1987\n",
      "-0.2093\n",
      " 0.0473\n",
      "-0.3830\n",
      "-0.2146\n",
      "-0.2367\n",
      "-0.2781\n",
      "-0.2138\n",
      "-0.2307\n",
      "-0.3747\n",
      "-0.1952\n",
      "-0.1502\n",
      " 0.0241\n",
      " 0.0466\n",
      "-0.1779\n",
      "-0.1541\n",
      " 0.0393\n",
      "-0.2910\n",
      "-0.1045\n",
      "-0.1852\n",
      "-0.2372\n",
      "-0.2936\n",
      "-0.0718\n",
      "-0.1318\n",
      "-0.1629\n",
      "-0.2205\n",
      "-0.0702\n",
      "-0.2281\n",
      "-0.2345\n",
      "-0.1530\n",
      "-0.1860\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  1.6850e-02  1.1413e-02  6.9076e-03\n",
      "  1.2062e-02  1.1302e-02  7.0607e-03\n",
      "  5.3452e-03  4.2335e-03 -3.4418e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -1.5170e-02 -1.2351e-02 -7.3857e-06\n",
      " -1.7423e-02 -2.4361e-03  4.2391e-04\n",
      " -7.3793e-03 -7.3296e-03  1.0859e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  2.6876e-02 -3.8699e-03 -1.2586e-03\n",
      "  6.0626e-03  2.1423e-04 -5.5946e-04\n",
      " -5.2541e-03 -1.0241e-02 -2.5159e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,509,.,.) = \n",
      " -1.5026e-02  1.4892e-03 -2.6277e-05\n",
      " -6.0221e-03  2.8105e-02  1.5512e-02\n",
      "  1.7562e-02  5.9226e-03 -2.2098e-02\n",
      "\n",
      "( 0 ,510,.,.) = \n",
      " -1.6068e-02 -1.4060e-02  2.1589e-03\n",
      " -4.1705e-03  3.4013e-03  1.6461e-02\n",
      "  9.1628e-03  2.8234e-03  1.2215e-02\n",
      "\n",
      "( 0 ,511,.,.) = \n",
      "  4.3956e-03  6.2214e-03  2.4800e-02\n",
      "  7.9237e-03  1.6633e-02  1.9953e-02\n",
      "  1.6984e-02  2.8252e-02  1.9477e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -2.8517e-02 -1.4798e-02 -2.0753e-02\n",
      " -2.3161e-02 -1.3899e-02 -2.6497e-02\n",
      "  1.5411e-03  2.4185e-03 -1.3549e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  8.0504e-03  4.3456e-02  3.6876e-03\n",
      "  2.4183e-04  1.0874e-02  7.5306e-03\n",
      "  1.3839e-03 -3.8280e-03 -2.9495e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -1.4516e-02 -5.7789e-03  5.3081e-04\n",
      " -5.9049e-03 -8.2871e-03  5.1101e-03\n",
      "  4.0952e-04 -1.5092e-02 -1.3015e-02\n",
      "    ... \n",
      "\n",
      "( 1 ,509,.,.) = \n",
      " -2.3607e-02 -1.2532e-02 -1.1132e-02\n",
      "  4.6020e-03  5.9349e-03  4.0852e-03\n",
      " -1.2354e-02 -1.0238e-02  1.6083e-03\n",
      "\n",
      "( 1 ,510,.,.) = \n",
      " -1.4527e-02 -1.4161e-02 -2.8053e-02\n",
      " -8.7586e-03 -1.0707e-02 -2.2753e-02\n",
      "  1.6844e-02  1.8184e-03 -1.9837e-02\n",
      "\n",
      "( 1 ,511,.,.) = \n",
      " -3.5307e-04  1.3903e-02  4.3495e-03\n",
      "  2.0436e-03  9.5626e-03 -2.6641e-03\n",
      "  6.3719e-03  1.6937e-02  3.4798e-03\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -3.4022e-02 -5.4156e-02 -4.9694e-02\n",
      " -3.5669e-02 -6.8638e-02 -6.4658e-02\n",
      " -1.6135e-02 -4.3318e-02 -4.4840e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -3.2716e-02 -8.6929e-04  5.1595e-03\n",
      " -3.0042e-03  6.4520e-03  2.1340e-02\n",
      " -5.4524e-03  2.3092e-03  1.7709e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -9.5837e-03 -2.0501e-02 -2.3794e-02\n",
      "  3.3871e-03 -1.3631e-03 -1.0469e-02\n",
      "  1.1088e-02  8.1407e-03  5.9059e-03\n",
      "    ... \n",
      "\n",
      "( 2 ,509,.,.) = \n",
      " -3.7997e-03 -6.0791e-03  7.0904e-03\n",
      "  1.1266e-02  6.0675e-03  1.6936e-02\n",
      "  9.3071e-03 -1.2726e-02 -1.7898e-03\n",
      "\n",
      "( 2 ,510,.,.) = \n",
      " -1.0844e-02  1.3761e-03 -6.6518e-03\n",
      "  1.1082e-02  9.3465e-04 -4.9212e-03\n",
      "  7.4793e-03 -8.6750e-03 -1.6989e-03\n",
      "\n",
      "( 2 ,511,.,.) = \n",
      "  1.9424e-02  1.7896e-02  2.0493e-02\n",
      "  1.7677e-02  2.1395e-02  1.9427e-02\n",
      "  4.0892e-03  3.1070e-03  8.7255e-03\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      " -7.0378e-03 -1.2888e-02  3.8520e-03\n",
      "  3.9363e-03  1.1032e-03  1.1910e-02\n",
      "  5.6964e-03  9.3433e-03  1.2230e-02\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      "  3.4345e-02  3.1105e-02  9.6998e-03\n",
      "  7.5742e-02  7.9719e-02  1.4692e-02\n",
      "  3.2563e-02  5.8369e-02  2.0584e-02\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      "  2.2530e-02  8.3445e-03  9.9385e-03\n",
      "  2.0669e-02  1.3496e-02 -3.9293e-03\n",
      "  1.7156e-02  1.8566e-02  1.2344e-02\n",
      "    ... \n",
      "\n",
      "(509,509,.,.) = \n",
      " -1.1474e-02 -1.0265e-02 -7.4536e-03\n",
      " -2.7349e-03  2.0285e-03  2.9813e-03\n",
      " -1.2841e-02  1.0114e-03  2.5923e-03\n",
      "\n",
      "(509,510,.,.) = \n",
      " -1.3012e-02  6.4716e-03  3.5011e-03\n",
      "  2.0929e-02  2.7778e-02  1.8885e-02\n",
      "  1.5627e-02  1.2651e-02  4.5808e-03\n",
      "\n",
      "(509,511,.,.) = \n",
      " -1.4966e-02 -2.1570e-03 -2.1294e-02\n",
      " -1.6941e-02  1.0282e-03 -2.1573e-02\n",
      " -2.2584e-02 -4.4479e-03 -2.9367e-02\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -1.4103e-02  6.7210e-03 -6.8273e-03\n",
      " -6.4729e-03  6.2947e-03  1.3302e-03\n",
      "  1.1876e-04 -5.8582e-03 -1.0001e-02\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      "  8.2923e-04 -7.7105e-03 -4.8412e-03\n",
      "  1.6704e-02  2.1317e-02  2.5513e-02\n",
      " -7.9846e-03 -2.2220e-02  6.9514e-03\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      "  2.1797e-02  2.3842e-02  2.0291e-02\n",
      "  1.6527e-02  2.7900e-02  9.9698e-03\n",
      " -1.5851e-03 -1.6917e-02 -1.0106e-02\n",
      "    ... \n",
      "\n",
      "(510,509,.,.) = \n",
      " -1.8934e-02 -2.6863e-02 -3.3445e-02\n",
      " -2.1999e-02 -1.8236e-02 -3.0985e-02\n",
      " -7.6121e-03 -4.8954e-03 -8.3902e-03\n",
      "\n",
      "(510,510,.,.) = \n",
      "  5.2007e-04 -6.0874e-03 -4.5363e-04\n",
      " -9.4446e-03 -2.1462e-02 -2.4088e-02\n",
      " -1.3746e-02 -1.6576e-02 -1.5094e-02\n",
      "\n",
      "(510,511,.,.) = \n",
      " -1.2306e-02 -6.6513e-03 -8.5334e-03\n",
      " -2.1064e-02 -1.1423e-02 -2.3302e-02\n",
      " -2.2632e-02 -1.4535e-02 -1.8394e-02\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      "  2.9740e-03  3.5790e-03 -1.3339e-02\n",
      "  8.8728e-03  9.8175e-03 -1.0511e-02\n",
      "  1.0351e-02 -4.4938e-03 -6.6180e-03\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      "  6.3839e-03  1.6388e-03 -4.5553e-03\n",
      "  1.2348e-02 -1.5638e-02 -2.8656e-03\n",
      "  8.7840e-03 -5.0710e-03  2.3329e-02\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      "  1.1940e-02  5.2924e-03 -9.7658e-03\n",
      "  5.1419e-04  1.0299e-03 -1.1745e-02\n",
      " -5.7697e-03 -3.8401e-03 -8.1110e-03\n",
      "    ... \n",
      "\n",
      "(511,509,.,.) = \n",
      " -2.4214e-03 -9.0768e-03  1.0162e-03\n",
      " -5.3813e-03 -2.0115e-02  1.0374e-02\n",
      " -1.5076e-03 -1.6754e-02 -1.5026e-02\n",
      "\n",
      "(511,510,.,.) = \n",
      "  1.9734e-03  2.6975e-03  1.1524e-02\n",
      " -7.1861e-03 -5.4590e-03  1.3657e-03\n",
      " -9.1742e-03 -1.3794e-02 -1.0324e-04\n",
      "\n",
      "(511,511,.,.) = \n",
      " -1.6436e-03  2.3253e-03  3.8553e-02\n",
      "  3.2037e-03 -1.7498e-02  2.0760e-02\n",
      "  8.1887e-03 -8.2345e-03  9.0428e-03\n",
      "[torch.FloatTensor of size 512x512x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      "  0.1990\n",
      " -0.8298\n",
      " -0.1423\n",
      " -0.0095\n",
      " -0.2032\n",
      "  0.2726\n",
      " -0.2810\n",
      " -0.2454\n",
      "  0.7551\n",
      " -0.0276\n",
      "  0.0289\n",
      "  0.0023\n",
      " -0.1573\n",
      " -0.6835\n",
      "  0.3725\n",
      "  0.0017\n",
      "  0.5369\n",
      "  0.0824\n",
      " -0.5229\n",
      " -0.3978\n",
      "  1.0463\n",
      " -0.3978\n",
      " -0.3130\n",
      " -0.7230\n",
      "  0.0539\n",
      "  0.0890\n",
      " -0.6732\n",
      " -0.6690\n",
      " -0.5345\n",
      " -0.4041\n",
      "  0.1677\n",
      " -0.1026\n",
      " -0.0589\n",
      " -0.6728\n",
      " -0.6800\n",
      " -0.2577\n",
      " -0.4543\n",
      "  0.1233\n",
      "  0.7634\n",
      "  0.3797\n",
      " -0.4065\n",
      " -0.5745\n",
      "  0.2983\n",
      " -0.6858\n",
      " -0.7400\n",
      " -0.4445\n",
      " -0.7780\n",
      " -0.3617\n",
      " -0.6782\n",
      " -0.1226\n",
      " -0.3834\n",
      " -0.4032\n",
      " -0.4404\n",
      " -0.6808\n",
      "  0.0631\n",
      " -0.4364\n",
      "  0.0814\n",
      "  0.6627\n",
      " -0.4357\n",
      "  0.0329\n",
      " -0.0809\n",
      "  0.3221\n",
      " -1.3318\n",
      " -0.3808\n",
      " -0.6650\n",
      "  0.0230\n",
      "  0.3597\n",
      " -0.0489\n",
      "  0.0397\n",
      " -0.3659\n",
      " -1.2810\n",
      "  0.2535\n",
      " -0.1251\n",
      " -0.4989\n",
      " -0.3927\n",
      " -0.4922\n",
      " -0.0780\n",
      "  0.0257\n",
      "  0.4116\n",
      " -0.2001\n",
      "  0.2848\n",
      " -0.6796\n",
      " -0.5490\n",
      " -0.2768\n",
      " -0.4194\n",
      " -0.5749\n",
      "  0.3069\n",
      "  0.1896\n",
      " -0.4698\n",
      "  0.2814\n",
      "  0.5669\n",
      "  0.5090\n",
      "  0.0720\n",
      " -0.2025\n",
      " -0.4254\n",
      " -0.0908\n",
      "  0.0224\n",
      " -0.0201\n",
      " -1.0254\n",
      "  0.3109\n",
      " -0.6217\n",
      " -1.0197\n",
      " -0.1500\n",
      " -1.0951\n",
      " -0.9758\n",
      "  0.6447\n",
      " -0.2165\n",
      " -0.5433\n",
      " -0.2193\n",
      " -0.6369\n",
      " -1.2529\n",
      " -0.0879\n",
      " -0.0460\n",
      " -0.0213\n",
      " -0.7336\n",
      " -1.2966\n",
      "  0.0354\n",
      " -0.2494\n",
      " -0.4053\n",
      " -0.0715\n",
      " -0.3736\n",
      " -0.8677\n",
      " -0.2488\n",
      "  0.2609\n",
      " -0.5258\n",
      " -0.5287\n",
      "  0.1226\n",
      " -0.1348\n",
      "  0.4943\n",
      " -0.8376\n",
      "  0.3236\n",
      " -1.1887\n",
      " -0.9076\n",
      "  0.3062\n",
      " -0.0147\n",
      " -0.3405\n",
      " -0.8876\n",
      " -0.3990\n",
      "  0.3970\n",
      " -0.4412\n",
      "  0.6488\n",
      "  0.0634\n",
      " -0.1138\n",
      " -0.3631\n",
      "  0.0671\n",
      " -0.7829\n",
      " -0.2575\n",
      " -0.4243\n",
      " -0.7126\n",
      " -0.3459\n",
      " -0.9456\n",
      " -0.9469\n",
      " -0.6448\n",
      " -0.4227\n",
      "  0.2413\n",
      " -0.7291\n",
      "  0.0600\n",
      " -0.3086\n",
      " -0.0426\n",
      "  0.4460\n",
      " -0.2799\n",
      "  0.2569\n",
      " -0.2189\n",
      "  0.0248\n",
      "  0.0879\n",
      " -0.3984\n",
      " -0.2647\n",
      " -0.4222\n",
      " -0.5399\n",
      " -0.9777\n",
      "  0.5476\n",
      " -0.5911\n",
      " -0.6083\n",
      "  0.9595\n",
      "  0.4720\n",
      "  0.2240\n",
      " -0.4585\n",
      "  0.4804\n",
      " -0.9912\n",
      " -0.0390\n",
      "  0.0294\n",
      "  0.6272\n",
      " -1.2239\n",
      " -0.3723\n",
      "  0.1032\n",
      " -0.6996\n",
      "  0.5674\n",
      "  0.1955\n",
      " -0.5028\n",
      " -0.4658\n",
      " -0.5276\n",
      " -0.5709\n",
      " -0.7393\n",
      " -0.5587\n",
      " -0.4696\n",
      "  0.3361\n",
      "  0.1076\n",
      " -0.1368\n",
      " -0.1620\n",
      " -0.2007\n",
      " -0.4896\n",
      " -0.7443\n",
      " -0.1217\n",
      "  0.7835\n",
      " -0.1354\n",
      " -0.5048\n",
      " -0.5487\n",
      " -0.1180\n",
      " -0.0379\n",
      "  0.8462\n",
      " -0.0284\n",
      " -0.5183\n",
      " -0.6759\n",
      " -0.1919\n",
      " -0.2286\n",
      " -0.0303\n",
      "  0.1714\n",
      "  0.1594\n",
      " -0.4695\n",
      " -0.6225\n",
      " -0.1236\n",
      "  0.2783\n",
      " -0.3018\n",
      " -0.5332\n",
      " -0.0447\n",
      " -0.0008\n",
      " -1.0072\n",
      "  0.2587\n",
      " -0.5714\n",
      "  0.0866\n",
      " -0.1280\n",
      " -0.2923\n",
      "  0.3994\n",
      "  0.2270\n",
      " -0.6531\n",
      " -0.5116\n",
      " -0.7729\n",
      " -0.4830\n",
      " -0.6986\n",
      " -0.9577\n",
      " -0.1756\n",
      " -0.0762\n",
      " -0.2845\n",
      " -0.2863\n",
      " -0.1151\n",
      "  0.3883\n",
      "  0.2146\n",
      "  0.5005\n",
      " -0.9031\n",
      "  0.0455\n",
      "  0.1203\n",
      "  0.5130\n",
      " -0.4707\n",
      " -0.5761\n",
      " -0.8697\n",
      "  0.0801\n",
      " -0.1206\n",
      " -0.6808\n",
      " -0.9171\n",
      " -1.0587\n",
      "  0.2085\n",
      " -0.0622\n",
      " -0.7100\n",
      " -0.6466\n",
      " -0.6157\n",
      " -0.0759\n",
      "  0.0087\n",
      " -0.1423\n",
      " -0.3503\n",
      " -0.5688\n",
      " -0.5805\n",
      " -0.9721\n",
      "  0.3780\n",
      " -0.7756\n",
      " -0.4676\n",
      " -0.5012\n",
      " -1.4190\n",
      " -0.1673\n",
      "  0.0536\n",
      " -1.0313\n",
      " -0.2366\n",
      "  0.1967\n",
      " -0.7075\n",
      " -0.1403\n",
      " -0.1773\n",
      " -0.1173\n",
      " -0.0059\n",
      " -0.2203\n",
      " -0.2505\n",
      "  0.0470\n",
      " -0.3572\n",
      " -0.2922\n",
      " -0.0668\n",
      " -0.6184\n",
      " -0.7527\n",
      " -0.5252\n",
      " -0.1367\n",
      " -1.1305\n",
      " -0.0080\n",
      " -0.4461\n",
      " -0.2952\n",
      " -1.4265\n",
      " -0.2629\n",
      " -0.2712\n",
      " -0.0944\n",
      " -1.0156\n",
      "  0.6949\n",
      " -0.6321\n",
      "  0.3118\n",
      " -0.2962\n",
      "  0.0759\n",
      "  0.3812\n",
      " -0.8152\n",
      "  0.3926\n",
      " -0.4357\n",
      " -0.2442\n",
      " -0.2154\n",
      "  0.0036\n",
      " -0.4869\n",
      "  0.1697\n",
      " -0.0961\n",
      " -0.5588\n",
      " -0.7776\n",
      "  0.0172\n",
      "  0.4817\n",
      "  0.0410\n",
      " -0.9897\n",
      "  0.3795\n",
      "  0.6179\n",
      " -0.0795\n",
      " -0.7756\n",
      " -0.2649\n",
      " -0.3745\n",
      " -0.2087\n",
      " -0.9786\n",
      "  0.6219\n",
      " -0.4818\n",
      " -0.2906\n",
      " -0.7317\n",
      "  0.0246\n",
      " -0.3613\n",
      " -0.2554\n",
      "  0.0224\n",
      " -0.3791\n",
      " -0.4465\n",
      "  0.4888\n",
      " -0.3968\n",
      " -0.1203\n",
      "  0.0827\n",
      "  0.7397\n",
      " -0.4957\n",
      " -0.6644\n",
      " -0.0154\n",
      "  0.5333\n",
      " -0.7348\n",
      " -0.6032\n",
      "  0.1818\n",
      " -0.2201\n",
      "  0.2838\n",
      "  0.4601\n",
      " -0.4553\n",
      " -1.2831\n",
      "  0.2569\n",
      " -0.6065\n",
      " -0.2119\n",
      " -0.4905\n",
      " -0.9879\n",
      " -0.1567\n",
      " -0.4604\n",
      " -0.9364\n",
      " -0.6099\n",
      " -0.4713\n",
      " -0.3529\n",
      "  0.1983\n",
      " -0.1857\n",
      "  0.1490\n",
      "  0.2542\n",
      " -1.1679\n",
      " -1.4150\n",
      " -0.2324\n",
      "  0.2115\n",
      " -0.0395\n",
      "  0.0338\n",
      " -0.1693\n",
      " -1.0596\n",
      " -0.2738\n",
      "  0.0083\n",
      " -0.4085\n",
      " -0.4474\n",
      "  0.0374\n",
      " -0.2301\n",
      " -0.4091\n",
      "  0.1234\n",
      "  0.9722\n",
      " -0.3114\n",
      " -0.3875\n",
      " -0.9251\n",
      " -0.0389\n",
      "  0.6332\n",
      " -0.4819\n",
      "  0.1921\n",
      " -0.5389\n",
      "  0.0476\n",
      " -0.1454\n",
      " -0.8689\n",
      " -0.5498\n",
      "  0.0854\n",
      " -0.4784\n",
      " -0.3532\n",
      "  0.8780\n",
      "  0.2303\n",
      " -0.7728\n",
      "  0.1868\n",
      "  0.0516\n",
      " -1.1196\n",
      "  0.1318\n",
      "  0.0634\n",
      "  0.0616\n",
      "  0.3531\n",
      " -1.1461\n",
      " -0.8199\n",
      "  0.3777\n",
      " -0.3176\n",
      " -0.7756\n",
      "  0.9246\n",
      " -0.0213\n",
      " -0.0417\n",
      " -0.1595\n",
      "  0.7199\n",
      "  0.1124\n",
      " -0.7213\n",
      " -0.0329\n",
      " -0.5242\n",
      " -1.7407\n",
      " -0.5325\n",
      "  0.1570\n",
      " -0.5755\n",
      " -0.4872\n",
      " -0.0704\n",
      " -0.4175\n",
      " -0.1445\n",
      " -0.7468\n",
      " -0.3369\n",
      " -0.0445\n",
      " -0.0196\n",
      " -0.5948\n",
      " -1.3187\n",
      " -0.3056\n",
      "  0.0447\n",
      "  0.3152\n",
      " -0.1547\n",
      " -0.3572\n",
      "  1.2849\n",
      " -0.2344\n",
      " -0.0580\n",
      "  0.9264\n",
      " -0.4246\n",
      "  0.5584\n",
      " -0.1882\n",
      "  0.0188\n",
      " -0.0859\n",
      " -0.2451\n",
      " -0.7727\n",
      "  0.5925\n",
      " -0.0728\n",
      " -0.0418\n",
      " -0.5482\n",
      " -0.2340\n",
      "  0.0285\n",
      " -0.6871\n",
      " -0.5447\n",
      " -0.2356\n",
      " -0.4265\n",
      " -0.2496\n",
      "  0.0634\n",
      " -0.2692\n",
      "  0.3634\n",
      " -0.8388\n",
      " -0.4859\n",
      " -0.1949\n",
      "  0.1749\n",
      " -0.0453\n",
      " -0.2393\n",
      " -0.3373\n",
      " -0.6149\n",
      " -1.2003\n",
      "  0.2305\n",
      " -0.0260\n",
      " -1.6430\n",
      "  0.1462\n",
      " -0.3400\n",
      " -1.0930\n",
      "  0.3705\n",
      " -0.0425\n",
      " -0.9772\n",
      " -1.1306\n",
      " -0.3583\n",
      "  0.1418\n",
      "  0.1418\n",
      " -1.4414\n",
      " -0.3484\n",
      " -0.1137\n",
      " -0.0236\n",
      "  0.6262\n",
      " -0.2013\n",
      "  0.1535\n",
      " -0.3785\n",
      "  0.2697\n",
      " -0.6814\n",
      "  0.0038\n",
      " -0.3109\n",
      " -0.3929\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2314\n",
      " 0.2987\n",
      " 0.2840\n",
      " 0.2335\n",
      " 0.2621\n",
      " 0.2540\n",
      " 0.2565\n",
      " 0.2455\n",
      " 0.2250\n",
      " 0.2473\n",
      " 0.1974\n",
      " 0.2187\n",
      " 0.2185\n",
      " 0.2400\n",
      " 0.2635\n",
      " 0.2443\n",
      " 0.2232\n",
      " 0.3169\n",
      " 0.2822\n",
      " 0.2278\n",
      " 0.2299\n",
      " 0.2778\n",
      " 0.2130\n",
      " 0.2544\n",
      " 0.2467\n",
      " 0.2795\n",
      " 0.2382\n",
      " 0.2959\n",
      " 0.4218\n",
      " 0.2217\n",
      " 0.2681\n",
      " 0.1731\n",
      " 0.0387\n",
      " 0.2790\n",
      " 0.2559\n",
      " 0.2271\n",
      " 0.2293\n",
      " 0.2458\n",
      " 0.2582\n",
      " 0.2026\n",
      " 0.2423\n",
      " 0.3231\n",
      " 0.2022\n",
      " 0.2316\n",
      " 0.2770\n",
      " 0.1774\n",
      " 0.2191\n",
      " 0.2476\n",
      " 0.2458\n",
      " 0.2353\n",
      " 0.2815\n",
      " 0.2527\n",
      " 0.2606\n",
      " 0.2749\n",
      " 0.2705\n",
      " 0.2567\n",
      " 0.3056\n",
      " 0.2627\n",
      " 0.2231\n",
      " 0.0706\n",
      " 0.2229\n",
      " 0.2222\n",
      " 0.2198\n",
      " 0.2799\n",
      " 0.2595\n",
      " 0.2697\n",
      " 0.2502\n",
      " 0.2159\n",
      " 0.2658\n",
      " 0.2868\n",
      " 0.2843\n",
      " 0.3030\n",
      " 0.2714\n",
      " 0.2495\n",
      " 0.3081\n",
      " 0.3435\n",
      " 0.2124\n",
      " 0.2504\n",
      " 0.2305\n",
      " 0.2424\n",
      " 0.2115\n",
      " 0.1795\n",
      " 0.2134\n",
      " 0.2832\n",
      " 0.2598\n",
      " 0.3347\n",
      " 0.1975\n",
      " 0.2076\n",
      " 0.2839\n",
      " 0.1729\n",
      " 0.2393\n",
      " 0.2612\n",
      " 0.2479\n",
      " 0.2373\n",
      " 0.2457\n",
      " 0.2687\n",
      " 0.2350\n",
      " 0.2096\n",
      " 0.2380\n",
      " 0.2359\n",
      " 0.2115\n",
      " 0.2457\n",
      " 0.2653\n",
      " 0.2191\n",
      " 0.2999\n",
      " 0.2263\n",
      " 0.2315\n",
      " 0.2677\n",
      " 0.0468\n",
      " 0.3347\n",
      " 0.2757\n",
      " 0.2458\n",
      " 0.2230\n",
      " 0.1993\n",
      " 0.2430\n",
      " 0.2853\n",
      " 0.1851\n",
      " 0.2555\n",
      " 0.2919\n",
      " 0.1983\n",
      " 0.2238\n",
      " 0.2327\n",
      " 0.2973\n",
      " 0.2545\n",
      " 0.2618\n",
      " 0.2128\n",
      " 0.3055\n",
      " 0.2252\n",
      " 0.2522\n",
      " 0.2740\n",
      " 0.1943\n",
      " 0.2515\n",
      " 0.2533\n",
      " 0.2423\n",
      " 0.2357\n",
      " 0.2077\n",
      " 0.2119\n",
      " 0.2010\n",
      " 0.2979\n",
      " 0.2355\n",
      " 0.2665\n",
      " 0.2320\n",
      " 0.2592\n",
      " 0.3069\n",
      " 0.2700\n",
      " 0.2408\n",
      " 0.2449\n",
      " 0.3029\n",
      " 0.2599\n",
      " 0.2898\n",
      " 0.2589\n",
      " 0.3033\n",
      " 0.2851\n",
      " 0.3124\n",
      " 0.2772\n",
      " 0.2405\n",
      " 0.2260\n",
      " 0.2838\n",
      " 0.2482\n",
      " 0.2475\n",
      " 0.2751\n",
      " 0.2482\n",
      " 0.2543\n",
      " 0.2758\n",
      " 0.2841\n",
      " 0.2522\n",
      " 0.2605\n",
      " 0.2177\n",
      " 0.3322\n",
      " 0.2468\n",
      " 0.3368\n",
      " 0.2343\n",
      " 0.2506\n",
      " 0.2567\n",
      " 0.2529\n",
      " 0.2328\n",
      " 0.2866\n",
      " 0.2257\n",
      " 0.2809\n",
      " 0.2836\n",
      " 0.2347\n",
      " 0.2094\n",
      " 0.2491\n",
      " 0.2557\n",
      " 0.2196\n",
      " 0.3878\n",
      " 0.2152\n",
      " 0.2655\n",
      " 0.2547\n",
      " 0.2489\n",
      " 0.2551\n",
      " 0.2254\n",
      " 0.2454\n",
      " 0.2541\n",
      " 0.2595\n",
      " 0.2536\n",
      " 0.2158\n",
      " 0.2472\n",
      " 0.3510\n",
      " 0.2845\n",
      " 0.1905\n",
      " 0.2848\n",
      " 0.2232\n",
      " 0.2136\n",
      " 0.2216\n",
      " 0.2304\n",
      " 0.2294\n",
      " 0.2400\n",
      " 0.2707\n",
      " 0.2155\n",
      " 0.2500\n",
      " 0.2762\n",
      " 0.2717\n",
      " 0.2843\n",
      " 0.3607\n",
      " 0.2315\n",
      " 0.2486\n",
      " 0.2799\n",
      " 0.2333\n",
      " 0.3042\n",
      " 0.1042\n",
      " 0.2464\n",
      " 0.2237\n",
      " 0.2602\n",
      " 0.2889\n",
      " 0.1857\n",
      " 0.2283\n",
      " 0.2430\n",
      " 0.2148\n",
      " 0.3004\n",
      " 0.2436\n",
      " 0.3131\n",
      " 0.2495\n",
      " 0.2794\n",
      " 0.2464\n",
      " 0.2238\n",
      " 0.2153\n",
      " 0.2353\n",
      " 0.2404\n",
      " 0.2817\n",
      " 0.1959\n",
      " 0.1808\n",
      " 0.2823\n",
      " 0.2458\n",
      " 0.2673\n",
      " 0.3369\n",
      " 0.2432\n",
      " 0.2680\n",
      " 0.2278\n",
      " 0.2463\n",
      " 0.3345\n",
      " 0.2313\n",
      " 0.2525\n",
      " 0.2654\n",
      " 0.3008\n",
      " 0.2399\n",
      " 0.3051\n",
      " 0.2516\n",
      " 0.3248\n",
      " 0.2390\n",
      " 0.3305\n",
      " 0.2455\n",
      " 0.2531\n",
      " 0.3148\n",
      " 0.3115\n",
      " 0.2363\n",
      " 0.2369\n",
      " 0.2407\n",
      " 0.2648\n",
      " 0.2716\n",
      " 0.2769\n",
      " 0.2712\n",
      " 0.2581\n",
      " 0.2229\n",
      " 0.2244\n",
      " 0.2448\n",
      " 0.2815\n",
      " 0.2605\n",
      " 0.2122\n",
      " 0.2980\n",
      " 0.1966\n",
      " 0.2489\n",
      " 0.2886\n",
      " 0.1899\n",
      " 0.2618\n",
      " 0.2728\n",
      " 0.3209\n",
      " 0.1908\n",
      " 0.2063\n",
      " 0.2407\n",
      " 0.2233\n",
      " 0.2282\n",
      " 0.2403\n",
      " 0.2775\n",
      " 0.2226\n",
      " 0.2402\n",
      " 0.2744\n",
      " 0.2953\n",
      " 0.2999\n",
      " 0.2797\n",
      " 0.2246\n",
      " 0.2438\n",
      " 0.2263\n",
      " 0.2596\n",
      " 0.2107\n",
      " 0.2247\n",
      " 0.2546\n",
      " 0.2590\n",
      " 0.2162\n",
      " 0.2283\n",
      " 0.3110\n",
      " 0.2957\n",
      " 0.2200\n",
      " 0.2397\n",
      " 0.2528\n",
      " 0.2381\n",
      " 0.2419\n",
      " 0.2443\n",
      " 0.1355\n",
      " 0.2760\n",
      " 0.2222\n",
      " 0.2539\n",
      " 0.2404\n",
      " 0.2971\n",
      " 0.2559\n",
      " 0.2544\n",
      " 0.2754\n",
      " 0.2629\n",
      " 0.2521\n",
      " 0.2812\n",
      " 0.2775\n",
      " 0.2769\n",
      " 0.3803\n",
      " 0.2923\n",
      " 0.2822\n",
      " 0.2409\n",
      " 0.2365\n",
      " 0.2100\n",
      " 0.2524\n",
      " 0.2955\n",
      " 0.2780\n",
      " 0.2812\n",
      " 0.1977\n",
      " 0.2649\n",
      " 0.2295\n",
      " 0.2241\n",
      " 0.3207\n",
      " 0.2278\n",
      " 0.2503\n",
      " 0.2058\n",
      " 0.2445\n",
      " 0.2251\n",
      " 0.2273\n",
      " 0.2489\n",
      " 0.2626\n",
      " 0.2262\n",
      " 0.2618\n",
      " 0.2337\n",
      " 0.2659\n",
      " 0.2397\n",
      " 0.2770\n",
      " 0.3110\n",
      " 0.3290\n",
      " 0.2665\n",
      " 0.3075\n",
      " 0.3019\n",
      " 0.2690\n",
      " 0.2475\n",
      " 0.2552\n",
      " 0.2674\n",
      " 0.1781\n",
      " 0.2334\n",
      " 0.2660\n",
      " 0.2116\n",
      " 0.2148\n",
      " 0.2366\n",
      " 0.2604\n",
      " 0.2863\n",
      " 0.2660\n",
      " 0.2644\n",
      " 0.2735\n",
      " 0.2738\n",
      " 0.2527\n",
      " 0.0463\n",
      " 0.2690\n",
      " 0.2399\n",
      " 0.3215\n",
      " 0.2225\n",
      " 0.2223\n",
      " 0.2206\n",
      " 0.0974\n",
      " 0.2611\n",
      " 0.2565\n",
      " 0.2355\n",
      " 0.2575\n",
      " 0.2699\n",
      " 0.2304\n",
      " 0.2598\n",
      " 0.1939\n",
      " 0.2628\n",
      " 0.2865\n",
      " 0.3086\n",
      " 0.2560\n",
      " 0.2596\n",
      " 0.2393\n",
      " 0.2511\n",
      " 0.2666\n",
      " 0.1879\n",
      " 0.2561\n",
      " 0.1807\n",
      " 0.2559\n",
      " 0.2574\n",
      " 0.2174\n",
      " 0.2467\n",
      " 0.2668\n",
      " 0.3059\n",
      " 0.2373\n",
      " 0.2232\n",
      " 0.2796\n",
      " 0.2856\n",
      " 0.2832\n",
      " 0.2566\n",
      " 0.2524\n",
      " 0.2294\n",
      " 0.2236\n",
      " 0.2711\n",
      " 0.2131\n",
      " 0.2921\n",
      " 0.2622\n",
      " 0.2406\n",
      " 0.3657\n",
      " 0.2405\n",
      " 0.2067\n",
      " 0.3657\n",
      " 0.2667\n",
      " 0.2526\n",
      " 0.2478\n",
      " 0.2060\n",
      " 0.2601\n",
      " 0.2670\n",
      " 0.2145\n",
      " 0.2835\n",
      " 0.2207\n",
      " 0.2822\n",
      " 0.2284\n",
      " 0.2621\n",
      " 0.2816\n",
      " 0.2995\n",
      " 0.2073\n",
      " 0.3244\n",
      " 0.2185\n",
      " 0.2617\n",
      " 0.3580\n",
      " 0.2084\n",
      " 0.2330\n",
      " 0.2943\n",
      " 0.2173\n",
      " 0.2675\n",
      " 0.2782\n",
      " 0.2376\n",
      " 0.2730\n",
      " 0.2392\n",
      " 0.2711\n",
      " 0.2245\n",
      " 0.2205\n",
      " 0.2582\n",
      " 0.2577\n",
      " 0.0551\n",
      " 0.2490\n",
      " 0.2551\n",
      " 0.2639\n",
      " 0.2348\n",
      " 0.2260\n",
      " 0.2633\n",
      " 0.2597\n",
      " 0.2742\n",
      " 0.2772\n",
      " 0.3150\n",
      " 0.2293\n",
      " 0.2750\n",
      " 0.2287\n",
      " 0.2373\n",
      " 0.3602\n",
      " 0.2488\n",
      " 0.2723\n",
      " 0.3022\n",
      " 0.2994\n",
      " 0.1925\n",
      " 0.2605\n",
      " 0.2476\n",
      " 0.3429\n",
      " 0.2402\n",
      " 0.2460\n",
      " 0.2386\n",
      " 0.2141\n",
      " 0.4273\n",
      " 0.2265\n",
      " 0.2330\n",
      " 0.3025\n",
      " 0.3313\n",
      " 0.2289\n",
      " 0.2931\n",
      " 0.2376\n",
      " 0.2665\n",
      " 0.2896\n",
      " 0.2328\n",
      " 0.2223\n",
      " 0.2658\n",
      " 0.2119\n",
      " 0.2960\n",
      " 0.2681\n",
      " 0.3058\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-0.3164\n",
      "-0.4038\n",
      "-0.3448\n",
      "-0.2217\n",
      "-0.2534\n",
      "-0.2756\n",
      "-0.2367\n",
      "-0.2850\n",
      "-0.1634\n",
      "-0.2241\n",
      "-0.1137\n",
      "-0.1790\n",
      "-0.1716\n",
      "-0.3065\n",
      "-0.2167\n",
      "-0.1981\n",
      "-0.2012\n",
      "-0.4891\n",
      "-0.3395\n",
      "-0.2604\n",
      "-0.1650\n",
      "-0.3490\n",
      "-0.1821\n",
      "-0.2091\n",
      "-0.3429\n",
      "-0.3466\n",
      "-0.1489\n",
      "-0.3809\n",
      "-0.6925\n",
      "-0.1800\n",
      "-0.3827\n",
      "-0.0978\n",
      "-0.1980\n",
      "-0.2671\n",
      "-0.2854\n",
      "-0.1733\n",
      "-0.1779\n",
      "-0.2683\n",
      "-0.2719\n",
      "-0.1693\n",
      "-0.2932\n",
      "-0.3986\n",
      "-0.0863\n",
      "-0.1587\n",
      "-0.3843\n",
      "-0.0951\n",
      "-0.1857\n",
      "-0.2848\n",
      "-0.2169\n",
      "-0.1729\n",
      "-0.3509\n",
      "-0.2583\n",
      "-0.3100\n",
      "-0.2561\n",
      "-0.3901\n",
      "-0.2674\n",
      "-0.4794\n",
      "-0.2772\n",
      "-0.1773\n",
      "-0.4038\n",
      "-0.2039\n",
      "-0.2060\n",
      "-0.2109\n",
      "-0.3329\n",
      "-0.2892\n",
      "-0.2952\n",
      "-0.2511\n",
      "-0.2019\n",
      "-0.3308\n",
      "-0.2970\n",
      "-0.3521\n",
      "-0.5576\n",
      "-0.3019\n",
      "-0.1925\n",
      "-0.4300\n",
      "-0.4526\n",
      "-0.1780\n",
      "-0.2752\n",
      "-0.2088\n",
      "-0.2390\n",
      "-0.1750\n",
      "-0.2931\n",
      "-0.1785\n",
      "-0.2918\n",
      "-0.2719\n",
      "-0.4895\n",
      "-0.1492\n",
      "-0.1153\n",
      "-0.3346\n",
      "-0.1142\n",
      "-0.2034\n",
      "-0.2553\n",
      "-0.1931\n",
      "-0.1913\n",
      "-0.2479\n",
      "-0.2834\n",
      "-0.2225\n",
      "-0.1733\n",
      "-0.1753\n",
      "-0.2385\n",
      "-0.1352\n",
      "-0.1882\n",
      "-0.3083\n",
      "-0.1920\n",
      "-0.3989\n",
      "-0.1934\n",
      "-0.1941\n",
      "-0.3186\n",
      "-0.2712\n",
      "-0.4681\n",
      "-0.3804\n",
      "-0.2961\n",
      "-0.2820\n",
      "-0.1455\n",
      "-0.2611\n",
      "-0.3345\n",
      "-0.0746\n",
      "-0.2407\n",
      "-0.4603\n",
      "-0.1181\n",
      "-0.2281\n",
      "-0.2072\n",
      "-0.3165\n",
      "-0.2261\n",
      "-0.3241\n",
      "-0.1996\n",
      "-0.4046\n",
      "-0.2311\n",
      "-0.2954\n",
      "-0.2732\n",
      "-0.1644\n",
      "-0.2526\n",
      "-0.2449\n",
      "-0.2478\n",
      "-0.1946\n",
      "-0.1367\n",
      "-0.1820\n",
      "-0.1330\n",
      "-0.4462\n",
      "-0.2288\n",
      "-0.2780\n",
      "-0.1569\n",
      "-0.2267\n",
      "-0.3496\n",
      "-0.3217\n",
      "-0.2615\n",
      "-0.2517\n",
      "-0.4253\n",
      "-0.3011\n",
      "-0.4246\n",
      "-0.3204\n",
      "-0.3747\n",
      "-0.3500\n",
      "-0.4269\n",
      "-0.3279\n",
      "-0.2797\n",
      "-0.1874\n",
      "-0.4164\n",
      "-0.1729\n",
      "-0.2201\n",
      "-0.2627\n",
      "-0.2552\n",
      "-0.2783\n",
      "-0.3197\n",
      "-0.4163\n",
      "-0.2776\n",
      "-0.3102\n",
      "-0.1798\n",
      "-0.4645\n",
      "-0.2819\n",
      "-0.4490\n",
      "-0.2410\n",
      "-0.2630\n",
      "-0.4330\n",
      "-0.2120\n",
      "-0.2610\n",
      "-0.3960\n",
      "-0.1950\n",
      "-0.3504\n",
      "-0.3525\n",
      "-0.1890\n",
      "-0.1464\n",
      "-0.2908\n",
      "-0.3387\n",
      "-0.1852\n",
      "-0.4714\n",
      "-0.1519\n",
      "-0.3430\n",
      "-0.2677\n",
      "-0.2297\n",
      "-0.2827\n",
      "-0.2080\n",
      "-0.2334\n",
      "-0.2411\n",
      "-0.3039\n",
      "-0.2282\n",
      "-0.1602\n",
      "-0.1913\n",
      "-0.6297\n",
      "-0.4016\n",
      "-0.0982\n",
      "-0.3079\n",
      "-0.2455\n",
      "-0.1615\n",
      "-0.1235\n",
      "-0.2648\n",
      "-0.2212\n",
      "-0.2930\n",
      "-0.2512\n",
      "-0.2006\n",
      "-0.3031\n",
      "-0.3499\n",
      "-0.3898\n",
      "-0.3402\n",
      "-0.4320\n",
      "-0.1351\n",
      "-0.2823\n",
      "-0.3150\n",
      "-0.1519\n",
      "-0.3801\n",
      "-0.5191\n",
      "-0.2424\n",
      "-0.2071\n",
      "-0.2159\n",
      "-0.3860\n",
      "-0.1428\n",
      "-0.1588\n",
      "-0.2230\n",
      "-0.1066\n",
      "-0.4592\n",
      "-0.2500\n",
      "-0.3859\n",
      "-0.2740\n",
      "-0.3413\n",
      "-0.2660\n",
      "-0.1501\n",
      "-0.1411\n",
      "-0.2407\n",
      "-0.1931\n",
      "-0.2529\n",
      "-0.1382\n",
      "-0.0804\n",
      "-0.3510\n",
      "-0.2707\n",
      "-0.3368\n",
      "-0.4825\n",
      "-0.2479\n",
      "-0.2717\n",
      "-0.2175\n",
      "-0.2237\n",
      "-0.5100\n",
      "-0.1864\n",
      "-0.2758\n",
      "-0.2278\n",
      "-0.3517\n",
      "-0.2739\n",
      "-0.2996\n",
      "-0.2549\n",
      "-0.4815\n",
      "-0.1754\n",
      "-0.4353\n",
      "-0.1554\n",
      "-0.1906\n",
      "-0.4226\n",
      "-0.5226\n",
      "-0.1915\n",
      "-0.2309\n",
      "-0.2945\n",
      "-0.2273\n",
      "-0.3417\n",
      "-0.3102\n",
      "-0.2338\n",
      "-0.3037\n",
      "-0.1632\n",
      "-0.1611\n",
      "-0.2436\n",
      "-0.3262\n",
      "-0.2630\n",
      "-0.1551\n",
      "-0.4244\n",
      "-0.0481\n",
      "-0.2120\n",
      "-0.3916\n",
      "-0.1084\n",
      "-0.2282\n",
      "-0.3074\n",
      "-0.3401\n",
      "-0.1181\n",
      "-0.1247\n",
      "-0.2252\n",
      "-0.1775\n",
      "-0.2267\n",
      "-0.1851\n",
      "-0.3780\n",
      "-0.2014\n",
      "-0.2317\n",
      "-0.4125\n",
      "-0.3836\n",
      "-0.2764\n",
      "-0.3360\n",
      "-0.2362\n",
      "-0.2236\n",
      "-0.2158\n",
      "-0.3843\n",
      "-0.1978\n",
      "-0.2074\n",
      "-0.2760\n",
      "-0.3024\n",
      "-0.1961\n",
      "-0.2430\n",
      "-0.4337\n",
      "-0.3693\n",
      "-0.1830\n",
      "-0.2381\n",
      "-0.2596\n",
      "-0.3383\n",
      "-0.2536\n",
      "-0.2452\n",
      "-0.0118\n",
      "-0.3720\n",
      "-0.2088\n",
      "-0.2395\n",
      "-0.3005\n",
      "-0.3939\n",
      "-0.3524\n",
      "-0.2600\n",
      "-0.3625\n",
      "-0.1789\n",
      "-0.2538\n",
      "-0.3212\n",
      "-0.2983\n",
      "-0.3854\n",
      "-0.4597\n",
      "-0.3300\n",
      "-0.2980\n",
      "-0.3322\n",
      "-0.2047\n",
      "-0.1483\n",
      "-0.2889\n",
      "-0.2509\n",
      "-0.3130\n",
      "-0.2935\n",
      "-0.1101\n",
      "-0.3022\n",
      "-0.2002\n",
      "-0.1718\n",
      "-0.3384\n",
      "-0.2333\n",
      "-0.2584\n",
      "-0.2387\n",
      "-0.2806\n",
      "-0.2493\n",
      "-0.2202\n",
      "-0.2922\n",
      "-0.3241\n",
      "-0.2446\n",
      "-0.2952\n",
      "-0.2535\n",
      "-0.2673\n",
      "-0.1574\n",
      "-0.3207\n",
      "-0.3644\n",
      "-0.3887\n",
      "-0.3483\n",
      "-0.2661\n",
      "-0.3470\n",
      "-0.2706\n",
      "-0.1867\n",
      "-0.2794\n",
      "-0.3107\n",
      "-0.0750\n",
      "-0.2001\n",
      "-0.3846\n",
      "-0.1395\n",
      "-0.1879\n",
      "-0.2087\n",
      "-0.2423\n",
      "-0.2440\n",
      "-0.3608\n",
      "-0.2691\n",
      "-0.2550\n",
      "-0.3403\n",
      "-0.2840\n",
      "-0.2567\n",
      "-0.2727\n",
      "-0.3258\n",
      "-0.4685\n",
      "-0.1711\n",
      "-0.1628\n",
      "-0.1669\n",
      "-0.5232\n",
      "-0.2591\n",
      "-0.1975\n",
      "-0.2548\n",
      "-0.2102\n",
      "-0.3200\n",
      "-0.2325\n",
      "-0.2742\n",
      "-0.1245\n",
      "-0.2478\n",
      "-0.3261\n",
      "-0.3934\n",
      "-0.2486\n",
      "-0.2610\n",
      "-0.2621\n",
      "-0.2661\n",
      "-0.2768\n",
      "-0.1104\n",
      "-0.2797\n",
      "-0.0604\n",
      "-0.2927\n",
      "-0.2584\n",
      "-0.1955\n",
      "-0.2602\n",
      "-0.3563\n",
      "-0.3800\n",
      "-0.1102\n",
      "-0.1931\n",
      "-0.2944\n",
      "-0.3324\n",
      "-0.2749\n",
      "-0.2846\n",
      "-0.3633\n",
      "-0.2154\n",
      "-0.1753\n",
      "-0.3130\n",
      "-0.1876\n",
      "-0.3530\n",
      "-0.2678\n",
      "-0.2800\n",
      "-0.6067\n",
      "-0.2775\n",
      "-0.1330\n",
      "-0.5319\n",
      "-0.3188\n",
      "-0.2469\n",
      "-0.2072\n",
      "-0.1419\n",
      "-0.3293\n",
      "-0.2684\n",
      "-0.2258\n",
      "-0.3607\n",
      "-0.2026\n",
      "-0.2484\n",
      "-0.1822\n",
      "-0.2535\n",
      "-0.3699\n",
      "-0.3774\n",
      "-0.1933\n",
      "-0.4310\n",
      "-0.1784\n",
      "-0.2879\n",
      "-0.5176\n",
      "-0.1478\n",
      "-0.2347\n",
      "-0.3145\n",
      "-0.1293\n",
      "-0.2967\n",
      "-0.3669\n",
      "-0.2640\n",
      "-0.3127\n",
      "-0.2592\n",
      "-0.3308\n",
      "-0.1655\n",
      "-0.1945\n",
      "-0.2869\n",
      "-0.3797\n",
      "-0.2861\n",
      "-0.2076\n",
      "-0.2737\n",
      "-0.3058\n",
      "-0.2012\n",
      "-0.1832\n",
      "-0.2991\n",
      "-0.2744\n",
      "-0.3735\n",
      "-0.3081\n",
      "-0.4350\n",
      "-0.1886\n",
      "-0.2937\n",
      "-0.2260\n",
      "-0.2230\n",
      "-0.4161\n",
      "-0.2484\n",
      "-0.2887\n",
      "-0.4677\n",
      "-0.3764\n",
      "-0.1028\n",
      "-0.2565\n",
      "-0.2621\n",
      "-0.4320\n",
      "-0.2553\n",
      "-0.1762\n",
      "-0.2263\n",
      "-0.2677\n",
      "-0.6746\n",
      "-0.2050\n",
      "-0.2586\n",
      "-0.3521\n",
      "-0.4346\n",
      "-0.2220\n",
      "-0.3528\n",
      "-0.1948\n",
      "-0.3046\n",
      "-0.4682\n",
      "-0.2525\n",
      "-0.1534\n",
      "-0.2513\n",
      "-0.1837\n",
      "-0.3968\n",
      "-0.3517\n",
      "-0.4096\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -4.7228e-03  6.1350e-03  9.2539e-03\n",
      " -9.5013e-04 -1.2995e-02 -9.7248e-03\n",
      " -1.1794e-02 -1.9362e-02 -1.6484e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  3.1288e-02  1.9738e-02  3.5423e-02\n",
      "  1.1064e-02 -7.8965e-03  2.7237e-02\n",
      " -3.0856e-03  7.9598e-03  8.3558e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  1.7003e-02  8.9700e-03  8.6330e-03\n",
      " -8.4372e-03 -1.7460e-02 -2.1673e-02\n",
      " -4.5785e-04 -1.2148e-02 -1.4268e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,509,.,.) = \n",
      " -1.4422e-02 -2.2441e-02 -2.2388e-02\n",
      "  1.1210e-02  3.2948e-03 -2.4308e-03\n",
      " -5.0929e-03 -5.1824e-03 -1.0678e-02\n",
      "\n",
      "( 0 ,510,.,.) = \n",
      " -1.9068e-02 -1.8938e-02 -2.4867e-02\n",
      " -6.0156e-03 -8.2365e-03 -1.3951e-02\n",
      " -1.1423e-02 -8.9651e-03 -1.3677e-02\n",
      "\n",
      "( 0 ,511,.,.) = \n",
      " -5.0744e-03 -9.5685e-03 -5.1837e-02\n",
      " -7.4568e-03 -8.1368e-03 -4.6485e-02\n",
      "  5.0472e-03  1.2508e-02 -1.4355e-03\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -2.0506e-02 -2.4464e-02 -3.3749e-02\n",
      " -4.1953e-02 -2.2862e-02 -2.4604e-02\n",
      " -2.4789e-02 -1.8484e-02 -2.7139e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -3.1903e-02 -1.6090e-02 -2.6388e-02\n",
      " -1.3948e-02 -8.8845e-03 -4.4012e-03\n",
      " -2.4939e-02 -1.7051e-02 -2.4359e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -1.2463e-03 -1.2292e-02  1.5639e-02\n",
      "  2.2680e-04 -1.5787e-02  3.9815e-03\n",
      "  1.3943e-02  6.1443e-03  6.9358e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,509,.,.) = \n",
      " -2.8947e-02 -3.4940e-02 -3.6833e-02\n",
      " -3.1172e-02 -3.9282e-02 -3.5906e-02\n",
      " -3.1699e-02 -2.6338e-02 -3.0023e-02\n",
      "\n",
      "( 1 ,510,.,.) = \n",
      " -1.1244e-02 -9.2203e-03 -1.3889e-02\n",
      " -1.1941e-02 -7.1538e-03 -5.6546e-03\n",
      "  1.0463e-02  1.2292e-02  2.0812e-02\n",
      "\n",
      "( 1 ,511,.,.) = \n",
      " -8.3597e-03  1.4503e-02  2.2285e-02\n",
      "  1.5133e-03  1.1354e-03  1.5807e-02\n",
      "  1.7326e-02  6.3278e-03  3.9437e-03\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -6.2493e-03  2.0414e-03  3.9562e-03\n",
      "  1.4905e-04 -6.3406e-04  6.8022e-03\n",
      " -4.8896e-03  5.2874e-03  4.9005e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  1.7870e-02  2.0390e-03  6.5005e-04\n",
      "  7.0086e-03 -5.4171e-04 -9.6794e-03\n",
      " -6.5785e-03 -9.2749e-03  5.8289e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  5.9966e-03  1.5847e-02  2.2420e-02\n",
      " -4.3756e-03  1.3127e-03  2.4885e-02\n",
      " -4.9599e-03 -2.6868e-02 -1.9085e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,509,.,.) = \n",
      "  3.9997e-02  2.6723e-02  3.1556e-02\n",
      " -3.2331e-03  8.8392e-03  1.7467e-02\n",
      " -1.6900e-02 -6.7076e-03 -1.2010e-02\n",
      "\n",
      "( 2 ,510,.,.) = \n",
      " -2.3467e-02 -1.6482e-02 -9.5478e-03\n",
      "  3.4332e-02  9.8374e-03  7.4398e-03\n",
      "  3.6922e-02  2.4428e-02  1.1186e-02\n",
      "\n",
      "( 2 ,511,.,.) = \n",
      " -4.2761e-03 -1.7345e-02  7.4758e-03\n",
      "  1.0596e-02 -2.0053e-02 -1.5464e-02\n",
      "  1.2240e-02  2.0988e-02  3.8752e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      " -7.3432e-03 -1.6985e-02 -1.6869e-02\n",
      " -1.5340e-02 -2.3045e-02 -2.2234e-02\n",
      " -1.3874e-02 -1.0892e-02 -1.8434e-02\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      " -1.1183e-02 -1.6038e-02 -2.0036e-02\n",
      "  1.6000e-02  1.6065e-02 -6.1795e-03\n",
      "  3.5738e-04  3.3417e-03 -1.0312e-02\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      " -2.9074e-02 -7.0707e-03 -1.4714e-02\n",
      " -3.1914e-02 -1.1593e-02 -2.9175e-02\n",
      " -3.4496e-02 -1.4085e-02 -2.5201e-02\n",
      "    ... \n",
      "\n",
      "(509,509,.,.) = \n",
      "  1.4782e-02 -3.4332e-03  2.2488e-02\n",
      "  2.0753e-02  6.6188e-03  2.0073e-02\n",
      "  2.5061e-02  1.3297e-02  1.4716e-02\n",
      "\n",
      "(509,510,.,.) = \n",
      " -1.5265e-02 -3.9711e-02 -1.8336e-02\n",
      " -1.9255e-02 -3.5705e-02 -1.0139e-02\n",
      " -2.0862e-03 -1.7968e-02 -1.1310e-02\n",
      "\n",
      "(509,511,.,.) = \n",
      " -1.5535e-02 -3.3631e-02 -2.7480e-02\n",
      " -3.7403e-03 -1.3513e-02 -1.8050e-03\n",
      " -8.8378e-03 -1.8366e-02  4.1493e-03\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -1.8019e-02  2.7220e-03 -7.2262e-03\n",
      " -8.4631e-03  6.9264e-03 -1.5196e-02\n",
      " -1.7576e-02 -4.5224e-03 -1.5965e-02\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      " -2.4200e-02 -3.5796e-02 -2.9869e-02\n",
      " -3.2335e-02 -2.6403e-02 -1.7582e-02\n",
      " -3.2528e-02 -3.5310e-02 -2.8377e-02\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      " -8.0535e-04 -1.4656e-02  8.7835e-03\n",
      "  5.8319e-03  1.8556e-02  2.3774e-02\n",
      " -2.5342e-02  2.9607e-03 -1.0286e-02\n",
      "    ... \n",
      "\n",
      "(510,509,.,.) = \n",
      " -1.3044e-02 -1.2902e-02 -1.1309e-02\n",
      " -1.8101e-02 -5.1900e-03 -6.9020e-03\n",
      " -2.4770e-02 -2.0816e-02 -1.5036e-02\n",
      "\n",
      "(510,510,.,.) = \n",
      "  2.8295e-02  4.0159e-02  7.1700e-03\n",
      "  1.4436e-02  2.8263e-02  6.6057e-03\n",
      " -6.8270e-03 -4.7419e-03 -5.8275e-03\n",
      "\n",
      "(510,511,.,.) = \n",
      " -8.7142e-03  3.1539e-02  2.8079e-02\n",
      " -3.1702e-02 -4.1517e-03  2.0790e-02\n",
      " -9.6525e-03 -6.0263e-03 -4.0770e-03\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      "  3.6268e-02  3.7963e-02  4.5701e-02\n",
      "  1.1818e-02 -9.0865e-03  1.5115e-02\n",
      "  4.0161e-02  3.4044e-02  2.4838e-02\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      " -2.1452e-03  1.6109e-02  1.3820e-02\n",
      "  1.8188e-02  1.8413e-02  2.9984e-02\n",
      "  2.4227e-02  2.9046e-02  2.5058e-02\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      " -2.2492e-02 -7.8466e-03 -3.3112e-02\n",
      " -4.9156e-02 -3.7869e-02 -4.5866e-02\n",
      " -3.5955e-02 -3.3286e-02 -3.6101e-02\n",
      "    ... \n",
      "\n",
      "(511,509,.,.) = \n",
      "  1.4127e-02  1.1208e-02  1.2181e-02\n",
      "  2.8429e-02  2.6581e-02  2.3278e-02\n",
      "  2.9209e-02  1.9824e-02  1.9819e-02\n",
      "\n",
      "(511,510,.,.) = \n",
      " -6.2235e-03 -5.5648e-03 -1.3373e-02\n",
      " -9.6088e-04 -5.1643e-03  3.5494e-03\n",
      " -8.8821e-03 -3.2781e-03  3.2117e-03\n",
      "\n",
      "(511,511,.,.) = \n",
      " -1.3250e-02 -4.2729e-03 -2.2231e-02\n",
      "  2.6518e-03 -1.0660e-03 -7.1279e-03\n",
      "  2.3389e-02  6.8477e-03 -3.7209e-03\n",
      "[torch.FloatTensor of size 512x512x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      "  0.0271\n",
      " -0.3963\n",
      "  0.1806\n",
      " -0.0475\n",
      "  0.4488\n",
      "  0.0424\n",
      " -0.0842\n",
      " -0.2452\n",
      "  0.0934\n",
      "  0.4198\n",
      "  0.4325\n",
      "  0.0263\n",
      "  0.1954\n",
      " -0.3022\n",
      "  0.3771\n",
      " -0.5106\n",
      " -0.3854\n",
      "  0.2156\n",
      " -0.5632\n",
      " -0.2149\n",
      " -0.1317\n",
      " -0.4651\n",
      "  0.0863\n",
      " -0.1754\n",
      " -0.2587\n",
      "  0.2444\n",
      " -0.2781\n",
      " -0.4891\n",
      "  0.3103\n",
      " -0.1675\n",
      "  0.3047\n",
      " -0.4228\n",
      " -0.2758\n",
      " -0.0431\n",
      "  0.1057\n",
      " -0.3293\n",
      " -0.0166\n",
      "  0.4815\n",
      " -0.4066\n",
      " -0.3783\n",
      "  0.1278\n",
      " -0.0822\n",
      " -0.4416\n",
      "  0.1509\n",
      " -0.4290\n",
      " -0.0019\n",
      " -0.3835\n",
      "  0.1407\n",
      " -0.7013\n",
      " -0.0224\n",
      " -0.2748\n",
      " -0.3161\n",
      "  0.2309\n",
      " -0.6917\n",
      " -0.1314\n",
      " -0.4785\n",
      " -0.4061\n",
      " -1.0046\n",
      "  0.0289\n",
      " -0.0956\n",
      "  0.4445\n",
      " -0.3391\n",
      " -0.5107\n",
      " -0.1765\n",
      " -0.2109\n",
      "  0.1746\n",
      "  0.0310\n",
      "  0.0452\n",
      "  0.7924\n",
      "  0.2573\n",
      " -0.3954\n",
      "  0.0868\n",
      "  0.3277\n",
      " -0.1369\n",
      " -0.0417\n",
      " -0.2943\n",
      " -0.1280\n",
      "  0.1169\n",
      " -0.7433\n",
      " -0.5428\n",
      "  0.0956\n",
      "  0.0124\n",
      " -0.5000\n",
      " -0.2888\n",
      "  0.0102\n",
      " -0.4184\n",
      "  0.1309\n",
      " -0.3633\n",
      "  0.0017\n",
      "  0.2413\n",
      " -0.6462\n",
      " -0.1800\n",
      " -0.0112\n",
      "  0.4989\n",
      " -0.2588\n",
      "  0.0271\n",
      " -0.3507\n",
      "  0.0912\n",
      " -0.1856\n",
      " -0.1155\n",
      "  0.1010\n",
      " -0.1091\n",
      "  0.2038\n",
      " -0.8839\n",
      " -0.3902\n",
      " -0.1427\n",
      " -0.1696\n",
      " -0.1441\n",
      " -0.2477\n",
      " -0.5415\n",
      " -0.4748\n",
      " -0.0623\n",
      "  0.3627\n",
      " -0.2222\n",
      "  0.0201\n",
      "  0.0004\n",
      " -0.2382\n",
      " -0.1459\n",
      " -0.4652\n",
      " -0.2882\n",
      " -0.4386\n",
      " -0.2148\n",
      " -0.3435\n",
      " -0.1939\n",
      " -0.6985\n",
      "  0.0343\n",
      " -0.6341\n",
      " -0.1118\n",
      "  0.6073\n",
      " -0.2042\n",
      " -0.1164\n",
      " -0.0204\n",
      " -0.0468\n",
      "  0.2913\n",
      " -0.1160\n",
      "  0.2862\n",
      "  0.4315\n",
      "  0.1554\n",
      "  0.1125\n",
      " -0.5827\n",
      "  0.0560\n",
      " -0.0244\n",
      " -0.9385\n",
      "  0.0182\n",
      " -0.0767\n",
      "  0.1663\n",
      "  0.0615\n",
      "  0.1846\n",
      " -0.5197\n",
      "  0.2767\n",
      " -0.0352\n",
      " -0.4616\n",
      " -0.6423\n",
      " -0.0900\n",
      " -0.6816\n",
      " -0.1326\n",
      " -0.3667\n",
      "  0.2322\n",
      " -0.4154\n",
      "  0.1685\n",
      "  0.1097\n",
      "  0.5097\n",
      " -0.4185\n",
      "  0.1082\n",
      " -0.0223\n",
      " -0.4389\n",
      "  0.5068\n",
      "  0.3829\n",
      " -0.0880\n",
      "  0.1368\n",
      " -0.5957\n",
      " -0.1788\n",
      " -0.1917\n",
      " -0.4561\n",
      " -0.2661\n",
      "  0.0538\n",
      "  0.0827\n",
      " -0.3598\n",
      " -0.1296\n",
      "  0.1518\n",
      "  0.2371\n",
      " -0.7311\n",
      " -0.0737\n",
      "  0.3848\n",
      " -0.0319\n",
      "  0.0754\n",
      "  0.6218\n",
      " -0.1576\n",
      " -0.0003\n",
      " -0.3811\n",
      "  0.0878\n",
      " -0.1191\n",
      "  0.1558\n",
      " -0.1148\n",
      " -0.5065\n",
      " -0.2195\n",
      "  0.2460\n",
      " -0.0654\n",
      "  0.1076\n",
      " -0.1909\n",
      " -0.1339\n",
      " -0.4271\n",
      " -0.5942\n",
      "  0.1685\n",
      " -0.2231\n",
      " -0.0562\n",
      " -0.1086\n",
      " -0.5036\n",
      "  0.0902\n",
      "  0.0853\n",
      "  0.0856\n",
      " -0.4025\n",
      "  0.1038\n",
      " -0.4726\n",
      " -0.6602\n",
      " -0.2397\n",
      " -0.2227\n",
      " -0.1917\n",
      "  0.0955\n",
      " -0.5669\n",
      "  0.0427\n",
      " -0.2977\n",
      " -0.4349\n",
      " -0.3100\n",
      "  0.1069\n",
      "  0.0735\n",
      "  1.0553\n",
      "  0.2221\n",
      " -0.5242\n",
      "  0.2754\n",
      " -0.7561\n",
      " -0.4564\n",
      "  0.0539\n",
      " -0.2600\n",
      "  0.3413\n",
      " -0.2055\n",
      " -0.1719\n",
      " -0.3248\n",
      " -0.1067\n",
      " -0.3081\n",
      "  0.1164\n",
      " -0.2354\n",
      " -0.0426\n",
      " -0.2859\n",
      " -0.1005\n",
      " -0.1703\n",
      " -0.0125\n",
      " -0.2140\n",
      " -0.4065\n",
      " -0.3131\n",
      "  0.2735\n",
      "  0.1437\n",
      " -0.3681\n",
      " -0.0914\n",
      "  0.0363\n",
      " -0.4026\n",
      " -0.3827\n",
      " -0.0114\n",
      " -0.2251\n",
      "  0.0885\n",
      "  0.2485\n",
      " -0.0672\n",
      " -0.1257\n",
      " -0.1937\n",
      " -0.1580\n",
      "  0.1971\n",
      "  0.1429\n",
      " -0.5371\n",
      " -0.1595\n",
      " -0.0254\n",
      "  0.1378\n",
      " -0.5123\n",
      " -0.2267\n",
      " -0.2168\n",
      "  0.4047\n",
      " -0.1640\n",
      " -0.5667\n",
      " -0.0415\n",
      " -0.0366\n",
      " -0.0390\n",
      " -0.9336\n",
      "  0.1119\n",
      "  0.1111\n",
      " -0.5990\n",
      " -0.2925\n",
      " -0.5240\n",
      " -0.2141\n",
      "  0.0593\n",
      "  0.0331\n",
      " -0.2995\n",
      " -0.2059\n",
      " -0.0120\n",
      " -0.5647\n",
      "  0.1947\n",
      " -0.0508\n",
      " -0.4043\n",
      " -0.4640\n",
      " -0.1435\n",
      " -0.2158\n",
      " -0.2227\n",
      " -0.1320\n",
      " -0.5299\n",
      " -0.4640\n",
      "  0.1007\n",
      " -0.2859\n",
      "  0.1195\n",
      "  0.0538\n",
      " -0.3812\n",
      " -0.1381\n",
      "  0.3738\n",
      " -0.3201\n",
      " -0.1072\n",
      " -0.2429\n",
      " -0.4193\n",
      " -0.0024\n",
      "  0.4533\n",
      "  0.4423\n",
      " -0.3668\n",
      "  0.0722\n",
      " -0.0716\n",
      " -0.1596\n",
      " -0.2369\n",
      " -0.2690\n",
      " -0.0834\n",
      " -0.0712\n",
      "  0.1968\n",
      " -0.5609\n",
      " -0.4806\n",
      " -0.2858\n",
      "  0.0001\n",
      " -0.2062\n",
      " -0.2987\n",
      " -0.1758\n",
      " -0.2816\n",
      " -0.3186\n",
      "  0.1155\n",
      " -0.2751\n",
      "  1.4155\n",
      " -0.4825\n",
      " -0.0188\n",
      " -0.0701\n",
      " -0.2447\n",
      " -0.0034\n",
      " -0.1004\n",
      " -0.1354\n",
      " -0.1803\n",
      " -0.1339\n",
      " -0.6877\n",
      " -0.7919\n",
      "  0.1967\n",
      " -0.0181\n",
      "  0.2006\n",
      " -0.0016\n",
      " -0.0908\n",
      "  0.3079\n",
      " -0.1309\n",
      "  0.0539\n",
      " -0.1009\n",
      " -0.4427\n",
      " -0.2439\n",
      " -0.1307\n",
      "  0.0601\n",
      "  0.2478\n",
      "  0.1851\n",
      "  0.1677\n",
      " -0.3500\n",
      " -0.5297\n",
      "  0.0038\n",
      " -0.4085\n",
      "  0.3931\n",
      "  0.1221\n",
      "  0.3980\n",
      " -0.1117\n",
      "  0.1383\n",
      "  0.1395\n",
      " -0.5845\n",
      " -0.4513\n",
      " -0.2319\n",
      " -0.1803\n",
      " -0.9178\n",
      " -0.3520\n",
      " -0.1692\n",
      " -0.4088\n",
      " -0.7895\n",
      " -0.2307\n",
      " -0.6089\n",
      "  0.4122\n",
      " -0.0880\n",
      " -0.1733\n",
      " -0.5382\n",
      " -0.3085\n",
      " -0.0475\n",
      "  0.5769\n",
      " -0.1241\n",
      " -0.0623\n",
      " -0.2768\n",
      " -0.4012\n",
      "  0.5308\n",
      " -0.2443\n",
      "  0.6646\n",
      "  0.0996\n",
      " -0.2754\n",
      "  0.3566\n",
      " -0.1830\n",
      " -0.4196\n",
      " -0.1193\n",
      " -0.2481\n",
      " -0.0871\n",
      "  0.1286\n",
      " -0.6577\n",
      " -0.1561\n",
      " -0.2394\n",
      " -0.1054\n",
      " -0.0306\n",
      " -0.1992\n",
      "  0.0004\n",
      " -0.1526\n",
      " -0.6579\n",
      " -0.0190\n",
      " -0.3777\n",
      " -0.0176\n",
      "  0.3833\n",
      "  0.2433\n",
      " -0.4195\n",
      " -0.0358\n",
      " -0.5214\n",
      "  0.0093\n",
      " -0.6101\n",
      "  0.3379\n",
      " -0.2756\n",
      " -0.1662\n",
      " -0.3785\n",
      " -0.3454\n",
      " -0.0015\n",
      " -0.0230\n",
      "  0.3129\n",
      " -0.0941\n",
      " -0.2924\n",
      " -0.3709\n",
      " -0.1877\n",
      " -0.7100\n",
      " -0.2753\n",
      " -0.7890\n",
      "  0.3509\n",
      " -0.2541\n",
      "  0.1024\n",
      " -0.2910\n",
      " -0.4584\n",
      " -0.1363\n",
      " -1.1333\n",
      "  0.1471\n",
      "  0.0475\n",
      " -0.5741\n",
      "  0.2899\n",
      "  0.4495\n",
      " -0.4466\n",
      "  0.0358\n",
      "  0.2905\n",
      " -0.1611\n",
      "  0.5779\n",
      " -0.5376\n",
      " -0.2015\n",
      " -0.0711\n",
      " -0.3093\n",
      "  0.0446\n",
      " -0.5169\n",
      " -0.3116\n",
      " -0.1540\n",
      " -0.3602\n",
      " -0.2172\n",
      " -0.1133\n",
      " -0.2895\n",
      " -0.2114\n",
      "  0.5283\n",
      " -0.3770\n",
      " -0.5478\n",
      "  0.0463\n",
      "  0.2642\n",
      " -0.1786\n",
      "  0.3091\n",
      " -0.2262\n",
      " -0.4705\n",
      "  0.1922\n",
      " -0.1270\n",
      " -0.4272\n",
      " -0.3818\n",
      "  0.0974\n",
      " -0.0400\n",
      "  0.2373\n",
      "  0.1196\n",
      " -0.5528\n",
      " -0.3807\n",
      "  0.0653\n",
      " -0.1779\n",
      "  0.4067\n",
      "  0.1074\n",
      " -0.4097\n",
      " -0.2322\n",
      " -0.4405\n",
      "  0.0284\n",
      "  0.3630\n",
      " -0.3323\n",
      "  0.0251\n",
      "  0.0304\n",
      "  0.0674\n",
      " -0.7945\n",
      "  0.1982\n",
      " -0.1536\n",
      " -0.1847\n",
      " -0.0046\n",
      " -0.3246\n",
      "  0.1017\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1921\n",
      " 0.2466\n",
      " 0.3574\n",
      " 0.2407\n",
      " 0.2554\n",
      " 0.2478\n",
      " 0.2754\n",
      " 0.2708\n",
      " 0.2406\n",
      " 0.2687\n",
      " 0.2686\n",
      " 0.3254\n",
      " 0.2164\n",
      " 0.2679\n",
      " 0.2361\n",
      " 0.2363\n",
      " 0.2696\n",
      " 0.2938\n",
      " 0.3226\n",
      " 0.2512\n",
      " 0.2167\n",
      " 0.2854\n",
      " 0.2361\n",
      " 0.2740\n",
      " 0.3323\n",
      " 0.2223\n",
      " 0.3698\n",
      " 0.3456\n",
      " 0.3381\n",
      " 0.2602\n",
      " 0.3108\n",
      " 0.2934\n",
      " 0.2745\n",
      " 0.2601\n",
      " 0.3445\n",
      " 0.2439\n",
      " 0.2607\n",
      " 0.2587\n",
      " 0.3028\n",
      " 0.2478\n",
      " 0.3365\n",
      " 0.3043\n",
      " 0.3076\n",
      " 0.3562\n",
      " 0.3599\n",
      " 0.2885\n",
      " 0.3165\n",
      " 0.2436\n",
      " 0.2451\n",
      " 0.2380\n",
      " 0.3060\n",
      " 0.2267\n",
      " 0.3204\n",
      " 0.3376\n",
      " 0.2486\n",
      " 0.2455\n",
      " 0.2465\n",
      " 0.3219\n",
      " 0.2828\n",
      " 0.3178\n",
      " 0.2936\n",
      " 0.3067\n",
      " 0.2353\n",
      " 0.2394\n",
      " 0.2618\n",
      " 0.3225\n",
      " 0.2870\n",
      " 0.2444\n",
      " 0.2876\n",
      " 0.3056\n",
      " 0.3114\n",
      " 0.2968\n",
      " 0.2147\n",
      " 0.2838\n",
      " 0.2678\n",
      " 0.3059\n",
      " 0.3138\n",
      " 0.3602\n",
      " 0.2800\n",
      " 0.2686\n",
      " 0.2958\n",
      " 0.3363\n",
      " 0.2986\n",
      " 0.2657\n",
      " 0.2802\n",
      " 0.2850\n",
      " 0.3780\n",
      " 0.3271\n",
      " 0.3290\n",
      " 0.3063\n",
      " 0.3281\n",
      " 0.3160\n",
      " 0.2546\n",
      " 0.2803\n",
      " 0.2505\n",
      " 0.3360\n",
      " 0.2673\n",
      " 0.2568\n",
      " 0.3231\n",
      " 0.3515\n",
      " 0.3098\n",
      " 0.2318\n",
      " 0.2527\n",
      " 0.3387\n",
      " 0.3041\n",
      " 0.2837\n",
      " 0.3480\n",
      " 0.2484\n",
      " 0.2584\n",
      " 0.4321\n",
      " 0.2548\n",
      " 0.3629\n",
      " 0.3005\n",
      " 0.3259\n",
      " 0.2699\n",
      " 0.3054\n",
      " 0.2665\n",
      " 0.3267\n",
      " 0.2455\n",
      " 0.1756\n",
      " 0.3386\n",
      " 0.2868\n",
      " 0.2586\n",
      " 0.2896\n",
      " 0.2479\n",
      " 0.2504\n",
      " 0.2761\n",
      " 0.3897\n",
      " 0.3391\n",
      " 0.2798\n",
      " 0.2595\n",
      " 0.3419\n",
      " 0.3306\n",
      " 0.3472\n",
      " 0.2547\n",
      " 0.3660\n",
      " 0.1938\n",
      " 0.2173\n",
      " 0.2653\n",
      " 0.2209\n",
      " 0.2458\n",
      " 0.2760\n",
      " 0.3130\n",
      " 0.2506\n",
      " 0.2524\n",
      " 0.2117\n",
      " 0.2522\n",
      " 0.2923\n",
      " 0.2902\n",
      " 0.2456\n",
      " 0.2689\n",
      " 0.3232\n",
      " 0.2733\n",
      " 0.2579\n",
      " 0.2880\n",
      " 0.3076\n",
      " 0.2475\n",
      " 0.2892\n",
      " 0.2924\n",
      " 0.3129\n",
      " 0.3320\n",
      " 0.3210\n",
      " 0.2944\n",
      " 0.2754\n",
      " 0.2467\n",
      " 0.3121\n",
      " 0.2452\n",
      " 0.3278\n",
      " 0.3415\n",
      " 0.3109\n",
      " 0.3080\n",
      " 0.3644\n",
      " 0.2976\n",
      " 0.2850\n",
      " 0.3469\n",
      " 0.2903\n",
      " 0.3346\n",
      " 0.3772\n",
      " 0.2630\n",
      " 0.3604\n",
      " 0.3813\n",
      " 0.1696\n",
      " 0.2635\n",
      " 0.1987\n",
      " 0.2465\n",
      " 0.2997\n",
      " 0.3196\n",
      " 0.2729\n",
      " 0.2339\n",
      " 0.2868\n",
      " 0.3033\n",
      " 0.2602\n",
      " 0.2990\n",
      " 0.2851\n",
      " 0.3069\n",
      " 0.3127\n",
      " 0.2750\n",
      " 0.3023\n",
      " 0.3082\n",
      " 0.3015\n",
      " 0.3491\n",
      " 0.3003\n",
      " 0.3379\n",
      " 0.2161\n",
      " 0.3147\n",
      " 0.2432\n",
      " 0.3223\n",
      " 0.2621\n",
      " 0.2367\n",
      " 0.2149\n",
      " 0.2762\n",
      " 0.2338\n",
      " 0.2395\n",
      " 0.2357\n",
      " 0.2455\n",
      " 0.3180\n",
      " 0.2761\n",
      " 0.2529\n",
      " 0.2662\n",
      " 0.3337\n",
      " 0.3392\n",
      " 0.2699\n",
      " 0.2471\n",
      " 0.2461\n",
      " 0.3539\n",
      " 0.3039\n",
      " 0.4281\n",
      " 0.2420\n",
      " 0.2277\n",
      " 0.3541\n",
      " 0.2275\n",
      " 0.3291\n",
      " 0.2648\n",
      " 0.3687\n",
      " 0.3023\n",
      " 0.2628\n",
      " 0.2659\n",
      " 0.3313\n",
      " 0.2467\n",
      " 0.2630\n",
      " 0.3280\n",
      " 0.2007\n",
      " 0.2726\n",
      " 0.2464\n",
      " 0.3713\n",
      " 0.3203\n",
      " 0.3579\n",
      " 0.2539\n",
      " 0.2996\n",
      " 0.3918\n",
      " 0.2481\n",
      " 0.2472\n",
      " 0.2738\n",
      " 0.2038\n",
      " 0.3153\n",
      " 0.2418\n",
      " 0.2727\n",
      " 0.2390\n",
      " 0.2642\n",
      " 0.3141\n",
      " 0.2649\n",
      " 0.3546\n",
      " 0.2712\n",
      " 0.3280\n",
      " 0.3223\n",
      " 0.3190\n",
      " 0.2797\n",
      " 0.2659\n",
      " 0.4173\n",
      " 0.2606\n",
      " 0.3077\n",
      " 0.3312\n",
      " 0.2489\n",
      " 0.3118\n",
      " 0.3751\n",
      " 0.3748\n",
      " 0.3600\n",
      " 0.3081\n",
      " 0.3768\n",
      " 0.2564\n",
      " 0.2228\n",
      " 0.2259\n",
      " 0.2948\n",
      " 0.3048\n",
      " 0.3152\n",
      " 0.3131\n",
      " 0.2963\n",
      " 0.2380\n",
      " 0.2163\n",
      " 0.3574\n",
      " 0.2479\n",
      " 0.3253\n",
      " 0.3021\n",
      " 0.2545\n",
      " 0.2738\n",
      " 0.2465\n",
      " 0.2069\n",
      " 0.3125\n",
      " 0.2760\n",
      " 0.2836\n",
      " 0.2824\n",
      " 0.3097\n",
      " 0.2490\n",
      " 0.2397\n",
      " 0.1932\n",
      " 0.1693\n",
      " 0.2080\n",
      " 0.2534\n",
      " 0.2802\n",
      " 0.2686\n",
      " 0.2552\n",
      " 0.3017\n",
      " 0.2030\n",
      " 0.2773\n",
      " 0.2448\n",
      " 0.2012\n",
      " 0.2939\n",
      " 0.2789\n",
      " 0.3226\n",
      " 0.3427\n",
      " 0.3062\n",
      " 0.2754\n",
      " 0.3203\n",
      " 0.2967\n",
      " 0.3032\n",
      " 0.3378\n",
      " 0.3018\n",
      " 0.2821\n",
      " 0.3374\n",
      " 0.2746\n",
      " 0.3376\n",
      " 0.2460\n",
      " 0.3490\n",
      " 0.2413\n",
      " 0.2716\n",
      " 0.2791\n",
      " 0.3619\n",
      " 0.4643\n",
      " 0.4013\n",
      " 0.3233\n",
      " 0.3459\n",
      " 0.3612\n",
      " 0.2629\n",
      " 0.2859\n",
      " 0.2713\n",
      " 0.2434\n",
      " 0.2223\n",
      " 0.2541\n",
      " 0.3123\n",
      " 0.2274\n",
      " 0.2808\n",
      " 0.3803\n",
      " 0.3059\n",
      " 0.2667\n",
      " 0.2392\n",
      " 0.2828\n",
      " 0.2631\n",
      " 0.3833\n",
      " 0.2411\n",
      " 0.3126\n",
      " 0.3468\n",
      " 0.1922\n",
      " 0.3066\n",
      " 0.2703\n",
      " 0.3244\n",
      " 0.3003\n",
      " 0.2375\n",
      " 0.3576\n",
      " 0.3451\n",
      " 0.2583\n",
      " 0.2617\n",
      " 0.2514\n",
      " 0.2866\n",
      " 0.2814\n",
      " 0.3119\n",
      " 0.2866\n",
      " 0.2788\n",
      " 0.2929\n",
      " 0.3428\n",
      " 0.3606\n",
      " 0.3079\n",
      " 0.6690\n",
      " 0.4027\n",
      " 0.3017\n",
      " 0.2685\n",
      " 0.3048\n",
      " 0.2658\n",
      " 0.3115\n",
      " 0.2703\n",
      " 0.2790\n",
      " 0.3636\n",
      " 0.3029\n",
      " 0.3589\n",
      " 0.3138\n",
      " 0.3014\n",
      " 0.3471\n",
      " 0.2470\n",
      " 0.3724\n",
      " 0.2628\n",
      " 0.2762\n",
      " 0.2800\n",
      " 0.2851\n",
      " 0.2900\n",
      " 0.2187\n",
      " 0.2524\n",
      " 0.1912\n",
      " 0.2680\n",
      " 0.2259\n",
      " 0.3228\n",
      " 0.3694\n",
      " 0.3073\n",
      " 0.3076\n",
      " 0.2679\n",
      " 0.2601\n",
      " 0.3187\n",
      " 0.2838\n",
      " 0.2657\n",
      " 0.2890\n",
      " 0.3251\n",
      " 0.2654\n",
      " 0.3199\n",
      " 0.2998\n",
      " 0.2467\n",
      " 0.1412\n",
      " 0.2162\n",
      " 0.2714\n",
      " 0.2828\n",
      " 0.3509\n",
      " 0.2816\n",
      " 0.2462\n",
      " 0.2725\n",
      " 0.2077\n",
      " 0.2898\n",
      " 0.2482\n",
      " 0.2722\n",
      " 0.3317\n",
      " 0.3776\n",
      " 0.2630\n",
      " 0.2827\n",
      " 0.2293\n",
      " 0.2773\n",
      " 0.2779\n",
      " 0.4443\n",
      " 0.3461\n",
      " 0.3127\n",
      " 0.3890\n",
      " 0.2835\n",
      " 0.3821\n",
      " 0.2570\n",
      " 0.3604\n",
      " 0.2929\n",
      " 0.2824\n",
      " 0.3288\n",
      " 0.2134\n",
      " 0.2415\n",
      " 0.3315\n",
      " 0.2586\n",
      " 0.2429\n",
      " 0.3871\n",
      " 0.1293\n",
      " 0.2885\n",
      " 0.2760\n",
      " 0.2666\n",
      " 0.2257\n",
      " 0.2928\n",
      " 0.3392\n",
      " 0.3288\n",
      " 0.2522\n",
      " 0.3121\n",
      " 0.2747\n",
      " 0.2322\n",
      " 0.4000\n",
      " 0.3558\n",
      " 0.3000\n",
      " 0.2607\n",
      " 0.2847\n",
      " 0.2747\n",
      " 0.2811\n",
      " 0.2998\n",
      " 0.2184\n",
      " 0.3425\n",
      " 0.2721\n",
      " 0.2917\n",
      " 0.2795\n",
      " 0.3417\n",
      " 0.2118\n",
      " 0.2249\n",
      " 0.2837\n",
      " 0.3879\n",
      " 0.2694\n",
      " 0.2806\n",
      " 0.2545\n",
      " 0.2808\n",
      " 0.2632\n",
      " 0.2647\n",
      " 0.2587\n",
      " 0.3323\n",
      " 0.2793\n",
      " 0.2752\n",
      " 0.2908\n",
      " 0.3565\n",
      " 0.2759\n",
      " 0.3249\n",
      " 0.2579\n",
      " 0.3103\n",
      " 0.3313\n",
      " 0.3187\n",
      " 0.3249\n",
      " 0.2771\n",
      " 0.2571\n",
      " 0.2769\n",
      " 0.2207\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-0.0083\n",
      "-0.1531\n",
      "-0.3982\n",
      "-0.1484\n",
      "-0.1707\n",
      "-0.1660\n",
      "-0.2124\n",
      "-0.1908\n",
      "-0.1329\n",
      "-0.1873\n",
      "-0.1653\n",
      "-0.3240\n",
      "-0.0883\n",
      "-0.2148\n",
      "-0.1155\n",
      "-0.1670\n",
      "-0.2416\n",
      "-0.1596\n",
      "-0.2783\n",
      "-0.1571\n",
      "-0.0496\n",
      "-0.2262\n",
      "-0.1144\n",
      "-0.2020\n",
      "-0.2841\n",
      "-0.0977\n",
      "-0.3612\n",
      "-0.3327\n",
      "-0.2602\n",
      "-0.1302\n",
      "-0.1780\n",
      "-0.2202\n",
      "-0.2145\n",
      "-0.1633\n",
      "-0.3694\n",
      "-0.1474\n",
      "-0.2056\n",
      "-0.0959\n",
      "-0.2154\n",
      "-0.1245\n",
      "-0.3637\n",
      "-0.3129\n",
      "-0.2143\n",
      "-0.2814\n",
      "-0.3255\n",
      "-0.2645\n",
      "-0.3418\n",
      "-0.0768\n",
      "-0.1243\n",
      "-0.1026\n",
      "-0.2333\n",
      "-0.1250\n",
      "-0.2616\n",
      "-0.2075\n",
      "-0.1601\n",
      "-0.1016\n",
      "-0.1535\n",
      "-0.2879\n",
      "-0.2291\n",
      "-0.2892\n",
      "-0.1877\n",
      "-0.2213\n",
      "-0.1342\n",
      "-0.1120\n",
      "-0.1900\n",
      "-0.3190\n",
      "-0.2217\n",
      "-0.1496\n",
      "-0.1438\n",
      "-0.2972\n",
      "-0.3073\n",
      "-0.1584\n",
      "-0.0998\n",
      "-0.2016\n",
      "-0.1619\n",
      "-0.2499\n",
      "-0.2782\n",
      "-0.3279\n",
      "-0.1255\n",
      "-0.2054\n",
      "-0.2103\n",
      "-0.3796\n",
      "-0.1908\n",
      "-0.1982\n",
      "-0.2084\n",
      "-0.1938\n",
      "-0.4478\n",
      "-0.2792\n",
      "-0.2763\n",
      "-0.1653\n",
      "-0.2876\n",
      "-0.2689\n",
      "-0.1511\n",
      "-0.1809\n",
      "-0.1570\n",
      "-0.3041\n",
      "-0.1671\n",
      "-0.1455\n",
      "-0.3176\n",
      "-0.3402\n",
      "-0.2164\n",
      "-0.0574\n",
      "-0.1852\n",
      "-0.4036\n",
      "-0.2209\n",
      "-0.2181\n",
      "-0.4080\n",
      "-0.1347\n",
      "-0.1567\n",
      "-0.5822\n",
      "-0.1197\n",
      "-0.3383\n",
      "-0.2397\n",
      "-0.2611\n",
      "-0.2320\n",
      "-0.2427\n",
      "-0.2186\n",
      "-0.3070\n",
      "-0.1263\n",
      " 0.0300\n",
      "-0.1743\n",
      "-0.2033\n",
      "-0.1826\n",
      "-0.1720\n",
      "-0.1494\n",
      "-0.1238\n",
      "-0.2653\n",
      "-0.2687\n",
      "-0.2522\n",
      "-0.1641\n",
      "-0.1057\n",
      "-0.2269\n",
      "-0.3109\n",
      "-0.3164\n",
      "-0.1236\n",
      "-0.3999\n",
      "-0.0672\n",
      "-0.0713\n",
      "-0.2059\n",
      "-0.0980\n",
      "-0.1368\n",
      "-0.2199\n",
      "-0.3441\n",
      "-0.1808\n",
      "-0.1884\n",
      "-0.0751\n",
      "-0.1454\n",
      "-0.2252\n",
      "-0.2101\n",
      "-0.1317\n",
      "-0.1826\n",
      "-0.3182\n",
      "-0.1601\n",
      "-0.1586\n",
      "-0.1768\n",
      "-0.2805\n",
      "-0.1404\n",
      "-0.2380\n",
      "-0.2188\n",
      "-0.2724\n",
      "-0.3535\n",
      "-0.2733\n",
      "-0.3876\n",
      "-0.2226\n",
      "-0.1598\n",
      "-0.2377\n",
      "-0.1456\n",
      "-0.1686\n",
      "-0.3441\n",
      "-0.2569\n",
      "-0.2297\n",
      "-0.2311\n",
      "-0.2862\n",
      "-0.2288\n",
      "-0.2337\n",
      "-0.1419\n",
      "-0.2919\n",
      "-0.4714\n",
      "-0.2551\n",
      "-0.4441\n",
      "-0.3566\n",
      " 0.0219\n",
      "-0.1583\n",
      "-0.0114\n",
      "-0.1652\n",
      "-0.2402\n",
      "-0.2075\n",
      "-0.1789\n",
      "-0.1476\n",
      "-0.1903\n",
      "-0.2141\n",
      "-0.1261\n",
      "-0.2148\n",
      "-0.2167\n",
      "-0.2976\n",
      "-0.2865\n",
      "-0.1603\n",
      "-0.2914\n",
      "-0.2781\n",
      "-0.2104\n",
      "-0.4453\n",
      "-0.2069\n",
      "-0.2661\n",
      "-0.0831\n",
      "-0.2566\n",
      "-0.1208\n",
      "-0.2275\n",
      "-0.1532\n",
      "-0.1399\n",
      "-0.0643\n",
      "-0.1752\n",
      "-0.1471\n",
      "-0.1448\n",
      "-0.1399\n",
      "-0.1544\n",
      "-0.2337\n",
      "-0.1815\n",
      "-0.1386\n",
      "-0.1599\n",
      "-0.3304\n",
      "-0.3227\n",
      "-0.1440\n",
      "-0.1260\n",
      "-0.1165\n",
      "-0.3116\n",
      "-0.2745\n",
      "-0.3192\n",
      "-0.0866\n",
      "-0.0822\n",
      "-0.3679\n",
      "-0.1207\n",
      "-0.2607\n",
      "-0.1662\n",
      "-0.2723\n",
      "-0.1986\n",
      "-0.1617\n",
      "-0.2083\n",
      "-0.2868\n",
      "-0.0924\n",
      "-0.1661\n",
      "-0.2850\n",
      "-0.0324\n",
      "-0.2092\n",
      "-0.1079\n",
      "-0.4258\n",
      "-0.2665\n",
      "-0.3958\n",
      "-0.1470\n",
      "-0.2562\n",
      "-0.4091\n",
      "-0.1043\n",
      "-0.1372\n",
      "-0.2325\n",
      "-0.0619\n",
      "-0.3172\n",
      "-0.1179\n",
      "-0.1409\n",
      "-0.1332\n",
      "-0.1865\n",
      "-0.3422\n",
      "-0.2144\n",
      "-0.2533\n",
      "-0.2658\n",
      "-0.3442\n",
      "-0.1971\n",
      "-0.0082\n",
      "-0.2641\n",
      "-0.1506\n",
      "-0.4913\n",
      "-0.2103\n",
      "-0.3669\n",
      "-0.2836\n",
      "-0.1238\n",
      "-0.3422\n",
      "-0.4461\n",
      "-0.4401\n",
      "-0.2061\n",
      "-0.2280\n",
      "-0.4969\n",
      "-0.1786\n",
      "-0.0897\n",
      "-0.1043\n",
      "-0.2487\n",
      "-0.1980\n",
      "-0.3233\n",
      "-0.2645\n",
      "-0.2609\n",
      "-0.1236\n",
      "-0.1127\n",
      "-0.3085\n",
      "-0.1818\n",
      "-0.3339\n",
      "-0.2770\n",
      "-0.1326\n",
      "-0.1733\n",
      "-0.1410\n",
      "-0.0460\n",
      "-0.2341\n",
      "-0.1255\n",
      "-0.2073\n",
      "-0.2193\n",
      "-0.2871\n",
      "-0.1621\n",
      "-0.1401\n",
      "-0.0664\n",
      " 0.0016\n",
      "-0.0729\n",
      "-0.1837\n",
      "-0.2308\n",
      "-0.2323\n",
      "-0.1664\n",
      "-0.2486\n",
      "-0.0542\n",
      "-0.0880\n",
      "-0.1271\n",
      "-0.0744\n",
      "-0.1425\n",
      "-0.1497\n",
      "-0.2078\n",
      "-0.2859\n",
      "-0.2942\n",
      "-0.1978\n",
      "-0.3011\n",
      "-0.2469\n",
      "-0.2236\n",
      "-0.2733\n",
      "-0.2445\n",
      "-0.2196\n",
      "-0.2271\n",
      "-0.2044\n",
      "-0.3282\n",
      "-0.1296\n",
      "-0.3556\n",
      "-0.0970\n",
      "-0.1672\n",
      "-0.2153\n",
      "-0.3683\n",
      "-0.2866\n",
      "-0.2385\n",
      "-0.3189\n",
      "-0.3387\n",
      "-0.4051\n",
      "-0.1715\n",
      "-0.1939\n",
      "-0.2188\n",
      "-0.1247\n",
      "-0.0955\n",
      "-0.1316\n",
      "-0.1927\n",
      "-0.0939\n",
      "-0.1988\n",
      "-0.4029\n",
      "-0.2866\n",
      "-0.1463\n",
      "-0.1234\n",
      "-0.2210\n",
      "-0.1572\n",
      "-0.2872\n",
      "-0.1177\n",
      "-0.0273\n",
      "-0.3901\n",
      "-0.0838\n",
      "-0.2138\n",
      "-0.2233\n",
      "-0.3070\n",
      "-0.3112\n",
      "-0.0847\n",
      "-0.4158\n",
      "-0.3115\n",
      "-0.2178\n",
      "-0.1073\n",
      "-0.1570\n",
      "-0.2286\n",
      "-0.1726\n",
      "-0.2473\n",
      "-0.1239\n",
      "-0.1988\n",
      "-0.2644\n",
      "-0.2592\n",
      "-0.2924\n",
      "-0.2466\n",
      "-0.6011\n",
      "-0.2874\n",
      "-0.1980\n",
      "-0.1861\n",
      "-0.2860\n",
      "-0.1754\n",
      "-0.3060\n",
      "-0.1502\n",
      "-0.1901\n",
      "-0.3047\n",
      "-0.2266\n",
      "-0.4749\n",
      "-0.2495\n",
      "-0.2082\n",
      "-0.2770\n",
      "-0.1292\n",
      "-0.4451\n",
      "-0.2047\n",
      "-0.1921\n",
      "-0.2049\n",
      "-0.2111\n",
      "-0.2547\n",
      "-0.1091\n",
      "-0.0730\n",
      "-0.0093\n",
      "-0.2671\n",
      "-0.0972\n",
      "-0.2453\n",
      "-0.4540\n",
      "-0.1952\n",
      "-0.1670\n",
      "-0.1601\n",
      "-0.2209\n",
      "-0.3124\n",
      "-0.2674\n",
      "-0.2331\n",
      "-0.1820\n",
      "-0.3947\n",
      "-0.1539\n",
      "-0.2135\n",
      "-0.2372\n",
      "-0.1175\n",
      " 0.0162\n",
      "-0.1010\n",
      "-0.2044\n",
      "-0.2336\n",
      "-0.3588\n",
      "-0.1992\n",
      "-0.1890\n",
      "-0.1405\n",
      "-0.0314\n",
      "-0.2610\n",
      "-0.1739\n",
      "-0.1285\n",
      "-0.3043\n",
      "-0.5264\n",
      "-0.1252\n",
      "-0.2817\n",
      "-0.0812\n",
      "-0.2034\n",
      "-0.1273\n",
      "-0.6001\n",
      "-0.3635\n",
      "-0.2735\n",
      "-0.4125\n",
      "-0.2199\n",
      "-0.4947\n",
      "-0.1698\n",
      "-0.1768\n",
      "-0.2686\n",
      "-0.2153\n",
      "-0.3111\n",
      "-0.0782\n",
      "-0.1416\n",
      "-0.2698\n",
      "-0.1548\n",
      "-0.1514\n",
      "-0.3908\n",
      " 0.1090\n",
      "-0.2506\n",
      "-0.1846\n",
      "-0.2210\n",
      "-0.1181\n",
      "-0.1427\n",
      "-0.3149\n",
      "-0.3458\n",
      "-0.1396\n",
      "-0.2814\n",
      "-0.1865\n",
      "-0.1042\n",
      "-0.3850\n",
      "-0.3419\n",
      "-0.2181\n",
      "-0.1578\n",
      "-0.2603\n",
      "-0.1579\n",
      "-0.2089\n",
      "-0.3071\n",
      "-0.0712\n",
      "-0.3158\n",
      "-0.2096\n",
      "-0.2970\n",
      "-0.1659\n",
      "-0.4034\n",
      "-0.0827\n",
      "-0.0999\n",
      "-0.1632\n",
      "-0.5130\n",
      "-0.1720\n",
      "-0.1544\n",
      "-0.1792\n",
      "-0.2934\n",
      "-0.1542\n",
      "-0.1614\n",
      "-0.1475\n",
      "-0.2180\n",
      "-0.2237\n",
      "-0.2022\n",
      "-0.2513\n",
      "-0.2766\n",
      "-0.1649\n",
      "-0.3812\n",
      "-0.1244\n",
      "-0.2114\n",
      "-0.2540\n",
      "-0.2701\n",
      "-0.3119\n",
      "-0.1534\n",
      "-0.1142\n",
      "-0.1988\n",
      "-0.0825\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -1.0573e-02 -2.3080e-02 -8.2624e-03\n",
      " -9.6049e-03 -3.0893e-03  6.0450e-03\n",
      " -5.9078e-03  5.7698e-03  1.1516e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  6.2357e-03  6.0532e-03  1.2681e-02\n",
      "  5.4235e-03  4.6782e-03  1.4768e-02\n",
      " -1.9718e-02 -2.2287e-02 -8.9134e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  1.4071e-02  1.9870e-02  2.3649e-02\n",
      " -2.5480e-02 -1.8377e-02 -7.3886e-03\n",
      " -3.4670e-02 -3.6721e-02 -1.3963e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,509,.,.) = \n",
      "  7.2494e-03 -4.9630e-03 -2.7216e-04\n",
      " -1.9867e-02 -7.3550e-03 -1.0644e-02\n",
      " -2.9972e-02 -1.1375e-02 -2.6737e-02\n",
      "\n",
      "( 0 ,510,.,.) = \n",
      " -5.6509e-04 -3.8046e-03  5.7745e-04\n",
      "  7.0539e-03 -7.6276e-04  3.5874e-03\n",
      "  4.1435e-03  3.9963e-03  1.1695e-02\n",
      "\n",
      "( 0 ,511,.,.) = \n",
      "  1.3928e-02  1.1280e-02  7.7352e-03\n",
      " -4.5099e-04  8.4986e-03  2.1827e-03\n",
      " -6.0249e-03 -3.1066e-03 -1.0127e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -3.4816e-03 -5.4190e-03  2.5856e-03\n",
      " -1.0169e-02 -1.1685e-02 -1.4309e-02\n",
      " -1.5721e-02 -1.7909e-02 -3.0576e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  7.8347e-03  5.0811e-03  1.5678e-02\n",
      "  7.1776e-03  1.3225e-03  2.0253e-02\n",
      "  1.0266e-02  3.7866e-03  1.7974e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -1.8385e-02 -1.0362e-02 -9.6565e-03\n",
      " -1.0898e-02 -1.6486e-02 -1.4646e-02\n",
      "  2.9643e-03 -1.8096e-03 -9.2001e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,509,.,.) = \n",
      " -3.6587e-03 -6.4348e-03 -2.1737e-02\n",
      "  9.0283e-03  8.9539e-03 -7.7811e-05\n",
      " -3.9871e-03  3.3909e-03 -6.2260e-04\n",
      "\n",
      "( 1 ,510,.,.) = \n",
      "  6.9961e-04  3.4083e-03  1.5333e-02\n",
      "  3.8815e-03  9.7278e-04  1.0979e-02\n",
      "  7.8087e-03  1.0486e-03  8.9498e-03\n",
      "\n",
      "( 1 ,511,.,.) = \n",
      " -2.9100e-02 -2.5907e-02 -2.6973e-02\n",
      " -3.1529e-02 -2.2142e-02 -3.0956e-02\n",
      " -4.8455e-02 -4.3162e-02 -4.9559e-02\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -4.9463e-03  5.6497e-03 -7.7470e-03\n",
      "  5.0171e-03  1.3692e-02  3.6422e-03\n",
      " -1.7338e-02 -2.2711e-02 -3.6788e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -1.1550e-03 -9.3287e-03 -1.1604e-02\n",
      "  2.7320e-03 -5.3242e-03 -6.2057e-03\n",
      "  6.9242e-03 -7.9411e-05 -3.5128e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  2.7009e-02  2.8683e-02  3.1290e-02\n",
      " -4.6660e-03 -3.9792e-03 -3.2517e-03\n",
      " -9.8957e-03 -3.5948e-03 -5.4772e-03\n",
      "    ... \n",
      "\n",
      "( 2 ,509,.,.) = \n",
      " -1.5418e-03 -5.6432e-03 -3.2558e-03\n",
      "  2.0864e-02  2.5809e-02  1.0936e-02\n",
      " -3.6827e-03  1.0013e-02  4.1186e-03\n",
      "\n",
      "( 2 ,510,.,.) = \n",
      " -1.0699e-02 -2.8809e-02 -1.8278e-02\n",
      " -1.6317e-02 -2.8533e-02 -2.1907e-02\n",
      " -8.4787e-03 -1.3452e-02 -7.3141e-03\n",
      "\n",
      "( 2 ,511,.,.) = \n",
      " -1.8229e-02 -1.2668e-02 -1.6124e-02\n",
      "  1.1329e-03  4.5261e-03 -1.5916e-03\n",
      " -1.5618e-03 -5.3713e-03 -1.3541e-02\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      "  1.8995e-03 -1.1615e-03 -1.1701e-02\n",
      "  8.9200e-03  1.0615e-02  1.0839e-03\n",
      " -9.0176e-03  2.2087e-03 -2.6790e-04\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      " -4.9661e-03 -1.0520e-02 -4.8805e-03\n",
      "  5.1304e-03 -6.1249e-03  2.6541e-03\n",
      "  1.9308e-03 -3.6334e-03  5.4643e-03\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      " -2.8191e-02 -2.3394e-02 -1.9835e-02\n",
      " -1.7330e-02 -3.0708e-02 -1.9624e-02\n",
      " -8.5598e-03 -1.2165e-02 -3.5892e-03\n",
      "    ... \n",
      "\n",
      "(509,509,.,.) = \n",
      "  2.8580e-02  1.1540e-02  1.6333e-02\n",
      "  3.5655e-03  3.0921e-03  6.2314e-04\n",
      " -6.0142e-03  4.9470e-03 -1.2353e-02\n",
      "\n",
      "(509,510,.,.) = \n",
      "  1.6731e-02  9.2603e-03  7.4134e-03\n",
      "  8.4430e-03 -3.3077e-03 -9.8997e-03\n",
      "  1.0844e-02  4.6723e-03 -3.2837e-03\n",
      "\n",
      "(509,511,.,.) = \n",
      "  5.9150e-03  1.0548e-02  8.2850e-03\n",
      " -2.8559e-02 -1.3702e-02 -6.3485e-03\n",
      " -1.5160e-02 -9.7387e-04 -2.0933e-03\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -9.2458e-03 -1.9390e-02 -1.1899e-02\n",
      " -3.0216e-03 -1.3411e-02 -1.0540e-02\n",
      "  2.9361e-03 -3.8641e-03 -3.3083e-03\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      " -5.1476e-03  9.0093e-04 -7.9004e-03\n",
      " -5.0558e-03  2.8987e-03 -8.6751e-03\n",
      " -3.1220e-02 -2.0705e-02 -2.2539e-02\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      " -2.8862e-02 -2.2248e-02 -2.0408e-02\n",
      " -2.4245e-02 -1.4869e-02 -1.7826e-02\n",
      " -1.5971e-02 -8.5851e-03 -1.6539e-02\n",
      "    ... \n",
      "\n",
      "(510,509,.,.) = \n",
      " -1.0620e-02 -2.6428e-03 -2.6403e-03\n",
      " -7.3340e-03  7.9813e-03  6.9675e-03\n",
      " -2.2117e-03  2.7574e-03  9.2432e-03\n",
      "\n",
      "(510,510,.,.) = \n",
      " -1.8625e-02 -2.5682e-02 -1.2349e-02\n",
      " -1.9975e-02 -2.0301e-02 -1.0094e-02\n",
      " -1.0676e-02 -1.9254e-02 -1.2749e-02\n",
      "\n",
      "(510,511,.,.) = \n",
      "  3.4059e-03  1.4062e-02  6.3822e-03\n",
      "  7.7439e-03  2.8893e-02  2.5849e-02\n",
      "  1.5936e-03  1.8298e-02  1.2561e-02\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      " -2.0868e-02 -2.2585e-02 -2.6590e-02\n",
      " -1.0163e-02 -3.1835e-03 -1.8090e-02\n",
      " -1.3867e-02 -1.1745e-02 -2.2730e-02\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      "  1.1219e-02  1.0959e-02  2.0457e-02\n",
      " -1.6701e-02 -2.3295e-02 -9.2590e-03\n",
      " -1.4287e-02 -1.8845e-02 -9.4243e-03\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      " -2.3989e-02 -2.0146e-02 -1.6260e-02\n",
      " -2.7748e-02 -4.1667e-02 -3.7429e-02\n",
      " -2.2337e-03 -1.4443e-02 -1.1122e-03\n",
      "    ... \n",
      "\n",
      "(511,509,.,.) = \n",
      " -1.6560e-02 -5.9334e-03 -2.5924e-02\n",
      " -2.7319e-02 -2.2783e-03 -2.5772e-02\n",
      " -1.3904e-02 -6.3360e-03 -3.5024e-02\n",
      "\n",
      "(511,510,.,.) = \n",
      " -5.8042e-04 -9.4781e-03 -4.1842e-03\n",
      "  2.7480e-03 -3.4412e-03  1.5086e-03\n",
      " -4.4348e-03 -1.0695e-02 -4.4539e-03\n",
      "\n",
      "(511,511,.,.) = \n",
      " -7.4739e-03 -1.7955e-03 -4.7578e-03\n",
      " -2.4754e-03  7.5311e-03  5.9112e-03\n",
      " -1.3112e-02 -1.1173e-02 -1.3277e-02\n",
      "[torch.FloatTensor of size 512x512x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-07 *\n",
      " -0.7273\n",
      " -0.5062\n",
      " -0.8660\n",
      " -0.1829\n",
      " -0.2756\n",
      " -0.4906\n",
      " -0.7194\n",
      " -0.2494\n",
      "  0.1050\n",
      " -0.4734\n",
      "  0.1627\n",
      "  0.1358\n",
      " -0.1707\n",
      " -0.0416\n",
      " -0.3924\n",
      " -0.1118\n",
      " -0.4518\n",
      " -0.3551\n",
      " -0.6074\n",
      " -0.1253\n",
      " -0.8829\n",
      "  0.3988\n",
      " -0.4768\n",
      " -0.6369\n",
      " -0.0163\n",
      " -0.2174\n",
      "  0.0795\n",
      " -0.2214\n",
      " -0.3254\n",
      " -0.4477\n",
      " -0.2001\n",
      " -0.3598\n",
      " -0.8069\n",
      "  0.0160\n",
      " -0.5016\n",
      " -0.0886\n",
      " -0.4938\n",
      " -0.6920\n",
      " -0.3257\n",
      "  0.1714\n",
      " -0.7496\n",
      " -0.6987\n",
      "  0.2256\n",
      " -0.0836\n",
      " -0.2332\n",
      "  0.0464\n",
      " -0.3838\n",
      " -0.7728\n",
      " -0.0070\n",
      " -0.5156\n",
      "  0.2469\n",
      " -0.3229\n",
      " -0.4505\n",
      "  0.1773\n",
      " -1.0256\n",
      " -0.8404\n",
      " -0.4485\n",
      " -0.3738\n",
      " -0.3592\n",
      " -0.6099\n",
      " -0.4257\n",
      " -0.3787\n",
      " -0.7217\n",
      " -0.8847\n",
      "  0.4733\n",
      " -0.1996\n",
      " -0.2811\n",
      " -0.7984\n",
      " -0.4465\n",
      " -0.7310\n",
      " -0.2232\n",
      " -0.8856\n",
      " -0.3138\n",
      "  0.0613\n",
      " -0.7583\n",
      " -0.3295\n",
      " -0.4069\n",
      " -0.2006\n",
      "  0.1005\n",
      " -0.4514\n",
      "  0.4165\n",
      "  0.1292\n",
      " -0.2063\n",
      " -0.4001\n",
      " -0.5299\n",
      " -0.1434\n",
      " -0.3745\n",
      " -0.3750\n",
      " -0.4965\n",
      " -0.4724\n",
      " -0.4961\n",
      " -0.3810\n",
      " -0.6445\n",
      " -0.4161\n",
      "  0.1433\n",
      " -0.0774\n",
      " -0.0332\n",
      " -0.4672\n",
      " -0.3188\n",
      " -0.8337\n",
      " -0.4054\n",
      " -0.6234\n",
      " -0.1341\n",
      "  0.2504\n",
      " -0.2775\n",
      "  0.1256\n",
      " -0.7855\n",
      " -0.2856\n",
      " -0.8984\n",
      " -0.3220\n",
      "  0.2328\n",
      " -0.2201\n",
      " -0.1564\n",
      " -0.6431\n",
      " -0.3569\n",
      " -0.7689\n",
      " -0.7556\n",
      " -0.4840\n",
      " -0.2784\n",
      "  0.5056\n",
      "  0.1118\n",
      " -1.0042\n",
      " -0.3128\n",
      " -0.6168\n",
      " -1.2644\n",
      "  0.6719\n",
      " -1.0735\n",
      "  0.1742\n",
      " -0.1975\n",
      " -0.1000\n",
      "  0.3005\n",
      " -1.1800\n",
      " -0.2006\n",
      " -0.7590\n",
      " -0.3161\n",
      " -0.4269\n",
      " -0.0275\n",
      " -0.1021\n",
      " -0.1501\n",
      "  0.0846\n",
      " -0.1038\n",
      " -0.3442\n",
      " -1.0734\n",
      " -0.8253\n",
      "  0.7501\n",
      " -0.3464\n",
      "  0.1461\n",
      " -0.2874\n",
      " -0.1095\n",
      " -0.5173\n",
      " -0.3901\n",
      " -0.4620\n",
      " -0.3088\n",
      " -0.4748\n",
      " -0.7588\n",
      " -0.3199\n",
      " -0.5455\n",
      " -0.2444\n",
      " -0.0055\n",
      " -0.1744\n",
      " -1.1030\n",
      " -0.2185\n",
      " -0.3154\n",
      " -0.7924\n",
      " -0.5738\n",
      " -0.0270\n",
      " -0.1011\n",
      "  0.0679\n",
      " -0.7692\n",
      " -0.7856\n",
      "  0.1246\n",
      " -0.4527\n",
      "  0.2659\n",
      "  0.4008\n",
      " -0.8674\n",
      "  0.0063\n",
      "  0.2728\n",
      " -1.2464\n",
      " -0.2982\n",
      " -0.3369\n",
      " -0.4401\n",
      " -0.4426\n",
      " -0.8767\n",
      " -0.3574\n",
      " -0.2778\n",
      " -0.1772\n",
      " -0.3065\n",
      " -0.2445\n",
      " -0.1883\n",
      " -0.6018\n",
      " -0.9344\n",
      " -0.9043\n",
      " -0.2307\n",
      " -0.6959\n",
      "  0.0129\n",
      " -0.2562\n",
      " -0.6632\n",
      " -0.4916\n",
      " -0.4036\n",
      " -1.4789\n",
      " -0.3271\n",
      " -0.5294\n",
      " -0.1481\n",
      " -0.4438\n",
      "  0.3215\n",
      " -0.5625\n",
      " -0.2250\n",
      " -0.1236\n",
      " -0.1171\n",
      " -0.0081\n",
      "  0.8345\n",
      " -0.1570\n",
      " -0.4679\n",
      "  0.1399\n",
      "  0.3504\n",
      " -0.4674\n",
      " -0.2851\n",
      " -0.1976\n",
      " -0.7362\n",
      "  0.2590\n",
      " -0.0692\n",
      " -0.1998\n",
      " -0.8559\n",
      " -0.5392\n",
      " -0.3098\n",
      " -0.5924\n",
      "  0.2798\n",
      " -0.7897\n",
      "  0.1885\n",
      " -0.3435\n",
      "  0.0545\n",
      " -0.1062\n",
      " -0.8879\n",
      " -0.3692\n",
      "  0.0761\n",
      "  0.4228\n",
      "  0.1866\n",
      " -0.7533\n",
      "  0.1370\n",
      " -0.2661\n",
      " -0.1746\n",
      " -0.3462\n",
      " -0.2379\n",
      "  0.1145\n",
      " -0.6890\n",
      " -0.2400\n",
      " -0.8634\n",
      " -0.0322\n",
      " -0.3567\n",
      "  0.2481\n",
      "  0.5067\n",
      " -0.5575\n",
      " -0.4460\n",
      " -0.6450\n",
      " -0.2024\n",
      " -0.3740\n",
      " -0.8895\n",
      " -0.3642\n",
      " -0.3741\n",
      " -0.3290\n",
      " -0.1007\n",
      " -0.2238\n",
      " -0.2655\n",
      "  0.3621\n",
      " -0.4183\n",
      " -0.8628\n",
      " -0.4042\n",
      " -0.1863\n",
      " -0.4868\n",
      " -0.5590\n",
      " -0.4017\n",
      " -0.4070\n",
      " -0.4451\n",
      " -0.5121\n",
      " -0.4079\n",
      " -0.9348\n",
      " -0.5093\n",
      "  0.2716\n",
      " -0.4221\n",
      "  0.4254\n",
      "  0.2484\n",
      " -0.8790\n",
      " -0.2737\n",
      " -0.3919\n",
      " -0.0635\n",
      " -0.2475\n",
      "  0.0022\n",
      " -0.6297\n",
      " -0.4381\n",
      " -0.0566\n",
      " -0.3163\n",
      " -0.6928\n",
      " -0.7243\n",
      " -0.4094\n",
      " -0.6093\n",
      " -0.5837\n",
      " -0.1059\n",
      " -0.2815\n",
      "  0.0320\n",
      "  0.1606\n",
      " -0.5444\n",
      " -0.8113\n",
      " -0.4004\n",
      " -0.0524\n",
      " -0.3216\n",
      " -0.3063\n",
      " -0.0345\n",
      " -0.1067\n",
      " -0.1855\n",
      " -0.4380\n",
      " -0.8193\n",
      " -0.1681\n",
      "  0.0556\n",
      " -0.4197\n",
      " -0.3236\n",
      " -0.4281\n",
      " -0.8767\n",
      "  0.2465\n",
      " -0.8358\n",
      " -0.5163\n",
      " -0.1860\n",
      " -0.4305\n",
      " -0.1096\n",
      " -0.6860\n",
      " -0.6509\n",
      " -0.0143\n",
      " -0.2812\n",
      " -0.4928\n",
      "  0.3071\n",
      "  0.0390\n",
      " -0.8753\n",
      " -0.4128\n",
      " -0.6270\n",
      "  0.0135\n",
      " -0.1249\n",
      " -0.2884\n",
      " -0.1708\n",
      "  0.0923\n",
      "  0.1470\n",
      " -0.1507\n",
      "  0.0912\n",
      " -0.0291\n",
      " -0.3254\n",
      " -0.4160\n",
      " -0.0677\n",
      " -0.6386\n",
      " -0.5255\n",
      " -0.9042\n",
      " -0.4085\n",
      " -0.1997\n",
      "  0.0248\n",
      "  0.0962\n",
      " -0.0926\n",
      " -0.3317\n",
      " -0.4396\n",
      "  0.4367\n",
      " -0.4527\n",
      " -0.0627\n",
      " -0.5783\n",
      "  0.0980\n",
      " -0.1659\n",
      " -0.1410\n",
      " -0.2582\n",
      "  0.0731\n",
      " -0.0092\n",
      "  0.0127\n",
      " -0.2629\n",
      " -0.1084\n",
      " -0.0002\n",
      " -0.4822\n",
      " -0.2271\n",
      " -0.4379\n",
      " -0.8949\n",
      " -0.0243\n",
      " -0.9208\n",
      " -0.7277\n",
      "  0.0171\n",
      " -0.1155\n",
      " -0.8723\n",
      " -0.2559\n",
      " -0.1791\n",
      " -0.0400\n",
      " -0.3642\n",
      "  0.1501\n",
      "  0.3682\n",
      "  0.0345\n",
      "  0.0669\n",
      "  0.1106\n",
      " -0.2308\n",
      "  0.2567\n",
      " -0.2584\n",
      " -0.6280\n",
      " -0.2128\n",
      " -0.1573\n",
      " -0.2558\n",
      " -0.6209\n",
      " -0.2963\n",
      " -1.0960\n",
      " -0.3955\n",
      " -0.4763\n",
      " -0.4378\n",
      " -0.5759\n",
      "  0.1010\n",
      " -0.5433\n",
      "  0.2373\n",
      " -0.2581\n",
      " -0.0968\n",
      " -0.0066\n",
      "  0.0023\n",
      " -0.7746\n",
      " -0.3176\n",
      " -0.2841\n",
      "  0.1549\n",
      " -0.2746\n",
      " -0.8061\n",
      " -0.2427\n",
      "  0.0257\n",
      "  0.0649\n",
      " -0.4916\n",
      " -0.1298\n",
      " -0.1245\n",
      " -0.0079\n",
      " -0.4257\n",
      " -0.4481\n",
      " -0.1978\n",
      " -0.5259\n",
      " -0.3344\n",
      " -0.4420\n",
      " -0.6828\n",
      " -0.6973\n",
      " -0.2455\n",
      " -0.2351\n",
      " -0.8369\n",
      " -0.1938\n",
      " -0.7828\n",
      "  0.4270\n",
      " -0.3679\n",
      " -0.9705\n",
      " -0.6327\n",
      " -0.0984\n",
      " -0.0206\n",
      " -0.7489\n",
      " -0.0499\n",
      " -0.6369\n",
      " -0.0845\n",
      " -0.0005\n",
      " -0.6824\n",
      " -0.2771\n",
      " -0.0349\n",
      " -0.4824\n",
      " -0.4166\n",
      " -0.3793\n",
      " -0.9538\n",
      "  0.0251\n",
      "  0.1116\n",
      " -0.3836\n",
      "  0.1831\n",
      "  0.1080\n",
      " -0.4776\n",
      " -0.1748\n",
      " -0.8666\n",
      " -0.1299\n",
      "  0.1291\n",
      " -1.0056\n",
      " -0.2091\n",
      " -0.6710\n",
      " -0.0423\n",
      " -0.0563\n",
      " -0.4222\n",
      " -0.5365\n",
      " -0.3757\n",
      " -0.5214\n",
      " -0.0638\n",
      " -0.6818\n",
      " -0.4120\n",
      "  0.1903\n",
      " -0.2164\n",
      " -0.0055\n",
      " -0.6301\n",
      "  0.1963\n",
      " -0.7359\n",
      " -0.7009\n",
      " -0.9759\n",
      " -0.1856\n",
      "  0.0460\n",
      "  0.0966\n",
      " -0.5154\n",
      " -0.4357\n",
      " -0.9476\n",
      " -0.4744\n",
      "  0.0016\n",
      "  0.0242\n",
      "  0.2484\n",
      " -0.6058\n",
      " -0.5710\n",
      "  0.1961\n",
      " -0.0748\n",
      "  0.1725\n",
      " -0.1768\n",
      "  0.3199\n",
      " -0.4294\n",
      " -0.9164\n",
      " -0.3656\n",
      " -0.8227\n",
      " -0.1311\n",
      "  0.1311\n",
      " -0.3460\n",
      " -0.5034\n",
      "  0.2951\n",
      " -0.4338\n",
      " -0.0741\n",
      " -0.6933\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 0.3018\n",
      " 0.2847\n",
      " 0.3348\n",
      " 0.2648\n",
      " 0.2303\n",
      " 0.3118\n",
      " 0.3544\n",
      " 0.3043\n",
      " 0.2952\n",
      " 0.3515\n",
      " 0.3786\n",
      " 0.3209\n",
      " 0.2653\n",
      " 0.2347\n",
      " 0.2602\n",
      " 0.3044\n",
      " 0.2762\n",
      " 0.2976\n",
      " 0.2345\n",
      " 0.2738\n",
      " 0.3097\n",
      " 0.2150\n",
      " 0.3184\n",
      " 0.3773\n",
      " 0.2642\n",
      " 0.1980\n",
      " 0.2262\n",
      " 0.2711\n",
      " 0.2981\n",
      " 0.2418\n",
      " 0.2810\n",
      " 0.2341\n",
      " 0.3082\n",
      " 0.2675\n",
      " 0.2378\n",
      " 0.2419\n",
      " 0.2508\n",
      " 0.2466\n",
      " 0.2794\n",
      " 0.4442\n",
      " 0.2871\n",
      " 0.2972\n",
      " 0.3052\n",
      " 0.3650\n",
      " 0.2915\n",
      " 0.1944\n",
      " 0.3310\n",
      " 0.2551\n",
      " 0.2208\n",
      " 0.3038\n",
      " 0.2035\n",
      " 0.3116\n",
      " 0.2995\n",
      " 0.2100\n",
      " 0.3326\n",
      " 0.2982\n",
      " 0.3029\n",
      " 0.2616\n",
      " 0.2592\n",
      " 0.2819\n",
      " 0.3019\n",
      " 0.3024\n",
      " 0.3110\n",
      " 0.2804\n",
      " 0.3285\n",
      " 0.2188\n",
      " 0.2741\n",
      " 0.2994\n",
      " 0.2917\n",
      " 0.2720\n",
      " 0.2972\n",
      " 0.2318\n",
      " 0.2741\n",
      " 0.1924\n",
      " 0.2898\n",
      " 0.2704\n",
      " 0.3065\n",
      " 0.5719\n",
      " 0.3151\n",
      " 0.2628\n",
      " 0.3605\n",
      " 0.2908\n",
      " 0.2072\n",
      " 0.2574\n",
      " 0.2751\n",
      " 0.2774\n",
      " 0.2954\n",
      " 0.2382\n",
      " 0.2289\n",
      " 0.2687\n",
      " 0.3480\n",
      " 0.3682\n",
      " 0.2560\n",
      " 0.2185\n",
      " 0.2680\n",
      " 0.2844\n",
      " 0.3142\n",
      " 0.3034\n",
      " 0.3199\n",
      " 0.2668\n",
      " 0.1827\n",
      " 0.2469\n",
      " 0.3130\n",
      " 0.2446\n",
      " 0.3227\n",
      " 0.2166\n",
      " 0.2775\n",
      " 0.2512\n",
      " 0.3538\n",
      " 0.2917\n",
      " 0.2745\n",
      " 0.2748\n",
      " 0.2586\n",
      " 0.2275\n",
      " 0.2649\n",
      " 0.2567\n",
      " 0.2532\n",
      " 0.3310\n",
      " 0.1429\n",
      " 0.2762\n",
      " 0.3577\n",
      " 0.3279\n",
      " 0.2644\n",
      " 0.2788\n",
      " 0.3244\n",
      " 0.3951\n",
      " 0.3752\n",
      " 0.2883\n",
      " 0.2139\n",
      " 0.2504\n",
      " 0.3263\n",
      " 0.2582\n",
      " 0.2407\n",
      " 0.3330\n",
      " 0.2336\n",
      " 0.3559\n",
      " 0.1809\n",
      " 0.2941\n",
      " 0.2648\n",
      " 0.3548\n",
      " 0.1911\n",
      " 0.3318\n",
      " 0.3259\n",
      " 0.2716\n",
      " 0.3332\n",
      " 0.2071\n",
      " 0.2374\n",
      " 0.1908\n",
      " 0.1903\n",
      " 0.2339\n",
      " 0.2319\n",
      " 0.2132\n",
      " 0.2452\n",
      " 0.1692\n",
      " 0.2521\n",
      " 0.2627\n",
      " 0.2561\n",
      " 0.2525\n",
      " 0.2390\n",
      " 0.3703\n",
      " 0.3710\n",
      " 0.3343\n",
      " 0.3165\n",
      " 0.3202\n",
      " 0.3101\n",
      " 0.2238\n",
      " 0.2585\n",
      " 0.2325\n",
      " 0.4173\n",
      " 0.2713\n",
      " 0.1846\n",
      " 0.2699\n",
      " 0.2707\n",
      " 0.2942\n",
      " 0.2950\n",
      " 0.2149\n",
      " 0.3398\n",
      " 0.3667\n",
      " 0.2336\n",
      " 0.2956\n",
      " 0.2554\n",
      " 0.2840\n",
      " 0.2484\n",
      " 0.2905\n",
      " 0.2610\n",
      " 0.2478\n",
      " 0.2417\n",
      " 0.2583\n",
      " 0.2956\n",
      " 0.2407\n",
      " 0.2864\n",
      " 0.2696\n",
      " 0.3019\n",
      " 0.2017\n",
      " 0.3171\n",
      " 0.2470\n",
      " 0.3168\n",
      " 0.2373\n",
      " 0.3060\n",
      " 0.3261\n",
      " 0.2128\n",
      " 0.2166\n",
      " 0.2363\n",
      " 0.2772\n",
      " 0.2434\n",
      " 0.2873\n",
      " 0.2060\n",
      " 0.2560\n",
      " 0.2747\n",
      " 0.3787\n",
      " 0.2863\n",
      " 0.2250\n",
      " 0.3308\n",
      " 0.2358\n",
      " 0.2653\n",
      " 0.2732\n",
      " 0.3224\n",
      " 0.2447\n",
      " 0.2664\n",
      " 0.3260\n",
      " 0.2499\n",
      " 0.1693\n",
      " 0.3223\n",
      " 0.2542\n",
      " 0.3166\n",
      " 0.3504\n",
      " 0.2827\n",
      " 0.3544\n",
      " 0.2429\n",
      " 0.3793\n",
      " 0.3080\n",
      " 0.1581\n",
      " 0.3430\n",
      " 0.2629\n",
      " 0.3225\n",
      " 0.3353\n",
      " 0.3079\n",
      " 0.2803\n",
      " 0.3149\n",
      " 0.2901\n",
      " 0.2211\n",
      " 0.2874\n",
      " 0.3065\n",
      " 0.1877\n",
      " 0.2944\n",
      " 0.2354\n",
      " 0.2865\n",
      " 0.4092\n",
      " 0.2009\n",
      " 0.2483\n",
      " 0.2101\n",
      " 0.1963\n",
      " 0.3359\n",
      " 0.2601\n",
      " 0.2674\n",
      " 0.2803\n",
      " 0.3322\n",
      " 0.2290\n",
      " 0.2409\n",
      " 0.2673\n",
      " 0.2949\n",
      " 0.3368\n",
      " 0.2902\n",
      " 0.2478\n",
      " 0.2699\n",
      " 0.3288\n",
      " 0.2850\n",
      " 0.2702\n",
      " 0.3282\n",
      " 0.3522\n",
      " 0.2976\n",
      " 0.2807\n",
      " 0.2730\n",
      " 0.2784\n",
      " 0.2185\n",
      " 0.3057\n",
      " 0.2904\n",
      " 0.3020\n",
      " 0.2051\n",
      " 0.2480\n",
      " 0.2542\n",
      " 0.2475\n",
      " 0.2731\n",
      " 0.2678\n",
      " 0.2554\n",
      " 0.3012\n",
      " 0.3253\n",
      " 0.3123\n",
      " 0.2352\n",
      " 0.2968\n",
      " 0.2360\n",
      " 0.3583\n",
      " 0.3805\n",
      " 0.3185\n",
      " 0.2636\n",
      " 0.2695\n",
      " 0.3223\n",
      " 0.2171\n",
      " 0.2516\n",
      " 0.3727\n",
      " 0.2282\n",
      " 0.3157\n",
      " 0.2301\n",
      " 0.2582\n",
      " 0.3473\n",
      " 0.1583\n",
      " 0.2848\n",
      " 0.3669\n",
      " 0.3053\n",
      " 0.2475\n",
      " 0.2798\n",
      " 0.3217\n",
      " 0.2853\n",
      " 0.2061\n",
      " 0.2652\n",
      " 0.3179\n",
      " 0.3047\n",
      " 0.2484\n",
      " 0.2439\n",
      " 0.2650\n",
      " 0.2212\n",
      " 0.2662\n",
      " 0.3067\n",
      " 0.3450\n",
      " 0.2978\n",
      " 0.3045\n",
      " 0.3046\n",
      " 0.2847\n",
      " 0.2622\n",
      " 0.2926\n",
      " 0.2632\n",
      " 0.2961\n",
      " 0.2953\n",
      " 0.2972\n",
      " 0.2831\n",
      " 0.2004\n",
      " 0.3131\n",
      " 0.2434\n",
      " 0.2262\n",
      " 0.2365\n",
      " 0.2474\n",
      " 0.2721\n",
      " 0.3087\n",
      " 0.2456\n",
      " 0.2574\n",
      " 0.2824\n",
      " 0.2426\n",
      " 0.3535\n",
      " 0.2822\n",
      " 0.1307\n",
      " 0.2345\n",
      " 0.3859\n",
      " 0.2845\n",
      " 0.3392\n",
      " 0.2867\n",
      " 0.2446\n",
      " 0.3002\n",
      " 0.2958\n",
      " 0.3385\n",
      " 0.2593\n",
      " 0.1516\n",
      " 0.3156\n",
      " 0.1638\n",
      " 0.3088\n",
      " 0.2630\n",
      " 0.2619\n",
      " 0.2887\n",
      " 0.2671\n",
      " 0.2984\n",
      " 0.3035\n",
      " 0.2238\n",
      " 0.2908\n",
      " 0.2761\n",
      " 0.2336\n",
      " 0.2916\n",
      " 0.2755\n",
      " 0.2759\n",
      " 0.3653\n",
      " 0.2647\n",
      " 0.2998\n",
      " 0.1880\n",
      " 0.1600\n",
      " 0.2938\n",
      " 0.2202\n",
      " 0.2790\n",
      " 0.2720\n",
      " 0.2856\n",
      " 0.2439\n",
      " 0.2767\n",
      " 0.2976\n",
      " 0.2947\n",
      " 0.1800\n",
      " 0.2582\n",
      " 0.3292\n",
      " 0.3558\n",
      " 0.2600\n",
      " 0.2779\n",
      " 0.2993\n",
      " 0.2153\n",
      " 0.2972\n",
      " 0.2363\n",
      " 0.3103\n",
      " 0.3361\n",
      " 0.3031\n",
      " 0.3032\n",
      " 0.2207\n",
      " 0.2365\n",
      " 0.2435\n",
      " 0.2558\n",
      " 0.2786\n",
      " 0.2984\n",
      " 0.3082\n",
      " 0.3195\n",
      " 0.2462\n",
      " 0.3793\n",
      " 0.2501\n",
      " 0.2973\n",
      " 0.2996\n",
      " 0.2898\n",
      " 0.2060\n",
      " 0.2344\n",
      " 0.2984\n",
      " 0.2505\n",
      " 0.1627\n",
      " 0.3101\n",
      " 0.3094\n",
      " 0.3466\n",
      " 0.2354\n",
      " 0.2899\n",
      " 0.2685\n",
      " 0.2907\n",
      " 0.2967\n",
      " 0.2364\n",
      " 0.2479\n",
      " 0.2473\n",
      " 0.3722\n",
      " 0.2823\n",
      " 0.2517\n",
      " 0.2566\n",
      " 0.3262\n",
      " 0.2684\n",
      " 0.3332\n",
      " 0.3590\n",
      " 0.1643\n",
      " 0.2772\n",
      " 0.3562\n",
      " 0.2075\n",
      " 0.1716\n",
      " 0.2444\n",
      " 0.4248\n",
      " 0.2933\n",
      " 0.2014\n",
      " 0.3150\n",
      " 0.4192\n",
      " 0.1648\n",
      " 0.2900\n",
      " 0.2641\n",
      " 0.2527\n",
      " 0.2534\n",
      " 0.2784\n",
      " 0.2712\n",
      " 0.2136\n",
      " 0.3263\n",
      " 0.3162\n",
      " 0.1883\n",
      " 0.3673\n",
      " 0.2546\n",
      " 0.2620\n",
      " 0.3052\n",
      " 0.3418\n",
      " 0.3176\n",
      " 0.2941\n",
      " 0.2677\n",
      " 0.5361\n",
      " 0.3191\n",
      " 0.2738\n",
      " 0.3310\n",
      " 0.3147\n",
      " 0.2525\n",
      " 0.3353\n",
      " 0.2961\n",
      " 0.3583\n",
      " 0.2711\n",
      " 0.2010\n",
      " 0.2788\n",
      " 0.2385\n",
      " 0.2707\n",
      " 0.2571\n",
      " 0.3079\n",
      " 0.2701\n",
      " 0.2735\n",
      " 0.2347\n",
      " 0.2194\n",
      " 0.2942\n",
      " 0.3015\n",
      " 0.2852\n",
      " 0.2214\n",
      " 0.3115\n",
      " 0.2830\n",
      " 0.3249\n",
      " 0.3111\n",
      " 0.4178\n",
      " 0.3272\n",
      " 0.3516\n",
      " 0.2790\n",
      " 0.3254\n",
      " 0.2122\n",
      " 0.3170\n",
      " 0.3294\n",
      " 0.3805\n",
      " 0.2642\n",
      " 0.3363\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2563\n",
      "-0.2269\n",
      "-0.3236\n",
      "-0.2007\n",
      "-0.1550\n",
      "-0.3083\n",
      "-0.3842\n",
      "-0.2981\n",
      "-0.2550\n",
      "-0.3225\n",
      "-0.3320\n",
      "-0.3418\n",
      "-0.2063\n",
      "-0.1669\n",
      "-0.1932\n",
      "-0.3057\n",
      "-0.2286\n",
      "-0.3088\n",
      "-0.1508\n",
      "-0.2107\n",
      "-0.3015\n",
      "-0.1073\n",
      "-0.3702\n",
      "-0.4415\n",
      "-0.1699\n",
      "-0.0833\n",
      "-0.1460\n",
      "-0.2254\n",
      "-0.2856\n",
      "-0.1628\n",
      "-0.2756\n",
      "-0.1336\n",
      "-0.2794\n",
      "-0.2397\n",
      "-0.1595\n",
      "-0.1677\n",
      "-0.1806\n",
      "-0.1575\n",
      "-0.2331\n",
      "-0.6329\n",
      "-0.2645\n",
      "-0.2810\n",
      "-0.2846\n",
      "-0.5285\n",
      "-0.2628\n",
      "-0.0591\n",
      "-0.3463\n",
      "-0.1708\n",
      "-0.1508\n",
      "-0.2580\n",
      "-0.1191\n",
      "-0.3380\n",
      "-0.2427\n",
      "-0.0960\n",
      "-0.2992\n",
      "-0.3098\n",
      "-0.3487\n",
      "-0.1922\n",
      "-0.2089\n",
      "-0.2556\n",
      "-0.2991\n",
      "-0.3013\n",
      "-0.3599\n",
      "-0.2232\n",
      "-0.2739\n",
      "-0.1353\n",
      "-0.2119\n",
      "-0.2132\n",
      "-0.2342\n",
      "-0.1710\n",
      "-0.2516\n",
      "-0.1151\n",
      "-0.2183\n",
      "-0.0531\n",
      "-0.2386\n",
      "-0.2596\n",
      "-0.2848\n",
      "-0.5438\n",
      "-0.3392\n",
      "-0.1999\n",
      "-0.4158\n",
      "-0.2034\n",
      "-0.1118\n",
      "-0.1605\n",
      "-0.2034\n",
      "-0.2910\n",
      "-0.2399\n",
      "-0.1603\n",
      "-0.1517\n",
      "-0.1880\n",
      "-0.3968\n",
      "-0.4091\n",
      "-0.1869\n",
      "-0.1195\n",
      "-0.2296\n",
      "-0.2822\n",
      "-0.3117\n",
      "-0.2844\n",
      "-0.3622\n",
      "-0.2272\n",
      "-0.0617\n",
      "-0.1711\n",
      "-0.3385\n",
      "-0.1301\n",
      "-0.3123\n",
      "-0.1788\n",
      "-0.1880\n",
      "-0.1657\n",
      "-0.4427\n",
      "-0.2210\n",
      "-0.2074\n",
      "-0.1895\n",
      "-0.1857\n",
      "-0.1315\n",
      "-0.2308\n",
      "-0.1489\n",
      "-0.1858\n",
      "-0.2932\n",
      " 0.0389\n",
      "-0.2176\n",
      "-0.4308\n",
      "-0.3359\n",
      "-0.1938\n",
      "-0.2015\n",
      "-0.3288\n",
      "-0.5418\n",
      "-0.4583\n",
      "-0.2452\n",
      "-0.1108\n",
      "-0.2015\n",
      "-0.3321\n",
      "-0.1833\n",
      "-0.1573\n",
      "-0.3319\n",
      "-0.1054\n",
      "-0.4194\n",
      "-0.0449\n",
      "-0.2587\n",
      "-0.2222\n",
      "-0.3946\n",
      "-0.0491\n",
      "-0.4529\n",
      "-0.3936\n",
      "-0.2194\n",
      "-0.3164\n",
      "-0.0576\n",
      "-0.1279\n",
      "-0.0965\n",
      "-0.0753\n",
      "-0.1436\n",
      "-0.1232\n",
      "-0.0950\n",
      "-0.1837\n",
      "-0.0285\n",
      "-0.1641\n",
      "-0.1682\n",
      "-0.2037\n",
      "-0.2005\n",
      "-0.1875\n",
      "-0.4441\n",
      "-0.3883\n",
      "-0.3933\n",
      "-0.3129\n",
      "-0.3009\n",
      "-0.3179\n",
      "-0.1185\n",
      "-0.2187\n",
      "-0.1018\n",
      "-0.2982\n",
      "-0.2342\n",
      "-0.0434\n",
      "-0.1894\n",
      "-0.1615\n",
      "-0.2255\n",
      "-0.3054\n",
      "-0.1262\n",
      "-0.3634\n",
      "-0.4223\n",
      "-0.1607\n",
      "-0.2519\n",
      "-0.2264\n",
      "-0.2398\n",
      "-0.1830\n",
      "-0.2887\n",
      "-0.2175\n",
      "-0.1781\n",
      "-0.1781\n",
      "-0.1801\n",
      "-0.2696\n",
      "-0.1481\n",
      "-0.1764\n",
      "-0.2037\n",
      "-0.3605\n",
      "-0.0865\n",
      "-0.2793\n",
      "-0.1734\n",
      "-0.2628\n",
      "-0.1742\n",
      "-0.2701\n",
      "-0.3372\n",
      "-0.0747\n",
      "-0.1185\n",
      "-0.1385\n",
      "-0.2787\n",
      "-0.1741\n",
      "-0.2602\n",
      "-0.1165\n",
      "-0.1900\n",
      "-0.2079\n",
      "-0.4553\n",
      "-0.2977\n",
      "-0.1047\n",
      "-0.4082\n",
      "-0.1175\n",
      "-0.2315\n",
      "-0.2157\n",
      "-0.3381\n",
      "-0.1648\n",
      "-0.1934\n",
      "-0.4579\n",
      "-0.1932\n",
      "-0.0305\n",
      "-0.3346\n",
      "-0.1542\n",
      "-0.2530\n",
      "-0.4312\n",
      "-0.2425\n",
      "-0.4146\n",
      "-0.1783\n",
      "-0.4531\n",
      "-0.2775\n",
      "-0.0131\n",
      "-0.3644\n",
      "-0.2266\n",
      "-0.3524\n",
      "-0.3765\n",
      "-0.3168\n",
      "-0.2674\n",
      "-0.3091\n",
      "-0.2826\n",
      "-0.1128\n",
      "-0.2126\n",
      "-0.2577\n",
      "-0.0451\n",
      "-0.2358\n",
      "-0.1245\n",
      "-0.2502\n",
      "-0.4693\n",
      "-0.0866\n",
      "-0.1933\n",
      "-0.1153\n",
      "-0.0824\n",
      "-0.3684\n",
      "-0.1958\n",
      "-0.2080\n",
      "-0.2930\n",
      "-0.3520\n",
      "-0.1792\n",
      "-0.1751\n",
      "-0.2361\n",
      "-0.2441\n",
      "-0.4147\n",
      "-0.2492\n",
      "-0.1754\n",
      "-0.1993\n",
      "-0.3691\n",
      "-0.2304\n",
      "-0.1728\n",
      "-0.3858\n",
      "-0.3596\n",
      "-0.2558\n",
      "-0.1787\n",
      "-0.2090\n",
      "-0.2495\n",
      "-0.1005\n",
      "-0.3069\n",
      "-0.3007\n",
      "-0.3227\n",
      "-0.1023\n",
      "-0.1347\n",
      "-0.2122\n",
      "-0.1472\n",
      "-0.2649\n",
      "-0.2165\n",
      "-0.1587\n",
      "-0.2310\n",
      "-0.4234\n",
      "-0.3155\n",
      "-0.1577\n",
      "-0.2182\n",
      "-0.1520\n",
      "-0.4023\n",
      "-0.4421\n",
      "-0.3547\n",
      "-0.2160\n",
      "-0.2165\n",
      "-0.3209\n",
      "-0.0862\n",
      "-0.1847\n",
      "-0.5066\n",
      "-0.1692\n",
      "-0.2675\n",
      "-0.1224\n",
      "-0.1812\n",
      "-0.3951\n",
      "-0.0030\n",
      "-0.2171\n",
      "-0.2567\n",
      "-0.2528\n",
      "-0.1930\n",
      "-0.2730\n",
      "-0.3543\n",
      "-0.2554\n",
      "-0.0918\n",
      "-0.1654\n",
      "-0.3301\n",
      "-0.3223\n",
      "-0.2120\n",
      "-0.1824\n",
      "-0.2108\n",
      "-0.1212\n",
      "-0.2146\n",
      "-0.2588\n",
      "-0.3524\n",
      "-0.2482\n",
      "-0.1973\n",
      "-0.2994\n",
      "-0.2284\n",
      "-0.2061\n",
      "-0.2232\n",
      "-0.1703\n",
      "-0.2771\n",
      "-0.2680\n",
      "-0.2609\n",
      "-0.2368\n",
      "-0.0928\n",
      "-0.3250\n",
      "-0.1833\n",
      "-0.1375\n",
      "-0.1359\n",
      "-0.1748\n",
      "-0.2076\n",
      "-0.2539\n",
      "-0.1447\n",
      "-0.1668\n",
      "-0.2513\n",
      "-0.1730\n",
      "-0.4243\n",
      "-0.2669\n",
      " 0.0273\n",
      "-0.1698\n",
      "-0.5458\n",
      "-0.2497\n",
      "-0.3537\n",
      "-0.2813\n",
      "-0.1150\n",
      "-0.2529\n",
      "-0.3073\n",
      "-0.4134\n",
      "-0.1986\n",
      "-0.0150\n",
      "-0.3217\n",
      "-0.0287\n",
      "-0.3652\n",
      "-0.2078\n",
      "-0.2080\n",
      "-0.2184\n",
      "-0.2165\n",
      "-0.2849\n",
      "-0.3105\n",
      "-0.1371\n",
      "-0.2348\n",
      "-0.2481\n",
      "-0.1559\n",
      "-0.2339\n",
      "-0.2400\n",
      "-0.2399\n",
      "-0.4201\n",
      "-0.2056\n",
      "-0.3121\n",
      "-0.0631\n",
      "-0.0625\n",
      "-0.2306\n",
      "-0.1172\n",
      "-0.2170\n",
      "-0.1682\n",
      "-0.2360\n",
      "-0.1432\n",
      "-0.2261\n",
      "-0.2874\n",
      "-0.2588\n",
      "-0.0676\n",
      "-0.1951\n",
      "-0.3432\n",
      "-0.3267\n",
      "-0.2070\n",
      "-0.2100\n",
      "-0.3047\n",
      "-0.0585\n",
      "-0.2821\n",
      "-0.1494\n",
      "-0.2531\n",
      "-0.4081\n",
      "-0.2640\n",
      "-0.3577\n",
      "-0.1417\n",
      "-0.1436\n",
      "-0.1473\n",
      "-0.1940\n",
      "-0.2673\n",
      "-0.2829\n",
      "-0.2934\n",
      "-0.3192\n",
      "-0.1623\n",
      "-0.4229\n",
      "-0.1795\n",
      "-0.2362\n",
      "-0.1424\n",
      "-0.2403\n",
      "-0.0923\n",
      "-0.1630\n",
      "-0.3076\n",
      "-0.2001\n",
      "-0.0250\n",
      "-0.2622\n",
      "-0.2805\n",
      "-0.3552\n",
      "-0.1232\n",
      "-0.2514\n",
      "-0.2121\n",
      "-0.3007\n",
      "-0.2719\n",
      "-0.1552\n",
      "-0.1917\n",
      "-0.1542\n",
      "-0.4314\n",
      "-0.2740\n",
      "-0.1765\n",
      "-0.1976\n",
      "-0.3103\n",
      "-0.2385\n",
      "-0.3080\n",
      "-0.2692\n",
      "-0.0232\n",
      "-0.2082\n",
      "-0.4916\n",
      "-0.0929\n",
      "-0.0388\n",
      "-0.1579\n",
      "-0.5756\n",
      "-0.2616\n",
      "-0.0892\n",
      "-0.3013\n",
      "-0.4113\n",
      "-0.0102\n",
      "-0.2294\n",
      "-0.2226\n",
      "-0.1760\n",
      "-0.1593\n",
      "-0.3092\n",
      "-0.2044\n",
      "-0.0928\n",
      "-0.2432\n",
      "-0.2944\n",
      "-0.0768\n",
      "-0.4229\n",
      "-0.1922\n",
      "-0.2303\n",
      "-0.2700\n",
      "-0.3889\n",
      "-0.3560\n",
      "-0.2143\n",
      "-0.1645\n",
      "-0.4522\n",
      "-0.3077\n",
      "-0.2316\n",
      "-0.3659\n",
      "-0.3244\n",
      "-0.1771\n",
      "-0.3415\n",
      "-0.2808\n",
      "-0.4154\n",
      "-0.1945\n",
      "-0.0796\n",
      "-0.2264\n",
      "-0.1673\n",
      "-0.2486\n",
      "-0.1810\n",
      "-0.2567\n",
      "-0.2325\n",
      "-0.1681\n",
      "-0.1342\n",
      "-0.1029\n",
      "-0.2498\n",
      "-0.2477\n",
      "-0.2641\n",
      "-0.1370\n",
      "-0.4035\n",
      "-0.2259\n",
      "-0.3797\n",
      "-0.2973\n",
      "-0.5561\n",
      "-0.3350\n",
      "-0.3916\n",
      "-0.2072\n",
      "-0.2577\n",
      "-0.1027\n",
      "-0.2803\n",
      "-0.2543\n",
      "-0.4517\n",
      "-0.1821\n",
      "-0.3708\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  4.8097e-03  6.9162e-04  9.9851e-04\n",
      " -6.2526e-03 -1.2559e-02 -4.9773e-03\n",
      " -7.5619e-07 -3.3279e-03  6.1700e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -1.1608e-02 -1.5385e-02 -1.0941e-02\n",
      " -8.8582e-03 -1.0885e-02 -2.8053e-03\n",
      " -6.3653e-03 -7.7471e-03 -8.5226e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -1.1916e-02 -1.2521e-02 -1.3789e-02\n",
      " -1.8345e-02 -1.2241e-02 -1.2356e-02\n",
      " -7.8214e-03 -1.3127e-02 -1.7433e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,509,.,.) = \n",
      " -2.2039e-03 -8.9512e-04 -3.4030e-03\n",
      " -4.6303e-03  1.7919e-03  3.2453e-03\n",
      "  9.2735e-03  1.0165e-02  8.0554e-03\n",
      "\n",
      "( 0 ,510,.,.) = \n",
      "  6.5467e-03  1.3800e-03  1.8701e-03\n",
      "  4.8530e-03 -4.6141e-03 -4.3389e-03\n",
      "  1.1134e-02  8.1298e-03  7.1727e-03\n",
      "\n",
      "( 0 ,511,.,.) = \n",
      " -3.0414e-03 -1.3113e-02 -1.1214e-02\n",
      "  4.9109e-04 -1.1282e-02 -5.7934e-03\n",
      " -4.2849e-03 -1.8209e-02 -1.2619e-02\n",
      "      â‹®  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  1.4743e-02  1.3113e-02  2.1808e-03\n",
      "  1.7514e-02  2.1039e-02  1.3896e-02\n",
      " -1.6650e-02 -1.9357e-02 -1.7357e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -1.3660e-02 -1.3575e-02 -2.1192e-02\n",
      " -8.2354e-03 -3.3456e-03 -1.4833e-02\n",
      " -7.0678e-03 -2.6961e-03 -4.9955e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -6.9830e-03 -5.6917e-03 -9.3048e-03\n",
      " -8.1261e-03 -7.0047e-03 -7.3489e-03\n",
      " -1.2291e-02 -1.4655e-02 -8.0737e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,509,.,.) = \n",
      " -2.2240e-02 -1.0931e-02 -1.3287e-02\n",
      " -9.6792e-03 -1.0724e-02 -1.9962e-02\n",
      " -6.4873e-04 -2.5435e-03 -1.2904e-02\n",
      "\n",
      "( 1 ,510,.,.) = \n",
      " -1.9566e-03 -3.8872e-03 -3.7152e-03\n",
      " -1.3939e-02 -2.2669e-02 -2.4512e-02\n",
      " -1.5182e-02 -2.8835e-02 -2.8025e-02\n",
      "\n",
      "( 1 ,511,.,.) = \n",
      " -8.7985e-03 -3.9324e-03 -1.4130e-02\n",
      "  1.0219e-02  1.1960e-02  7.4219e-05\n",
      "  1.3507e-02  1.5830e-02  2.6703e-03\n",
      "      â‹®  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -1.3118e-02 -1.8037e-02 -6.2815e-03\n",
      " -2.2417e-02 -2.4473e-02 -1.2383e-02\n",
      " -8.3970e-03 -6.5390e-03 -2.2582e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -6.5997e-03 -2.9062e-03 -1.8149e-03\n",
      " -1.0380e-02 -9.7480e-03 -1.6655e-02\n",
      " -1.0608e-02 -1.3035e-02 -1.9465e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -1.4447e-03  7.8364e-03  4.1482e-03\n",
      " -2.0937e-02 -1.2840e-02 -1.1146e-02\n",
      " -2.0035e-02 -1.6109e-02 -1.8362e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,509,.,.) = \n",
      "  3.1755e-03 -2.9160e-03 -4.0026e-04\n",
      " -1.2940e-02 -2.1294e-02 -1.6432e-02\n",
      " -1.8777e-02 -1.7011e-02 -1.3906e-02\n",
      "\n",
      "( 2 ,510,.,.) = \n",
      " -2.5469e-03  5.6103e-04 -5.7330e-03\n",
      "  2.9395e-03  5.5965e-03  1.4657e-04\n",
      "  1.6186e-02  2.1615e-02  1.8510e-02\n",
      "\n",
      "( 2 ,511,.,.) = \n",
      "  1.8703e-02  2.0079e-02  1.3319e-02\n",
      "  1.3445e-02  8.5972e-03  5.1178e-03\n",
      " -4.9829e-03 -9.7513e-03 -4.4635e-03\n",
      "...     \n",
      "      â‹®  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      "  3.0037e-02  2.2150e-02  1.6669e-02\n",
      "  1.2098e-02  6.0631e-03  1.3050e-03\n",
      "  1.2871e-02  1.2528e-02  1.8927e-02\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      "  2.3062e-02  1.1691e-02  1.9048e-02\n",
      "  2.3560e-02  5.4955e-03  1.8966e-02\n",
      "  1.5328e-02  6.0644e-04  1.3019e-02\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      " -1.3646e-02 -8.9991e-03 -2.2615e-03\n",
      " -1.9990e-03 -1.8466e-03 -4.7327e-04\n",
      " -1.2601e-02 -1.3529e-02 -2.1462e-03\n",
      "    ... \n",
      "\n",
      "(509,509,.,.) = \n",
      " -6.2542e-03  2.8485e-03 -2.2351e-03\n",
      "  6.1509e-03  1.2996e-02  6.4575e-03\n",
      "  1.7797e-02  1.5100e-02  1.8097e-02\n",
      "\n",
      "(509,510,.,.) = \n",
      " -9.5429e-03 -1.2790e-02 -1.0741e-02\n",
      " -1.2037e-02 -1.4995e-02 -1.5169e-02\n",
      " -5.5194e-03 -7.7862e-03 -6.6505e-03\n",
      "\n",
      "(509,511,.,.) = \n",
      " -9.1402e-04 -6.4995e-03  3.5009e-03\n",
      "  1.3887e-02  1.6742e-03  8.8002e-03\n",
      "  2.1755e-02  1.3032e-02  1.9909e-02\n",
      "      â‹®  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -1.5802e-02 -7.0679e-03 -7.3501e-03\n",
      " -2.2411e-02 -1.4427e-02 -2.0362e-02\n",
      " -2.0127e-02 -2.1373e-02 -2.9535e-02\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      " -5.4015e-03 -8.8148e-03 -6.2057e-03\n",
      "  1.6595e-03  6.7143e-03  5.5742e-03\n",
      " -4.8688e-03  3.5798e-03  4.2011e-03\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      "  7.4703e-03  1.4123e-02  6.6948e-03\n",
      " -3.6537e-03 -4.0611e-04 -3.2095e-03\n",
      " -1.6227e-02 -1.3900e-02 -1.1405e-02\n",
      "    ... \n",
      "\n",
      "(510,509,.,.) = \n",
      "  1.3705e-02  2.0156e-02  3.4179e-02\n",
      "  1.9682e-02  7.2794e-03  2.4422e-02\n",
      "  2.1427e-02  2.3641e-02  4.1680e-02\n",
      "\n",
      "(510,510,.,.) = \n",
      " -1.2316e-02 -1.1480e-02 -2.1500e-02\n",
      " -1.8474e-03  1.4959e-03 -4.1257e-03\n",
      "  1.6982e-03  5.4100e-03  3.5914e-04\n",
      "\n",
      "(510,511,.,.) = \n",
      " -7.1422e-03 -1.2085e-02 -1.7181e-02\n",
      " -1.0105e-03 -3.8114e-03 -1.4981e-02\n",
      "  1.1654e-03 -9.1660e-03 -1.6541e-02\n",
      "      â‹®  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      " -1.5663e-02 -7.7784e-03 -4.1013e-03\n",
      " -1.6814e-03 -1.3965e-02 -1.1734e-03\n",
      " -1.1231e-02 -1.4530e-02 -4.5787e-03\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      "  1.5992e-02  3.1737e-02  3.3408e-02\n",
      "  2.6099e-02  3.4654e-02  2.9341e-02\n",
      "  9.0493e-04  1.3197e-02 -5.3442e-03\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      " -5.7927e-03 -4.1839e-03 -2.2677e-03\n",
      " -2.4124e-03 -1.0232e-02 -2.1683e-03\n",
      "  7.8198e-03  3.2852e-03  7.1552e-03\n",
      "    ... \n",
      "\n",
      "(511,509,.,.) = \n",
      " -3.8305e-03  8.4054e-03 -7.6319e-03\n",
      "  1.0255e-02  1.2925e-02  9.9205e-03\n",
      "  2.9790e-02  1.4236e-02  1.7698e-02\n",
      "\n",
      "(511,510,.,.) = \n",
      "  1.6758e-02  2.2814e-02  2.9799e-02\n",
      "  5.4145e-05  9.7495e-03  2.1958e-02\n",
      " -1.0225e-02 -4.4051e-03 -1.9306e-03\n",
      "\n",
      "(511,511,.,.) = \n",
      "  1.4948e-02  4.5217e-03  8.5707e-03\n",
      "  6.4393e-04  2.9769e-04  5.7453e-03\n",
      " -1.4612e-02 -8.1335e-03 -1.3466e-02\n",
      "[torch.FloatTensor of size 512x512x3x3]\n",
      "\n",
      "Parameter containing:\n",
      "-4.4551e-08\n",
      " 1.6699e-08\n",
      " 9.3026e-09\n",
      "-6.4536e-08\n",
      "-1.2949e-08\n",
      " 7.0923e-09\n",
      "-1.0645e-07\n",
      "-7.0371e-08\n",
      "-1.0230e-07\n",
      "-7.3355e-08\n",
      "-2.6698e-08\n",
      "-6.4394e-08\n",
      "-3.0659e-08\n",
      "-2.6292e-08\n",
      "-7.7410e-08\n",
      "-3.4860e-08\n",
      "-9.0725e-08\n",
      "-1.2291e-07\n",
      "-3.7078e-08\n",
      "-3.0936e-08\n",
      "-4.4562e-08\n",
      "-5.9565e-08\n",
      "-3.9442e-08\n",
      " 1.3361e-08\n",
      " 1.3167e-08\n",
      "-6.7793e-08\n",
      " 5.3129e-09\n",
      "-1.5057e-07\n",
      "-3.2535e-08\n",
      "-7.8520e-09\n",
      "-4.8460e-08\n",
      "-4.8173e-08\n",
      "-1.3087e-08\n",
      "-1.6639e-08\n",
      "-5.9867e-08\n",
      "-4.4211e-08\n",
      "-4.4312e-08\n",
      "-6.0996e-08\n",
      "-3.3872e-08\n",
      " 2.1367e-09\n",
      " 1.3661e-16\n",
      "-3.1711e-08\n",
      "-3.4863e-08\n",
      "-3.5334e-08\n",
      "-2.9957e-08\n",
      "-9.4980e-08\n",
      "-1.0600e-07\n",
      "-4.5474e-10\n",
      "-1.2634e-07\n",
      "-1.4519e-08\n",
      "-6.2626e-08\n",
      "-8.4349e-08\n",
      " 2.7549e-08\n",
      "-3.4712e-08\n",
      "-3.5200e-08\n",
      "-2.8899e-08\n",
      "-7.5989e-08\n",
      "-6.8002e-08\n",
      "-6.2128e-08\n",
      "-4.8515e-08\n",
      "-5.5816e-08\n",
      "-7.7266e-08\n",
      "-8.0073e-08\n",
      "-4.2173e-08\n",
      "-9.5783e-08\n",
      " 1.0922e-08\n",
      "-3.8677e-08\n",
      "-4.6611e-08\n",
      "-4.9189e-08\n",
      "-4.4288e-08\n",
      "-4.4676e-08\n",
      "-1.0655e-07\n",
      "-4.7189e-08\n",
      "-1.1566e-09\n",
      "-1.4492e-07\n",
      "-8.5677e-08\n",
      " 1.7563e-08\n",
      "-6.6628e-08\n",
      "-5.9700e-08\n",
      "-2.8061e-08\n",
      "-5.7423e-08\n",
      "-7.4347e-08\n",
      "-9.9848e-08\n",
      "-8.2586e-08\n",
      " 1.4711e-08\n",
      "-8.2401e-08\n",
      "-2.0173e-08\n",
      "-2.7814e-08\n",
      "-6.1166e-08\n",
      " 4.0104e-10\n",
      " 2.2985e-08\n",
      "-1.8146e-08\n",
      " 9.1370e-08\n",
      "-1.6160e-08\n",
      "-4.1723e-08\n",
      " 2.5472e-09\n",
      "-4.4076e-08\n",
      "-1.1063e-07\n",
      "-9.7258e-08\n",
      "-8.7804e-08\n",
      "-3.5772e-09\n",
      "-4.7253e-08\n",
      "-6.5099e-08\n",
      "-6.6407e-08\n",
      "-4.6357e-08\n",
      "-1.5697e-09\n",
      "-4.3321e-08\n",
      "-4.9564e-08\n",
      "-7.6552e-08\n",
      "-3.3077e-08\n",
      "-5.0094e-08\n",
      "-1.0051e-08\n",
      "-1.2606e-07\n",
      "-7.8907e-08\n",
      "-8.3286e-08\n",
      "-5.2929e-08\n",
      "-1.1121e-07\n",
      " 4.2625e-10\n",
      "-1.1347e-08\n",
      "-7.6850e-08\n",
      "-5.4752e-08\n",
      "-6.7958e-09\n",
      "-1.2617e-17\n",
      "-6.4311e-08\n",
      "-3.9554e-08\n",
      "-7.7794e-08\n",
      "-1.6433e-08\n",
      " 7.3885e-10\n",
      "-5.4085e-08\n",
      "-2.6128e-09\n",
      "-2.6122e-08\n",
      "-1.7149e-07\n",
      "-4.8574e-08\n",
      "-3.4907e-08\n",
      "-5.1380e-08\n",
      "-5.7586e-08\n",
      "-1.0195e-07\n",
      "-1.4417e-08\n",
      " 1.0672e-08\n",
      "-6.9398e-08\n",
      " 1.0370e-08\n",
      "-9.5384e-08\n",
      "-8.3705e-08\n",
      "-6.2277e-08\n",
      "-1.4077e-08\n",
      "-9.8819e-08\n",
      "-1.2982e-07\n",
      "-7.9758e-08\n",
      "-3.7481e-08\n",
      "-7.6500e-08\n",
      "-7.1485e-08\n",
      "-5.6518e-08\n",
      "-2.6829e-08\n",
      "-9.5175e-08\n",
      "-5.9182e-08\n",
      "-9.5959e-08\n",
      "-4.7872e-08\n",
      "-7.0612e-08\n",
      "-4.2136e-08\n",
      "-8.3878e-08\n",
      "-6.0794e-08\n",
      "-7.8082e-08\n",
      "-1.4293e-08\n",
      "-8.0582e-08\n",
      "-8.9265e-08\n",
      "-2.6300e-08\n",
      "-2.5179e-08\n",
      "-3.6054e-08\n",
      "-8.6493e-08\n",
      "-7.6530e-08\n",
      "-6.5190e-08\n",
      "-1.2360e-08\n",
      "-2.5654e-08\n",
      "-1.3244e-08\n",
      "-9.8411e-08\n",
      "-5.1297e-08\n",
      "-5.0154e-09\n",
      "-5.4170e-09\n",
      "-7.2407e-08\n",
      "-3.4829e-08\n",
      "-9.5880e-09\n",
      "-8.9929e-08\n",
      "-7.0669e-08\n",
      "-4.2118e-08\n",
      "-5.9484e-08\n",
      "-2.0264e-10\n",
      "-3.7726e-08\n",
      "-3.2541e-08\n",
      "-4.9225e-08\n",
      "-8.7545e-08\n",
      "-5.3323e-08\n",
      "-4.7448e-08\n",
      "-4.8143e-08\n",
      "-8.0636e-08\n",
      "-9.5595e-08\n",
      " 4.3659e-08\n",
      " 6.7545e-09\n",
      "-1.6631e-08\n",
      "-5.7017e-08\n",
      "-4.9172e-08\n",
      "-1.8044e-08\n",
      "-8.8061e-08\n",
      "-5.1180e-08\n",
      "-7.1261e-08\n",
      "-1.0624e-07\n",
      "-8.7292e-08\n",
      " 7.4997e-08\n",
      "-7.5820e-08\n",
      "-4.4064e-08\n",
      "-6.8187e-08\n",
      "-5.9769e-08\n",
      "-1.0029e-07\n",
      "-5.3788e-08\n",
      "-1.5131e-07\n",
      "-1.6188e-09\n",
      "-3.3193e-08\n",
      "-4.5582e-08\n",
      "-6.8009e-08\n",
      " 8.0136e-09\n",
      "-5.7137e-08\n",
      "-8.4590e-08\n",
      "-9.4376e-08\n",
      "-2.6281e-08\n",
      "-3.5227e-08\n",
      "-9.4332e-08\n",
      "-8.0876e-08\n",
      "-8.1721e-09\n",
      "-4.5672e-08\n",
      "-3.5171e-08\n",
      "-6.4864e-08\n",
      "-1.5223e-08\n",
      "-6.3212e-08\n",
      "-7.5326e-08\n",
      "-3.6848e-08\n",
      "-9.2600e-08\n",
      "-4.6948e-09\n",
      "-6.1191e-08\n",
      " 1.1243e-08\n",
      "-4.7828e-08\n",
      "-3.8212e-08\n",
      "-2.9743e-08\n",
      "-5.9035e-08\n",
      "-3.6557e-08\n",
      "-4.3401e-09\n",
      "-3.8956e-08\n",
      "-3.1095e-08\n",
      "-2.2032e-08\n",
      "-7.7204e-08\n",
      "-6.6125e-08\n",
      "-5.6664e-08\n",
      "-5.5376e-08\n",
      "-6.6223e-08\n",
      "-6.4703e-08\n",
      "-2.6738e-08\n",
      "-7.1627e-08\n",
      "-3.5300e-08\n",
      "-3.7553e-08\n",
      "-3.6513e-08\n",
      "-1.9924e-08\n",
      "-5.0042e-08\n",
      "-2.3779e-08\n",
      "-6.1688e-08\n",
      "-5.0838e-08\n",
      "-1.0841e-07\n",
      "-8.3605e-08\n",
      "-5.8312e-08\n",
      "-3.3136e-09\n",
      "-1.9739e-08\n",
      "-1.9996e-09\n",
      "-2.7475e-08\n",
      "-2.2637e-09\n",
      "-1.4010e-07\n",
      "-7.7266e-08\n",
      "-3.8381e-08\n",
      "-6.0644e-08\n",
      "-3.4463e-08\n",
      "-3.7843e-08\n",
      "-1.3166e-09\n",
      "-9.9414e-08\n",
      "-1.0170e-07\n",
      "-4.7578e-08\n",
      "-5.5870e-08\n",
      "-4.2221e-08\n",
      "-1.0348e-07\n",
      "-3.4144e-08\n",
      "-5.2409e-08\n",
      "-8.8797e-09\n",
      "-6.1140e-08\n",
      "-1.2663e-07\n",
      "-1.1801e-07\n",
      "-6.0819e-08\n",
      "-1.0980e-07\n",
      "-3.5478e-08\n",
      "-5.8238e-08\n",
      " 1.2097e-08\n",
      "-1.2767e-07\n",
      "-6.0471e-08\n",
      " 1.5348e-08\n",
      " 3.7308e-08\n",
      "-7.6955e-08\n",
      "-7.3844e-08\n",
      "-2.7890e-08\n",
      "-1.0386e-07\n",
      "-5.7465e-08\n",
      "-2.5164e-08\n",
      "-3.3464e-08\n",
      "-4.9075e-08\n",
      "-5.3410e-08\n",
      "-1.5520e-08\n",
      "-5.9494e-08\n",
      "-1.7033e-07\n",
      "-4.0209e-08\n",
      "-6.9231e-08\n",
      "-7.5861e-08\n",
      "-1.2295e-07\n",
      "-5.6008e-08\n",
      "-5.7868e-08\n",
      "-6.9324e-08\n",
      "-1.8180e-08\n",
      "-3.2019e-08\n",
      "-4.8084e-08\n",
      "-2.4045e-07\n",
      "-1.0425e-07\n",
      "-5.5711e-08\n",
      "-3.0345e-08\n",
      "-8.1405e-08\n",
      "-1.0543e-07\n",
      "-1.0958e-07\n",
      "-3.9678e-08\n",
      " 2.4626e-08\n",
      "-6.5216e-08\n",
      "-6.5218e-08\n",
      "-5.6740e-08\n",
      " 1.6603e-09\n",
      "-6.0069e-08\n",
      "-4.2625e-09\n",
      "-2.2651e-08\n",
      " 6.3660e-09\n",
      "-6.4611e-08\n",
      "-4.8847e-08\n",
      "-7.0621e-08\n",
      "-8.4978e-08\n",
      "-8.6002e-08\n",
      "-5.7926e-08\n",
      "-2.7856e-08\n",
      "-8.6153e-09\n",
      "-1.0440e-08\n",
      "-6.4406e-08\n",
      "-8.4221e-08\n",
      "-3.5861e-08\n",
      " 8.7440e-09\n",
      "-3.7404e-08\n",
      "-9.4742e-08\n",
      "-2.3385e-08\n",
      "-1.0809e-07\n",
      " 1.4850e-08\n",
      "-2.7701e-08\n",
      "-3.6364e-08\n",
      "-8.8949e-08\n",
      "-3.4811e-08\n",
      "-3.1756e-08\n",
      "-5.7297e-08\n",
      "-6.3823e-08\n",
      "-7.0143e-08\n",
      "-4.3347e-08\n",
      "-5.6099e-08\n",
      "-7.0463e-08\n",
      "-4.2587e-08\n",
      "-1.9480e-08\n",
      " 6.2544e-09\n",
      " 4.2217e-08\n",
      " 4.5897e-08\n",
      "-8.6194e-09\n",
      "-9.9826e-09\n",
      "-2.0873e-08\n",
      " 2.3175e-08\n",
      " 1.7257e-08\n",
      "-2.8943e-08\n",
      "-7.0600e-08\n",
      "-4.8921e-08\n",
      "-3.8616e-08\n",
      "-3.3778e-08\n",
      "-1.0731e-07\n",
      " 1.1895e-08\n",
      "-5.9630e-08\n",
      "-1.0589e-08\n",
      " 1.7758e-08\n",
      "-8.1915e-08\n",
      "-3.4682e-08\n",
      "-7.2891e-08\n",
      "-5.0764e-08\n",
      "-1.2313e-07\n",
      " 2.6374e-08\n",
      " 5.4784e-08\n",
      "-3.5249e-08\n",
      " 1.8852e-08\n",
      "-4.6814e-08\n",
      "-2.4114e-08\n",
      "-7.3174e-08\n",
      "-2.3178e-08\n",
      "-1.0834e-08\n",
      "-3.5375e-08\n",
      "-9.6279e-08\n",
      "-5.6556e-08\n",
      "-5.7006e-08\n",
      "-6.0567e-08\n",
      "-7.4111e-08\n",
      "-8.0225e-08\n",
      "-6.8577e-08\n",
      "-1.3415e-07\n",
      "-7.2463e-08\n",
      "-6.3860e-08\n",
      "-5.4870e-08\n",
      "-2.8549e-08\n",
      "-4.1243e-08\n",
      "-9.5274e-08\n",
      " 8.5167e-09\n",
      "-1.8038e-09\n",
      "-8.2479e-08\n",
      "-6.3142e-08\n",
      "-8.3558e-08\n",
      "-7.4756e-08\n",
      "-3.3404e-08\n",
      "-1.6751e-07\n",
      "-1.3650e-07\n",
      "-9.0598e-08\n",
      "-2.5076e-08\n",
      "-1.0125e-07\n",
      "-9.1354e-08\n",
      "-2.9733e-08\n",
      "-8.1209e-08\n",
      "-4.4250e-08\n",
      "-7.7648e-08\n",
      "-6.5945e-08\n",
      " 1.8492e-08\n",
      "-1.1918e-07\n",
      "-4.3369e-08\n",
      "-4.4191e-09\n",
      "-5.8979e-08\n",
      "-9.2867e-08\n",
      "-7.1673e-08\n",
      "-6.9426e-08\n",
      "-3.2525e-08\n",
      "-4.1410e-08\n",
      "-1.5856e-08\n",
      "-4.7771e-08\n",
      "-2.5288e-08\n",
      "-8.2823e-08\n",
      "-1.2459e-07\n",
      "-8.3847e-08\n",
      "-5.9928e-09\n",
      "-5.9400e-08\n",
      "-5.0223e-08\n",
      "-3.5546e-08\n",
      "-3.4077e-08\n",
      "-5.4775e-08\n",
      "-9.0128e-08\n",
      "-9.0469e-08\n",
      "-7.1745e-08\n",
      "-4.7014e-08\n",
      "-4.5585e-08\n",
      "-4.5731e-08\n",
      "-7.5284e-08\n",
      "-7.0235e-08\n",
      "-5.4741e-08\n",
      "-2.8051e-08\n",
      "-6.1635e-08\n",
      "-8.5496e-09\n",
      "-6.0594e-08\n",
      "-1.0735e-07\n",
      "-1.0960e-07\n",
      "-1.2620e-09\n",
      " 1.3169e-08\n",
      "-1.0101e-08\n",
      "-5.7881e-08\n",
      "-4.8179e-08\n",
      "-2.7354e-08\n",
      "-7.8666e-08\n",
      "-2.6187e-08\n",
      "-1.0494e-07\n",
      "-3.8127e-09\n",
      "-5.3803e-08\n",
      "-5.3948e-08\n",
      "-1.2017e-07\n",
      "-9.0914e-08\n",
      "-2.7009e-08\n",
      "-2.0229e-08\n",
      "-3.7365e-08\n",
      "-7.3775e-08\n",
      "-5.0572e-08\n",
      "-9.1586e-08\n",
      "-6.8628e-08\n",
      "-6.8361e-08\n",
      " 2.2734e-08\n",
      "-7.0268e-08\n",
      "-1.0343e-07\n",
      "-7.2610e-08\n",
      "-9.0095e-08\n",
      " 5.1021e-09\n",
      "-2.7115e-08\n",
      "-9.4568e-08\n",
      "-5.0214e-08\n",
      "-6.6065e-08\n",
      "-7.9561e-08\n",
      "-3.3541e-08\n",
      "-6.7659e-08\n",
      "-5.1091e-08\n",
      "-1.2004e-08\n",
      "-8.7407e-08\n",
      "-9.2404e-08\n",
      "-3.0418e-08\n",
      "-1.2482e-07\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      " 7.0106e-01\n",
      " 7.4740e-01\n",
      " 8.0724e-01\n",
      " 8.0254e-01\n",
      " 7.9941e-01\n",
      " 7.5267e-01\n",
      " 8.1914e-01\n",
      " 9.1632e-01\n",
      " 1.0475e+00\n",
      " 9.3044e-01\n",
      " 6.5006e-01\n",
      " 7.0839e-01\n",
      " 7.7383e-01\n",
      " 8.2066e-01\n",
      " 7.2413e-01\n",
      " 7.3905e-01\n",
      " 7.1703e-01\n",
      " 9.0948e-01\n",
      " 9.0049e-01\n",
      " 8.4024e-01\n",
      " 8.4899e-01\n",
      " 8.1374e-01\n",
      " 7.4030e-01\n",
      " 6.8847e-01\n",
      " 7.6467e-01\n",
      " 8.6093e-01\n",
      " 7.3159e-01\n",
      " 7.4845e-01\n",
      " 7.9703e-01\n",
      " 7.1165e-01\n",
      " 9.0598e-01\n",
      " 8.1442e-01\n",
      " 9.3713e-01\n",
      " 7.5797e-01\n",
      " 8.9940e-01\n",
      " 7.8815e-01\n",
      " 6.3828e-01\n",
      " 9.2347e-01\n",
      " 8.4473e-01\n",
      " 7.4567e-01\n",
      "-4.8444e-10\n",
      " 7.0387e-01\n",
      " 8.5570e-01\n",
      " 7.1369e-01\n",
      " 7.6072e-01\n",
      " 7.5317e-01\n",
      " 8.6623e-01\n",
      " 7.1909e-01\n",
      " 7.5933e-01\n",
      " 6.5773e-01\n",
      " 7.9550e-01\n",
      " 9.7448e-01\n",
      " 8.4809e-01\n",
      " 8.3016e-01\n",
      " 8.2490e-01\n",
      " 6.5880e-01\n",
      " 7.7453e-01\n",
      " 9.8836e-01\n",
      " 8.0458e-01\n",
      " 7.6852e-01\n",
      " 7.8117e-01\n",
      " 7.5716e-01\n",
      " 8.9456e-01\n",
      " 6.2162e-01\n",
      " 8.3105e-01\n",
      " 7.6970e-01\n",
      " 9.0529e-01\n",
      " 8.5863e-01\n",
      " 8.8248e-01\n",
      " 8.0618e-01\n",
      " 7.5543e-01\n",
      " 5.1523e-01\n",
      " 7.6986e-01\n",
      " 8.1419e-01\n",
      " 8.9414e-01\n",
      " 7.6859e-01\n",
      " 7.1955e-01\n",
      " 8.4130e-01\n",
      " 7.5015e-01\n",
      " 8.7499e-01\n",
      " 7.1584e-01\n",
      " 8.1737e-01\n",
      " 6.7909e-01\n",
      " 8.0571e-01\n",
      " 7.2465e-01\n",
      " 7.8003e-01\n",
      " 8.2313e-01\n",
      " 7.8951e-01\n",
      " 6.9430e-01\n",
      " 8.0154e-01\n",
      " 7.4197e-01\n",
      " 6.6283e-01\n",
      "-8.9117e-01\n",
      " 7.6646e-01\n",
      " 9.3126e-01\n",
      " 8.3888e-01\n",
      " 7.4081e-01\n",
      " 7.6373e-01\n",
      " 9.2356e-01\n",
      " 8.5212e-01\n",
      " 6.5639e-01\n",
      " 1.0102e+00\n",
      " 7.9515e-01\n",
      " 8.1497e-01\n",
      " 7.5595e-01\n",
      " 7.2494e-01\n",
      " 7.0986e-01\n",
      " 8.9654e-01\n",
      " 8.9869e-01\n",
      " 7.6693e-01\n",
      " 7.5110e-01\n",
      " 7.9423e-01\n",
      " 1.0069e+00\n",
      " 8.9090e-01\n",
      " 7.5438e-01\n",
      " 7.4612e-01\n",
      " 8.6062e-01\n",
      " 8.5537e-01\n",
      " 8.7213e-01\n",
      " 7.8747e-01\n",
      " 8.1288e-01\n",
      " 8.0751e-01\n",
      " 2.4004e-10\n",
      " 8.4492e-01\n",
      " 6.8344e-01\n",
      " 1.0524e+00\n",
      " 6.1657e-01\n",
      " 7.2640e-01\n",
      " 7.9940e-01\n",
      " 7.5218e-01\n",
      " 8.4841e-01\n",
      " 6.8375e-01\n",
      " 8.1987e-01\n",
      " 8.8545e-01\n",
      " 8.3235e-01\n",
      " 8.4500e-01\n",
      " 6.9925e-01\n",
      " 7.0758e-01\n",
      " 7.0623e-01\n",
      " 8.6149e-01\n",
      " 7.3700e-01\n",
      " 7.4784e-01\n",
      " 7.2755e-01\n",
      " 7.2319e-01\n",
      " 7.9556e-01\n",
      " 8.3904e-01\n",
      " 9.2738e-01\n",
      " 6.7753e-01\n",
      " 7.5682e-01\n",
      " 9.9362e-01\n",
      " 7.4617e-01\n",
      " 6.6097e-01\n",
      " 8.6227e-01\n",
      " 8.1478e-01\n",
      " 7.6832e-01\n",
      " 8.2876e-01\n",
      " 8.0163e-01\n",
      " 7.6680e-01\n",
      " 7.6030e-01\n",
      " 9.4789e-01\n",
      " 8.7133e-01\n",
      " 7.3944e-01\n",
      " 8.0145e-01\n",
      " 8.8541e-01\n",
      " 9.1298e-01\n",
      " 7.4202e-01\n",
      " 8.4157e-01\n",
      " 8.0894e-01\n",
      " 9.9050e-01\n",
      " 8.7839e-01\n",
      " 6.7063e-01\n",
      " 7.0390e-01\n",
      " 7.9004e-01\n",
      " 7.5700e-01\n",
      " 8.0889e-01\n",
      " 7.6875e-01\n",
      " 8.3106e-01\n",
      " 6.9349e-01\n",
      " 1.0675e+00\n",
      " 7.7525e-01\n",
      " 7.6865e-01\n",
      " 8.2632e-01\n",
      " 7.6693e-01\n",
      " 6.1189e-01\n",
      " 7.6966e-01\n",
      " 8.2457e-01\n",
      " 8.8384e-01\n",
      " 8.4167e-01\n",
      " 7.7967e-01\n",
      " 7.3382e-01\n",
      " 7.7160e-01\n",
      " 8.4221e-01\n",
      " 7.6037e-01\n",
      " 7.5657e-01\n",
      " 7.1302e-01\n",
      " 7.5363e-01\n",
      " 8.3695e-01\n",
      " 7.3632e-01\n",
      " 6.9904e-01\n",
      " 9.8389e-01\n",
      " 8.2499e-01\n",
      " 1.0210e+00\n",
      " 7.3728e-01\n",
      " 7.8715e-01\n",
      " 7.9578e-01\n",
      " 7.7223e-01\n",
      " 6.9099e-01\n",
      " 7.3806e-01\n",
      " 8.4629e-01\n",
      " 8.7377e-01\n",
      " 9.3007e-01\n",
      " 8.0357e-01\n",
      " 8.4978e-01\n",
      " 7.7541e-01\n",
      " 7.6537e-01\n",
      " 6.5719e-01\n",
      " 8.1706e-01\n",
      " 7.1637e-01\n",
      " 7.0878e-01\n",
      " 7.4419e-01\n",
      " 8.1491e-01\n",
      " 7.6112e-01\n",
      " 6.9165e-01\n",
      " 7.3464e-01\n",
      " 7.0145e-01\n",
      " 7.3344e-01\n",
      " 7.4268e-01\n",
      " 9.1519e-01\n",
      " 8.0413e-01\n",
      " 8.3250e-01\n",
      " 6.4930e-01\n",
      " 9.2045e-01\n",
      " 9.2463e-01\n",
      " 8.0912e-01\n",
      " 7.8465e-01\n",
      " 7.7499e-01\n",
      " 7.1018e-01\n",
      " 6.9237e-01\n",
      " 6.9682e-01\n",
      " 1.0779e+00\n",
      " 7.5959e-01\n",
      " 9.1556e-01\n",
      " 7.7950e-01\n",
      " 8.7284e-01\n",
      " 8.2389e-01\n",
      " 7.8176e-01\n",
      " 7.6627e-01\n",
      " 5.9220e-01\n",
      " 7.3352e-01\n",
      " 7.5478e-01\n",
      " 7.4822e-01\n",
      " 7.1913e-01\n",
      " 6.8350e-01\n",
      " 7.4602e-01\n",
      " 7.1430e-01\n",
      " 6.9991e-01\n",
      " 8.4156e-01\n",
      " 8.4319e-01\n",
      " 7.2852e-01\n",
      " 8.2587e-01\n",
      " 6.7592e-01\n",
      " 9.0233e-01\n",
      " 7.2537e-01\n",
      " 9.8108e-01\n",
      " 6.9122e-01\n",
      " 8.3239e-01\n",
      " 7.7169e-01\n",
      " 9.9518e-01\n",
      " 8.6986e-01\n",
      " 7.0552e-01\n",
      " 8.2256e-01\n",
      " 7.6152e-01\n",
      " 5.9848e-01\n",
      " 8.5582e-01\n",
      " 8.5217e-01\n",
      " 7.0098e-01\n",
      " 9.1372e-01\n",
      " 8.6684e-01\n",
      " 1.0203e+00\n",
      " 7.9392e-01\n",
      " 7.3642e-01\n",
      " 9.7997e-01\n",
      " 8.1199e-01\n",
      " 7.6177e-01\n",
      " 7.1852e-01\n",
      " 7.4645e-01\n",
      " 8.0396e-01\n",
      " 8.2915e-01\n",
      " 8.6054e-01\n",
      " 7.9250e-01\n",
      " 6.5456e-01\n",
      " 7.3110e-01\n",
      " 8.8175e-01\n",
      " 8.9424e-01\n",
      " 7.9625e-01\n",
      " 8.4815e-01\n",
      " 7.1389e-01\n",
      " 6.8089e-01\n",
      " 7.4888e-01\n",
      " 7.2984e-01\n",
      " 8.4600e-01\n",
      " 7.9202e-01\n",
      " 8.7186e-01\n",
      " 7.5413e-01\n",
      " 9.3426e-01\n",
      " 8.2535e-01\n",
      " 7.7707e-01\n",
      " 7.9840e-01\n",
      " 8.5990e-01\n",
      " 8.2769e-01\n",
      " 7.8262e-01\n",
      " 7.8573e-01\n",
      " 9.4596e-01\n",
      " 7.0638e-01\n",
      " 7.0989e-01\n",
      " 8.1996e-01\n",
      " 7.2719e-01\n",
      " 8.2050e-01\n",
      " 9.1864e-01\n",
      " 8.3920e-01\n",
      " 9.9497e-01\n",
      " 6.3429e-01\n",
      " 7.4649e-01\n",
      " 9.3688e-01\n",
      " 8.0262e-01\n",
      " 7.7325e-01\n",
      " 7.8308e-01\n",
      " 1.0210e+00\n",
      " 7.4403e-01\n",
      " 9.5028e-01\n",
      " 7.6875e-01\n",
      " 7.2308e-01\n",
      " 7.9712e-01\n",
      " 1.0170e+00\n",
      " 9.0122e-01\n",
      " 7.6774e-01\n",
      " 6.9130e-01\n",
      " 7.8125e-01\n",
      " 7.4964e-01\n",
      " 8.8588e-01\n",
      " 8.7477e-01\n",
      " 6.9871e-01\n",
      " 8.0661e-01\n",
      " 6.8995e-01\n",
      " 6.6890e-01\n",
      " 7.8152e-01\n",
      " 7.3347e-01\n",
      " 8.1511e-01\n",
      " 7.8918e-01\n",
      " 7.7159e-01\n",
      " 6.9767e-01\n",
      " 6.7855e-01\n",
      " 7.6690e-01\n",
      " 7.2112e-01\n",
      " 6.6721e-01\n",
      " 7.7620e-01\n",
      " 7.3470e-01\n",
      " 8.1376e-01\n",
      " 8.9300e-01\n",
      " 6.5047e-01\n",
      " 6.8512e-01\n",
      " 7.8881e-01\n",
      " 6.2526e-01\n",
      " 8.2632e-01\n",
      " 7.9399e-01\n",
      " 7.3052e-01\n",
      " 7.6318e-01\n",
      " 8.3528e-01\n",
      " 8.1262e-01\n",
      " 8.2025e-01\n",
      " 9.0771e-01\n",
      " 7.6191e-01\n",
      " 7.4790e-01\n",
      " 8.0446e-01\n",
      " 6.9663e-01\n",
      " 6.3783e-01\n",
      " 7.1424e-01\n",
      " 8.4495e-01\n",
      " 7.9495e-01\n",
      " 7.6599e-01\n",
      " 7.9240e-01\n",
      " 8.1564e-01\n",
      " 7.7585e-01\n",
      " 7.7419e-01\n",
      " 7.8152e-01\n",
      " 9.8769e-01\n",
      " 7.3156e-01\n",
      " 8.2052e-01\n",
      " 8.0308e-01\n",
      " 7.5280e-01\n",
      " 8.3646e-01\n",
      " 8.6433e-01\n",
      " 7.3088e-01\n",
      " 6.8366e-01\n",
      " 9.6226e-01\n",
      " 7.8789e-01\n",
      " 7.1598e-01\n",
      " 7.5121e-01\n",
      " 8.0727e-01\n",
      " 7.0928e-01\n",
      " 8.3835e-01\n",
      " 6.8418e-01\n",
      " 8.2613e-01\n",
      " 6.9621e-01\n",
      " 6.5661e-01\n",
      " 7.8678e-01\n",
      " 8.4454e-01\n",
      " 6.6970e-01\n",
      " 7.8036e-01\n",
      " 7.6493e-01\n",
      " 8.6229e-01\n",
      " 8.2483e-01\n",
      " 7.3172e-01\n",
      " 6.5239e-01\n",
      " 7.8273e-01\n",
      " 7.3892e-01\n",
      " 6.1039e-01\n",
      " 7.2193e-01\n",
      " 7.2764e-01\n",
      " 7.2597e-01\n",
      " 6.2887e-01\n",
      " 7.2428e-01\n",
      " 7.7360e-01\n",
      " 7.8031e-01\n",
      " 8.5959e-01\n",
      " 8.2091e-01\n",
      " 9.0168e-01\n",
      " 9.3519e-01\n",
      " 7.8543e-01\n",
      " 7.1994e-01\n",
      " 8.4809e-01\n",
      " 8.3129e-01\n",
      " 8.2526e-01\n",
      " 8.3022e-01\n",
      " 8.3895e-01\n",
      " 8.1155e-01\n",
      " 8.5066e-01\n",
      " 7.8157e-01\n",
      " 7.3587e-01\n",
      " 6.9833e-01\n",
      " 7.1944e-01\n",
      " 7.0679e-01\n",
      " 9.9178e-01\n",
      " 8.0503e-01\n",
      " 7.2276e-01\n",
      " 9.3854e-01\n",
      " 7.2356e-01\n",
      " 6.7177e-01\n",
      " 8.2482e-01\n",
      " 8.9573e-01\n",
      " 8.2450e-01\n",
      " 8.8120e-01\n",
      " 8.2066e-01\n",
      " 8.0005e-01\n",
      " 7.8997e-01\n",
      " 7.6402e-01\n",
      " 7.5379e-01\n",
      " 7.9826e-01\n",
      " 7.7909e-01\n",
      " 7.5648e-01\n",
      " 8.5517e-01\n",
      " 7.9874e-01\n",
      " 7.7629e-01\n",
      " 7.5809e-01\n",
      " 7.1414e-01\n",
      " 7.1969e-01\n",
      " 8.9525e-01\n",
      " 8.1372e-01\n",
      " 8.0732e-01\n",
      " 9.3689e-01\n",
      " 8.6756e-01\n",
      " 7.8504e-01\n",
      " 7.5388e-01\n",
      " 7.7796e-01\n",
      " 8.6869e-01\n",
      " 7.2231e-01\n",
      " 8.4178e-01\n",
      " 8.4553e-01\n",
      " 7.3109e-01\n",
      " 6.9832e-01\n",
      " 7.6793e-01\n",
      " 7.2662e-01\n",
      " 7.7419e-01\n",
      " 7.2383e-01\n",
      " 9.5003e-01\n",
      " 7.3802e-01\n",
      " 8.9117e-01\n",
      " 8.9664e-01\n",
      " 8.0579e-01\n",
      " 8.7793e-01\n",
      " 7.7391e-01\n",
      " 7.2663e-01\n",
      " 8.7345e-01\n",
      " 7.0385e-01\n",
      " 7.4284e-01\n",
      " 7.6380e-01\n",
      " 7.9598e-01\n",
      " 8.1917e-01\n",
      " 8.1655e-01\n",
      " 7.3432e-01\n",
      " 8.8720e-01\n",
      " 7.6671e-01\n",
      " 7.9930e-01\n",
      " 8.3622e-01\n",
      " 8.0888e-01\n",
      " 8.4270e-01\n",
      " 7.4558e-01\n",
      " 7.4425e-01\n",
      " 8.1363e-01\n",
      " 7.1007e-01\n",
      " 7.6535e-01\n",
      " 7.4941e-01\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-4.9671e-01\n",
      "-5.7062e-01\n",
      "-5.9410e-01\n",
      "-6.6760e-01\n",
      "-5.7932e-01\n",
      "-5.7294e-01\n",
      "-6.0126e-01\n",
      "-7.7451e-01\n",
      "-7.9347e-01\n",
      "-7.2223e-01\n",
      "-3.4414e-01\n",
      "-4.4300e-01\n",
      "-6.0675e-01\n",
      "-5.3324e-01\n",
      "-5.7200e-01\n",
      "-7.2681e-01\n",
      "-6.5955e-01\n",
      "-8.5746e-01\n",
      "-5.5114e-01\n",
      "-7.1681e-01\n",
      "-6.5128e-01\n",
      "-7.2183e-01\n",
      "-4.8522e-01\n",
      "-5.0915e-01\n",
      "-6.2108e-01\n",
      "-7.5477e-01\n",
      "-6.4554e-01\n",
      "-5.7384e-01\n",
      "-5.6024e-01\n",
      "-5.0809e-01\n",
      "-8.1132e-01\n",
      "-6.0614e-01\n",
      "-7.7385e-01\n",
      "-6.4860e-01\n",
      "-8.2970e-01\n",
      "-4.5658e-01\n",
      "-3.7057e-01\n",
      "-7.2305e-01\n",
      "-6.8721e-01\n",
      "-5.5173e-01\n",
      "-1.1639e-08\n",
      "-5.5458e-01\n",
      "-6.8500e-01\n",
      "-6.1161e-01\n",
      "-4.4618e-01\n",
      "-5.8885e-01\n",
      "-7.0430e-01\n",
      "-6.1141e-01\n",
      "-4.8224e-01\n",
      "-4.5938e-01\n",
      "-6.6513e-01\n",
      "-7.8406e-01\n",
      "-6.4957e-01\n",
      "-7.8653e-01\n",
      "-7.0642e-01\n",
      "-4.1723e-01\n",
      "-5.9673e-01\n",
      "-8.7254e-01\n",
      "-5.8176e-01\n",
      "-5.7931e-01\n",
      "-6.4123e-01\n",
      "-5.6042e-01\n",
      "-6.5499e-01\n",
      "-4.1740e-01\n",
      "-6.5564e-01\n",
      "-5.6190e-01\n",
      "-7.3617e-01\n",
      "-7.1414e-01\n",
      "-7.9667e-01\n",
      "-6.4989e-01\n",
      "-6.3503e-01\n",
      "-2.6591e-02\n",
      "-6.8446e-01\n",
      "-7.8423e-01\n",
      "-7.3829e-01\n",
      "-6.7870e-01\n",
      "-5.4019e-01\n",
      "-5.7449e-01\n",
      "-4.8229e-01\n",
      "-7.3824e-01\n",
      "-4.3045e-01\n",
      "-6.3292e-01\n",
      "-5.2629e-01\n",
      "-5.5910e-01\n",
      "-5.1604e-01\n",
      "-6.2107e-01\n",
      "-6.9498e-01\n",
      "-6.5753e-01\n",
      "-4.4303e-01\n",
      "-7.4056e-01\n",
      "-5.0392e-01\n",
      "-5.1574e-01\n",
      "-8.4689e-01\n",
      "-6.8264e-01\n",
      "-8.4577e-01\n",
      "-5.6453e-01\n",
      "-5.0454e-01\n",
      "-6.3935e-01\n",
      "-7.7277e-01\n",
      "-4.3332e-01\n",
      "-4.9438e-01\n",
      "-8.4719e-01\n",
      "-5.3829e-01\n",
      "-6.2899e-01\n",
      "-5.7205e-01\n",
      "-4.3293e-01\n",
      "-5.1371e-01\n",
      "-6.4843e-01\n",
      "-7.0095e-01\n",
      "-5.0717e-01\n",
      "-6.4626e-01\n",
      "-5.7117e-01\n",
      "-7.8642e-01\n",
      "-7.9612e-01\n",
      "-5.5890e-01\n",
      "-5.9186e-01\n",
      "-7.7571e-01\n",
      "-7.5742e-01\n",
      "-7.6739e-01\n",
      "-5.7388e-01\n",
      "-5.7174e-01\n",
      "-6.0781e-01\n",
      "-1.0898e-08\n",
      "-7.5650e-01\n",
      "-3.7075e-01\n",
      "-9.3081e-01\n",
      "-3.6733e-01\n",
      "-4.5596e-01\n",
      "-6.4666e-01\n",
      "-5.8156e-01\n",
      "-7.3127e-01\n",
      "-3.6469e-01\n",
      "-7.1956e-01\n",
      "-6.6044e-01\n",
      "-7.1401e-01\n",
      "-7.6190e-01\n",
      "-4.6122e-01\n",
      "-4.4612e-01\n",
      "-6.1700e-01\n",
      "-7.3692e-01\n",
      "-5.3257e-01\n",
      "-5.9850e-01\n",
      "-5.8022e-01\n",
      "-5.5974e-01\n",
      "-6.6632e-01\n",
      "-6.3043e-01\n",
      "-7.6289e-01\n",
      "-3.5694e-01\n",
      "-5.6228e-01\n",
      "-2.4764e-01\n",
      "-6.3472e-01\n",
      "-4.5980e-01\n",
      "-8.2009e-01\n",
      "-5.1342e-01\n",
      "-5.9632e-01\n",
      "-7.4702e-01\n",
      "-6.0433e-01\n",
      "-5.9218e-01\n",
      "-5.5374e-01\n",
      "-7.6139e-01\n",
      "-8.0673e-01\n",
      "-5.7698e-01\n",
      "-6.2785e-01\n",
      "-7.2794e-01\n",
      "-7.6510e-01\n",
      "-6.1667e-01\n",
      "-7.1696e-01\n",
      "-6.4114e-01\n",
      "-8.9225e-01\n",
      "-6.9023e-01\n",
      "-4.1554e-01\n",
      "-5.3217e-01\n",
      "-6.8032e-01\n",
      "-6.1902e-01\n",
      "-6.8148e-01\n",
      "-6.4565e-01\n",
      "-6.7914e-01\n",
      "-6.1774e-01\n",
      "-7.9878e-01\n",
      "-6.8741e-01\n",
      "-6.7595e-01\n",
      "-6.4679e-01\n",
      "-5.6725e-01\n",
      "-4.6307e-01\n",
      "-5.3578e-01\n",
      "-7.6362e-01\n",
      "-6.7650e-01\n",
      "-7.2130e-01\n",
      "-6.1116e-01\n",
      "-5.2994e-01\n",
      "-6.4484e-01\n",
      "-6.3435e-01\n",
      "-6.4404e-01\n",
      "-5.2836e-01\n",
      "-5.5299e-01\n",
      "-2.9812e-01\n",
      "-7.0235e-01\n",
      "-5.3417e-01\n",
      "-4.3268e-01\n",
      "-7.9701e-01\n",
      "-6.2707e-01\n",
      "-7.1983e-01\n",
      "-5.8455e-01\n",
      "-6.8662e-01\n",
      "-6.2653e-01\n",
      "-6.3710e-01\n",
      "-5.5214e-01\n",
      "-6.0655e-01\n",
      "-7.0074e-01\n",
      "-6.6656e-01\n",
      "-7.2619e-01\n",
      "-6.8466e-01\n",
      "-6.6912e-01\n",
      "-6.2453e-01\n",
      "-6.8981e-01\n",
      "-4.3678e-01\n",
      "-7.0536e-01\n",
      "-5.8113e-01\n",
      "-5.4304e-01\n",
      "-5.9501e-01\n",
      "-5.3302e-01\n",
      "-6.7207e-01\n",
      "-5.7623e-01\n",
      "-4.7271e-01\n",
      "-4.3314e-01\n",
      "-5.9923e-01\n",
      "-6.3249e-01\n",
      "-7.0991e-01\n",
      "-5.7909e-01\n",
      "-7.4376e-01\n",
      "-4.0724e-01\n",
      "-7.9533e-01\n",
      "-7.4854e-01\n",
      "-6.2601e-01\n",
      "-6.4095e-01\n",
      "-6.0631e-01\n",
      "-5.7167e-01\n",
      "-5.0794e-01\n",
      "-5.6679e-01\n",
      "-8.5444e-01\n",
      "-4.9143e-01\n",
      "-7.7397e-01\n",
      "-6.7301e-01\n",
      "-6.4224e-01\n",
      "-7.1725e-01\n",
      "-6.6408e-01\n",
      "-4.8904e-01\n",
      "-4.2368e-01\n",
      "-5.1763e-01\n",
      "-4.7135e-01\n",
      "-5.5083e-01\n",
      "-5.9317e-01\n",
      "-5.0049e-01\n",
      "-5.1949e-01\n",
      "-6.2670e-01\n",
      "-5.9398e-01\n",
      "-6.8704e-01\n",
      "-6.7875e-01\n",
      "-4.8681e-01\n",
      "-7.2115e-01\n",
      "-4.2665e-01\n",
      "-7.7290e-01\n",
      "-5.6743e-01\n",
      "-7.4526e-01\n",
      "-5.3477e-01\n",
      "-7.0850e-01\n",
      "-4.9695e-01\n",
      "-8.0623e-01\n",
      "-6.6424e-01\n",
      "-4.4175e-01\n",
      "-6.5469e-01\n",
      "-4.6477e-01\n",
      "-3.3507e-01\n",
      "-7.4536e-01\n",
      "-6.0941e-01\n",
      "-5.4682e-01\n",
      "-6.0100e-01\n",
      "-7.6565e-01\n",
      "-7.0246e-01\n",
      "-6.5573e-01\n",
      "-6.2901e-01\n",
      "-7.8279e-01\n",
      "-6.8840e-01\n",
      "-5.8492e-01\n",
      "-6.4522e-01\n",
      "-5.5775e-01\n",
      "-7.0385e-01\n",
      "-6.4841e-01\n",
      "-7.4719e-01\n",
      "-6.7572e-01\n",
      "-4.4202e-01\n",
      "-4.8513e-01\n",
      "-7.3370e-01\n",
      "-8.8628e-01\n",
      "-6.2196e-01\n",
      "-7.8791e-01\n",
      "-5.6954e-01\n",
      "-4.7483e-01\n",
      "-5.5947e-01\n",
      "-4.0998e-01\n",
      "-4.9115e-01\n",
      "-6.9479e-01\n",
      "-7.8600e-01\n",
      "-6.0632e-01\n",
      "-8.8017e-01\n",
      "-6.3155e-01\n",
      "-5.0255e-01\n",
      "-6.5619e-01\n",
      "-7.3371e-01\n",
      "-5.6603e-01\n",
      "-5.2627e-01\n",
      "-6.2915e-01\n",
      "-6.5355e-01\n",
      "-5.8173e-01\n",
      "-4.6873e-01\n",
      "-6.8731e-01\n",
      "-5.4646e-01\n",
      "-6.6784e-01\n",
      "-7.4432e-01\n",
      "-6.7480e-01\n",
      "-7.9838e-01\n",
      "-2.6066e-01\n",
      "-5.3279e-01\n",
      "-7.6163e-01\n",
      "-6.6832e-01\n",
      "-4.9665e-01\n",
      "-6.1265e-01\n",
      "-8.8988e-01\n",
      "-6.3412e-01\n",
      "-9.3898e-01\n",
      "-6.0738e-01\n",
      "-5.8189e-01\n",
      "-5.5398e-01\n",
      "-7.5921e-01\n",
      "-7.6619e-01\n",
      "-6.0898e-01\n",
      "-4.9347e-01\n",
      "-5.1889e-01\n",
      "-5.9214e-01\n",
      "-7.7130e-01\n",
      "-6.6519e-01\n",
      "-4.7666e-01\n",
      "-7.2744e-01\n",
      "-5.1584e-01\n",
      "-4.5615e-01\n",
      "-6.3136e-01\n",
      "-5.5377e-01\n",
      "-6.8049e-01\n",
      "-6.9773e-01\n",
      "-6.0927e-01\n",
      "-4.5963e-01\n",
      "-4.3496e-01\n",
      "-6.7829e-01\n",
      "-5.4162e-01\n",
      "-4.4787e-01\n",
      "-6.3410e-01\n",
      "-4.8032e-01\n",
      "-7.1278e-01\n",
      "-6.8968e-01\n",
      "-5.1704e-01\n",
      "-4.8496e-01\n",
      "-5.0229e-01\n",
      "-3.7566e-01\n",
      "-6.7352e-01\n",
      "-6.4573e-01\n",
      "-5.0195e-01\n",
      "-6.1923e-01\n",
      "-6.4293e-01\n",
      "-5.1553e-01\n",
      "-6.3728e-01\n",
      "-6.9291e-01\n",
      "-6.0742e-01\n",
      "-4.8591e-01\n",
      "-5.7573e-01\n",
      "-4.9730e-01\n",
      "-4.9770e-01\n",
      "-6.0506e-01\n",
      "-7.5510e-01\n",
      "-6.6825e-01\n",
      "-5.8358e-01\n",
      "-6.5932e-01\n",
      "-6.3940e-01\n",
      "-6.1272e-01\n",
      "-6.0718e-01\n",
      "-6.6365e-01\n",
      "-7.3829e-01\n",
      "-6.3404e-01\n",
      "-7.1143e-01\n",
      "-6.1870e-01\n",
      "-5.9598e-01\n",
      "-6.5616e-01\n",
      "-7.0894e-01\n",
      "-5.4211e-01\n",
      "-2.8647e-01\n",
      "-7.4108e-01\n",
      "-6.2371e-01\n",
      "-5.0112e-01\n",
      "-6.5082e-01\n",
      "-6.6659e-01\n",
      "-5.4401e-01\n",
      "-6.6071e-01\n",
      "-4.4019e-01\n",
      "-6.3605e-01\n",
      "-5.8485e-01\n",
      "-4.0663e-01\n",
      "-5.8681e-01\n",
      "-7.1390e-01\n",
      "-5.7450e-01\n",
      "-4.7056e-01\n",
      "-5.6036e-01\n",
      "-6.2850e-01\n",
      "-6.7823e-01\n",
      "-6.3496e-01\n",
      "-3.9854e-01\n",
      "-6.0337e-01\n",
      "-6.2137e-01\n",
      "-1.9767e-01\n",
      "-5.3823e-01\n",
      "-6.3707e-01\n",
      "-6.4609e-01\n",
      "-4.0919e-01\n",
      "-4.9202e-01\n",
      "-6.1799e-01\n",
      "-6.3573e-01\n",
      "-8.5039e-01\n",
      "-7.6696e-01\n",
      "-7.4202e-01\n",
      "-8.1083e-01\n",
      "-6.8495e-01\n",
      "-5.3870e-01\n",
      "-6.2701e-01\n",
      "-6.7632e-01\n",
      "-6.5956e-01\n",
      "-5.5420e-01\n",
      "-6.2033e-01\n",
      "-6.1250e-01\n",
      "-6.4931e-01\n",
      "-5.6073e-01\n",
      "-3.9749e-01\n",
      "-5.7476e-01\n",
      "-4.6183e-01\n",
      "-4.2121e-01\n",
      "-8.2380e-01\n",
      "-6.8452e-01\n",
      "-6.0627e-01\n",
      "-6.8743e-01\n",
      "-5.4534e-01\n",
      "-4.7259e-01\n",
      "-6.6769e-01\n",
      "-7.2410e-01\n",
      "-6.8041e-01\n",
      "-7.4628e-01\n",
      "-6.3602e-01\n",
      "-6.8557e-01\n",
      "-6.5058e-01\n",
      "-5.4393e-01\n",
      "-6.0141e-01\n",
      "-4.0272e-01\n",
      "-6.0917e-01\n",
      "-5.5188e-01\n",
      "-5.9744e-01\n",
      "-6.8011e-01\n",
      "-6.6680e-01\n",
      "-6.9586e-01\n",
      "-6.1397e-01\n",
      "-5.4578e-01\n",
      "-7.7037e-01\n",
      "-6.6533e-01\n",
      "-7.1920e-01\n",
      "-7.9455e-01\n",
      "-6.9584e-01\n",
      "-7.0233e-01\n",
      "-5.4157e-01\n",
      "-6.3197e-01\n",
      "-6.8505e-01\n",
      "-5.0604e-01\n",
      "-6.2229e-01\n",
      "-6.5391e-01\n",
      "-5.6736e-01\n",
      "-4.8843e-01\n",
      "-5.3588e-01\n",
      "-4.7030e-01\n",
      "-6.3352e-01\n",
      "-4.7367e-01\n",
      "-8.7567e-01\n",
      "-6.1294e-01\n",
      "-8.5809e-01\n",
      "-7.7543e-01\n",
      "-5.7385e-01\n",
      "-7.0127e-01\n",
      "-5.3905e-01\n",
      "-5.8842e-01\n",
      "-6.4269e-01\n",
      "-4.4013e-01\n",
      "-4.6999e-01\n",
      "-4.1792e-01\n",
      "-5.7787e-01\n",
      "-6.2054e-01\n",
      "-7.2347e-01\n",
      "-5.9411e-01\n",
      "-6.4603e-01\n",
      "-5.1306e-01\n",
      "-5.6539e-01\n",
      "-6.8916e-01\n",
      "-7.2372e-01\n",
      "-6.9259e-01\n",
      "-5.8556e-01\n",
      "-5.6707e-01\n",
      "-6.4395e-01\n",
      "-6.0019e-01\n",
      "-6.2843e-01\n",
      "-4.9615e-01\n",
      "[torch.FloatTensor of size 512]\n",
      "\n",
      "Parameter containing:\n",
      "-5.1433e-03  1.4447e-03 -2.0302e-03  ...  -4.2616e-03  3.2284e-03  5.4096e-03\n",
      "-1.4533e-03  3.6960e-03  4.0049e-04  ...  -3.0048e-03  1.6141e-03  1.5717e-03\n",
      " 1.1294e-04  1.3715e-03  1.5156e-03  ...   4.3445e-03 -4.8541e-04  3.7755e-03\n",
      "                ...                   â‹±                   ...                \n",
      "-3.9146e-03 -2.8939e-03 -4.4591e-03  ...   2.3788e-04  1.6971e-03  1.6045e-04\n",
      "-1.4698e-03 -4.5146e-03 -5.3634e-03  ...   1.6210e-03  7.7839e-04 -4.4860e-03\n",
      "-1.8253e-03  2.4467e-03  2.3490e-04  ...  -7.2082e-03 -4.5467e-03  7.9109e-04\n",
      "[torch.FloatTensor of size 4096x25088]\n",
      "\n",
      "Parameter containing:\n",
      " 4.2639e-02\n",
      " 7.1186e-03\n",
      " 1.0525e-01\n",
      "     â‹®     \n",
      " 2.1179e-02\n",
      "-5.8841e-02\n",
      " 1.8548e-02\n",
      "[torch.FloatTensor of size 4096]\n",
      "\n",
      "Parameter containing:\n",
      " 2.1287e-02  5.8711e-04  4.7286e-03  ...  -1.1499e-02  7.1668e-03  1.2486e-02\n",
      " 4.6511e-03 -2.3969e-02  6.1767e-03  ...   1.0963e-02  8.7579e-04  1.8766e-02\n",
      " 1.3253e-02 -3.6758e-03  8.6062e-03  ...   1.0737e-02  4.6565e-03  5.1872e-03\n",
      "                ...                   â‹±                   ...                \n",
      "-5.9511e-03 -1.1356e-02  2.5384e-03  ...  -1.1728e-02 -3.6874e-03 -1.3641e-02\n",
      " 1.5902e-02 -4.8517e-03  6.3238e-04  ...  -1.5051e-03  1.3808e-04  1.8150e-03\n",
      "-2.6469e-03 -1.2951e-02 -2.2374e-02  ...  -3.6973e-03 -7.1306e-03  1.9545e-03\n",
      "[torch.FloatTensor of size 4096x4096]\n",
      "\n",
      "Parameter containing:\n",
      " 2.8859e-02\n",
      " 1.0506e-01\n",
      " 1.2699e-01\n",
      "     â‹®     \n",
      " 1.0793e-01\n",
      " 7.3345e-02\n",
      " 9.5089e-02\n",
      "[torch.FloatTensor of size 4096]\n",
      "\n",
      "Parameter containing:\n",
      "-7.1171e-04  3.1785e-02 -1.8842e-02  ...  -1.5870e-03  4.0962e-02  1.9549e-03\n",
      "-1.7012e-02  4.7915e-02 -1.6017e-02  ...  -1.1342e-02  3.7112e-02 -2.4083e-06\n",
      " 1.9150e-03 -1.3026e-02 -6.5281e-03  ...  -1.3581e-02 -2.7002e-02  4.0695e-02\n",
      "                ...                   â‹±                   ...                \n",
      " 2.2554e-02  2.1627e-03 -5.9601e-03  ...  -2.0709e-02 -8.1781e-03 -1.1481e-02\n",
      "-8.4101e-03  2.8122e-03 -1.0132e-02  ...   1.6001e-02 -4.0399e-03  3.7749e-02\n",
      "-1.4075e-02 -3.9555e-03  2.6998e-02  ...  -1.8265e-02  3.6453e-03 -1.4508e-02\n",
      "[torch.FloatTensor of size 1000x4096]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -1.3794\n",
      " -1.7240\n",
      " -2.7464\n",
      "    â‹®   \n",
      "  1.4711\n",
      " -0.1662\n",
      "  0.1756\n",
      "[torch.FloatTensor of size 1000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vgg16 = pre_vgg.vgg16_bn(pretrained=True)\n",
    "for param in vgg16.parameters():\n",
    "    print(param.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for  i in net.features:\n",
    "#     print(i)\n",
    "#     break\n",
    "# for i in i.parameters():\n",
    "#     print(i)\n",
    "len(net.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.505 | Acc: 73.500% (943/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.502 | Acc: 76.636% (246/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.351 | Acc: 83.788% (1075/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.524 | Acc: 74.766% (240/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.282 | Acc: 88.854% (1140/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.484 | Acc: 66.044% (212/321)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.245 | Acc: 90.101% (1156/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.247 | Acc: 90.031% (289/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.197 | Acc: 92.985% (1193/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.576 | Acc: 71.028% (228/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.199 | Acc: 92.595% (1188/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.240 | Acc: 89.720% (288/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.157 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.282 | Acc: 88.162% (283/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.214 | Acc: 91.589% (294/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.144 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.214 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.141 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.212 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.116 | Acc: 95.947% (1231/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 92.212% (296/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.110 | Acc: 96.415% (1237/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.125 | Acc: 95.791% (1229/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.205 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.113 | Acc: 96.493% (1238/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.117 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.112 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.216 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.112 | Acc: 96.493% (1238/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.217 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.113 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.112 | Acc: 96.804% (1242/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.114 | Acc: 97.038% (1245/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.107 | Acc: 96.882% (1243/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.213 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.118 | Acc: 95.947% (1231/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.114 | Acc: 96.960% (1244/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.119 | Acc: 96.415% (1237/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.104 | Acc: 97.350% (1249/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.232 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.106 | Acc: 96.960% (1244/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.120 | Acc: 96.337% (1236/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.220 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.110 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.121 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.224 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.109 | Acc: 97.038% (1245/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.117 | Acc: 95.869% (1230/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.112 | Acc: 96.648% (1240/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.209 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.113 | Acc: 97.038% (1245/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.105 | Acc: 96.804% (1242/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.212 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.118 | Acc: 96.493% (1238/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.121 | Acc: 96.259% (1235/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.119 | Acc: 96.493% (1238/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.210 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.101 | Acc: 96.882% (1243/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.205 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.120 | Acc: 96.181% (1234/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.110 | Acc: 97.116% (1246/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.105 | Acc: 97.116% (1246/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.107 | Acc: 97.116% (1246/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.111 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.106 | Acc: 96.960% (1244/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.209 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.115 | Acc: 96.415% (1237/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.116 | Acc: 96.259% (1235/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.109 | Acc: 96.804% (1242/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:48\n",
      "[===>.....  12/ 81 .......]Step: 0ms| Tot: 4ms|Loss: 0.091 | Acc: 97.396% (187/192)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3043:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b987a19ba3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-e8875581f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3044:\n",
      "Process Process-3045:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "result=[]\n",
    "for i in range(1):\n",
    "    vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "    num = 256\n",
    "    vgg16_bn.classifier = nn.Sequential(\n",
    "                nn.Linear(512+1, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(num, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(num, 2)\n",
    "            )\n",
    "\n",
    "    net= vgg16_bn\n",
    "    # net.load_state_dict(torch.load('vgg_fcn_loss.pth'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # #Adam does not perform so good here   \n",
    "    # #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "    # optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "    scheduler = MultiStepLR(optimizer, [5,11,16], gamma=0.1)\n",
    "#     scheduler = MultiStepLR(optimizer, [10,18,26], gamma=0.1)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'max', patience =3,min_lr= 0.00001)\n",
    "    #5e-3 86\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "        net.cuda()\n",
    "    #     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "    #     cudnn.benchmark = True   \n",
    "\n",
    "    a = train(epoch=60,early_stopping =20)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21936874226246297, 91.27725856697819, 39),\n",
       " (0.21782404165773006, 91.58878504672897, 55),\n",
       " (0.22525541061924254, 90.96573208722741, 53)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24811193430535147, 90.03115264797508, 29),\n",
       " (0.21092650229314405, 91.58878504672897, 28),\n",
       " (0.22480700989007207, 90.65420560747664, 53)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.218 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 93.146% (299/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.082 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.225 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.086 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:11\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.091 | Acc: 96.648% (1240/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.084 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:16\n",
      "lr change from 0.000001 to 0.000000\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.038% (1245/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.090 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:21\n",
      "[========>  29/ 81 .......]Step: 0ms| Tot: 1s|Loss: 0.075 | Acc: 96.552% (448/464))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8923:\n",
      "Process Process-8924:\n",
      "Process Process-8925:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-77835431faca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e8875581f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "net.load_state_dict(torch.load('cnn_ang_loss.pth'))\n",
    "for i in vgg16_bn.features:\n",
    "    i.requires_grad = False\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #Adam does not perform so good here   \n",
    "# #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "# optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "scheduler = MultiStepLR(optimizer, [5,10,15], gamma=0.1)\n",
    "# scheduler = MultiStepLR(optimizer, [8,18], gamma=0.1)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'max', patience =10,min_lr= 0.0001)\n",
    "#5e-3 86\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "    net.cuda()\n",
    "#     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "#     cudnn.benchmark = True   \n",
    "\n",
    "train(epoch=250,early_stopping =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 3, 75, 75)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test_set)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k =np.stack(result).mean(axis=0)\n",
    "# #sub.shape\n",
    "# result[1].shape\n",
    "# np.concatenate(prob).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test_set['id']\n",
    "sub['is_iceberg'] =  np.concatenate(prob)\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>is_iceberg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.886197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg  is_iceberg2\n",
       "is_iceberg     1.000000     0.886197\n",
       "is_iceberg2    0.886197     1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp= pd.read_csv('submission3.csv') #0.0001 wd one\n",
    "sub['is_iceberg2'] = temp['is_iceberg']\n",
    "sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.573 | Acc: 67.186% (862/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 2.636 | Acc: 52.336% (168/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.390 | Acc: 82.697% (1061/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.734 | Acc: 52.336% (168/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.383 | Acc: 82.697% (1061/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.704 | Acc: 62.617% (201/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.299 | Acc: 87.685% (1125/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.460 | Acc: 81.620% (262/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.265 | Acc: 89.244% (1145/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.240 | Acc: 89.720% (288/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.222 | Acc: 90.413% (1160/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.215 | Acc: 90.654% (291/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.218 | Acc: 90.959% (1167/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.205 | Acc: 92.595% (1188/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.256 | Acc: 89.408% (287/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.179 | Acc: 93.998% (1206/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.213 | Acc: 91.277% (293/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.165 | Acc: 94.232% (1209/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.210 | Acc: 90.654% (291/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.180 | Acc: 93.765% (1203/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 90.966% (292/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.165 | Acc: 93.765% (1203/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.277% (293/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.166 | Acc: 94.388% (1211/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 90.966% (292/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.167 | Acc: 94.622% (1214/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.277% (293/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.176 | Acc: 93.453% (1199/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.161 | Acc: 94.622% (1214/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.246% (1222/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.589% (294/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.167 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.170 | Acc: 93.609% (1201/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.149 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.167 | Acc: 94.076% (1207/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.218 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.167 | Acc: 94.310% (1210/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.161 | Acc: 95.246% (1222/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.161 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 94.700% (1215/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.170 | Acc: 93.531% (1200/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.155 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 94.934% (1218/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 94.934% (1218/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.153 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.162 | Acc: 93.998% (1206/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.162 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.197 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.166 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.147 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.158 | Acc: 94.622% (1214/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.162 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.156 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.156 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.204 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.161 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.153 | Acc: 95.635% (1227/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 95.791% (1229/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.536 | Acc: 72.642% (932/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.419 | Acc: 79.751% (256/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.367 | Acc: 83.788% (1075/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.544 | Acc: 50.467% (162/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.309 | Acc: 86.282% (1107/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 2.054 | Acc: 55.452% (178/321)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.301 | Acc: 85.347% (1095/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.456 | Acc: 56.075% (180/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.312 | Acc: 86.594% (1111/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.266 | Acc: 88.474% (284/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.238 | Acc: 91.270% (1171/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.208 | Acc: 92.835% (298/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.199 | Acc: 92.751% (1190/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.230 | Acc: 90.966% (292/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.185 | Acc: 93.219% (1196/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 92.212% (296/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.180 | Acc: 92.985% (1193/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 92.835% (298/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.166 | Acc: 93.920% (1205/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.156 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.241 | Acc: 89.097% (286/321)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 94.934% (1218/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.197 | Acc: 92.835% (298/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 92.835% (298/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.138 | Acc: 96.259% (1235/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.204 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.163 | Acc: 94.076% (1207/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.194 | Acc: 93.146% (299/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.160 | Acc: 94.310% (1210/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.193 | Acc: 93.146% (299/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.195 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.141 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.138 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.193 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.140 | Acc: 95.713% (1228/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.191 | Acc: 93.458% (300/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.713% (1228/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.191 | Acc: 93.458% (300/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.150 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.191 | Acc: 93.458% (300/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.195 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.143 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.143 | Acc: 95.713% (1228/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.133 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.142 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 96.259% (1235/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.136 | Acc: 96.337% (1236/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.150 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.138 | Acc: 95.479% (1225/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.160 | Acc: 94.544% (1213/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.204 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.207 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.141 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.197 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.160 | Acc: 94.934% (1218/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.193 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.147 | Acc: 95.635% (1227/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.149 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.195 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.196 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.575 | Acc: 68.277% (876/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.804 | Acc: 52.648% (169/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.404 | Acc: 81.684% (1048/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.743 | Acc: 74.766% (240/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.344 | Acc: 84.489% (1084/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.379 | Acc: 80.374% (258/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.279 | Acc: 88.075% (1130/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.502 | Acc: 67.290% (216/321)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.258 | Acc: 89.322% (1146/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.255 | Acc: 88.785% (285/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.194 | Acc: 92.440% (1186/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.250 | Acc: 90.031% (289/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.177 | Acc: 93.375% (1198/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.266 | Acc: 87.850% (282/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.198 | Acc: 92.362% (1185/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.237 | Acc: 91.277% (293/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.179 | Acc: 93.609% (1201/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.234 | Acc: 92.212% (296/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.164 | Acc: 93.765% (1203/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.238 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.150 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.239 | Acc: 92.835% (298/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.134 | Acc: 95.791% (1229/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.237 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.147 | Acc: 95.168% (1221/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.237 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 95.246% (1222/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.238 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.131 | Acc: 96.571% (1239/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.240 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.713% (1228/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.244 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.122 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.240 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.869% (1230/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.246 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.135 | Acc: 95.635% (1227/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.245 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.135 | Acc: 95.869% (1230/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.238 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.134 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.238 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.401% (1224/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.238 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.129 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.239 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.128 | Acc: 97.272% (1248/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.244 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.129 | Acc: 96.259% (1235/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.237 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.139 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.237 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.137 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.239 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.120 | Acc: 96.726% (1241/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.241 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.133 | Acc: 96.337% (1236/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.239 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.143 | Acc: 95.557% (1226/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.249 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.133 | Acc: 95.947% (1231/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.240 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.556 | Acc: 69.914% (897/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.796 | Acc: 52.336% (168/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.465 | Acc: 76.383% (980/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.969 | Acc: 52.336% (168/321)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.353 | Acc: 84.489% (1084/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.578 | Acc: 57.009% (183/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.352 | Acc: 83.944% (1077/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.965 | Acc: 67.913% (218/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.264 | Acc: 88.465% (1135/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.081 | Acc: 63.240% (203/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.233 | Acc: 90.491% (1161/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.239 | Acc: 90.654% (291/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.196 | Acc: 92.985% (1193/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.317 | Acc: 85.670% (275/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.194 | Acc: 93.219% (1196/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.224 | Acc: 90.031% (289/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.189 | Acc: 92.595% (1188/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.223 | Acc: 90.343% (290/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.192 | Acc: 92.595% (1188/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.168 | Acc: 93.998% (1206/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.347 | Acc: 83.801% (269/321)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.204 | Acc: 92.212% (296/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.159 | Acc: 94.544% (1213/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 92.212% (296/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.155 | Acc: 94.544% (1213/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.171 | Acc: 94.310% (1210/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.147 | Acc: 94.778% (1216/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.277% (293/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.157 | Acc: 94.622% (1214/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.153 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.156 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.156 | Acc: 94.700% (1215/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.209 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.163 | Acc: 94.154% (1208/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.198 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.155 | Acc: 94.388% (1211/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.166 | Acc: 94.076% (1207/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.212 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.205 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.141 | Acc: 95.791% (1229/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.158 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.145 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.146 | Acc: 95.635% (1227/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.158 | Acc: 94.076% (1207/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.206 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.163 | Acc: 94.232% (1209/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.150 | Acc: 95.246% (1222/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.151 | Acc: 95.323% (1223/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.162 | Acc: 94.466% (1212/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 95.012% (1219/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.164 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.199 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.148 | Acc: 94.700% (1215/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.152 | Acc: 94.856% (1217/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.155 | Acc: 94.544% (1213/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.201 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.153 | Acc: 95.090% (1220/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.200 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.550 | Acc: 71.495% (918/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 1.916 | Acc: 52.500% (168/320)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.398 | Acc: 82.009% (1053/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 2.002 | Acc: 52.500% (168/320)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.313 | Acc: 85.748% (1101/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.289 | Acc: 85.938% (275/320)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.280 | Acc: 87.850% (1128/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.228 | Acc: 89.375% (286/320)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.225 | Acc: 90.265% (1159/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.569 | Acc: 72.500% (232/320)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.194 | Acc: 92.056% (1182/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.211 | Acc: 92.188% (295/320)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.187 | Acc: 93.302% (1198/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.202 | Acc: 91.250% (292/320)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.153 | Acc: 95.093% (1221/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.259 | Acc: 88.125% (282/320)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.154 | Acc: 94.626% (1215/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.258 | Acc: 89.062% (285/320)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.143 | Acc: 95.093% (1221/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.194 | Acc: 92.500% (296/320)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.136 | Acc: 94.938% (1219/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.234 | Acc: 89.375% (286/320)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.125 | Acc: 96.106% (1234/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.187 | Acc: 93.438% (299/320)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.117 | Acc: 96.807% (1243/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.189 | Acc: 93.125% (298/320)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.126 | Acc: 95.717% (1229/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.185 | Acc: 93.438% (299/320)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.130 | Acc: 95.717% (1229/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.187 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.124 | Acc: 96.495% (1239/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.183 | Acc: 93.438% (299/320)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.114 | Acc: 96.729% (1242/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.194 | Acc: 91.562% (293/320)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.110 | Acc: 96.573% (1240/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.203 | Acc: 91.875% (294/320)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.123 | Acc: 96.340% (1237/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.130 | Acc: 95.405% (1225/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.126 | Acc: 96.417% (1238/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.190 | Acc: 92.812% (297/320)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.115 | Acc: 96.262% (1236/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.188 | Acc: 93.125% (298/320)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.127 | Acc: 96.184% (1235/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.107 | Acc: 96.807% (1243/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.118 | Acc: 96.340% (1237/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.109 | Acc: 96.885% (1244/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.125% (298/320)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.119 | Acc: 95.872% (1231/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.185 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.107 | Acc: 97.040% (1246/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.112 | Acc: 96.807% (1243/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.107 | Acc: 97.430% (1251/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.185 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.133 | Acc: 95.639% (1228/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.189 | Acc: 92.812% (297/320)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.119 | Acc: 96.651% (1241/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.188 | Acc: 92.812% (297/320)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.117 | Acc: 96.028% (1233/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.186 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.113 | Acc: 96.417% (1238/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.188 | Acc: 93.125% (298/320)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.116 | Acc: 96.340% (1237/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.184 | Acc: 93.438% (299/320)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s2ms|Loss: 0.119 | Acc: 96.651% (1241/1284)\n",
      "[=========   5/  5 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.183 | Acc: 93.438% (299/320)\n"
     ]
    }
   ],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_ang_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_ang_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    ran_num = 9220\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        \n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(1):\n",
    "            vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [5,11,16], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=60,early_stopping =20)\n",
    "            with open(\"vgg_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase3, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg_ang_loss.pth vgg_ang_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "        \n",
    "        for g in range(1):\n",
    "            cmd = 'cp vgg_ang_loss{}.pth vgg_models/r3_5vgg{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        net.train()\n",
    "        for j,(batch_x, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            y = Variable(batch_y)\n",
    "            out = net(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                acc= best_acc+ 0.01\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg_loss.pth')\n",
    "            if loss_avg/total > val_loss:\n",
    "                e_s = 0\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x, val_y) in enumerate(val_load):\n",
    "        #len(val_x.size())==1\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        \n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        if len(out.size())==1: #in case it's one dimensional\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "        \n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    ran_num = 9021\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = StratifiedKFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del, train_y_del)):\n",
    "        \n",
    "        train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "        train_transform = T.Compose([\n",
    "            T.Normalize(train_mean, train_std)\n",
    "        ])\n",
    "        #af_train_X, af_train_y = data_aug(train_X_del[train_index], train_y_del[train_index])\n",
    "        #af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "        af_train_X, af_train_y = train_X_del[train_index], train_y_del[train_index]\n",
    "\n",
    "        train_dataset = iceberg_dataset(data= af_train_X, label=af_train_y, transform=train_transform)\n",
    "        val_dataset = iceberg_dataset(data= train_X_del[val_index], label=train_y_del[val_index], transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(2):\n",
    "            vgg16_bn = vgg_fcn.vgg16_bn(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            # vgg16_bn.avg= nn.Conv2d(512, 512, kernel_size=2,\n",
    "            #                                bias=False)\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Conv2d(512,512, kernel_size= 3,padding=1),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.6),\n",
    "                        nn.Conv2d(512, 2, kernel_size=3, padding=1,\n",
    "                                           bias=False),\n",
    "                        nn.AvgPool2d(3)\n",
    "                    )\n",
    "\n",
    "\n",
    "            net= vgg16_bn\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # #Adam does not perform so good here   \n",
    "            # #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "            # optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "            scheduler = MultiStepLR(optimizer, [5,11], gamma=0.1)\n",
    "            # scheduler = StepLR(optimizer, step_size=7, gamma=0.5)\n",
    "            # scheduler = ReduceLROnPlateau(optimizer, 'max', patience =10,min_lr= 0.0001)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            #     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "            #     cudnn.benchmark = True   \n",
    "\n",
    "            result = train(epoch=100,early_stopping =20)\n",
    "            with open(\"vgg_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '5folds, Phase1, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg_loss.pth vgg_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "\n",
    "        #actually an array\n",
    "        #also change here\n",
    "\n",
    "        \n",
    "        for g in range(2):\n",
    "            cmd = 'cp vgg_loss{}.pth vgg_models/r1_5vgg{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vgg_models/r1_5vgg0_00.pth', 'vgg_models/r1_5vgg0_11.pth', 'vgg_models/r1_5vgg0_20.pth', 'vgg_models/r1_5vgg0_31.pth', 'vgg_models/r1_5vgg0_40.pth']\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6mss\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "waiting_list=  \n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "vgg16_bn = vgg_fcn.vgg16_bn(pretrained=True)#copy.deepcopy(vgg16)\n",
    "# vgg16_bn.avg= nn.Conv2d(512, 512, kernel_size=2,\n",
    "#                                bias=False)\n",
    "\n",
    "# vgg16_bn.classifier = nn.Sequential(\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.6),\n",
    "#             nn.Linear(512, 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Conv2d(512,512, kernel_size= 3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Conv2d(512, 2, kernel_size=3, padding=1,\n",
    "                               bias=False),\n",
    "            nn.AvgPool2d(3)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "\n",
    "print(waiting_list)\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle=test.inc_angle.values.astype(np.float), transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "num = 256\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "# waiting_list=  ['r2_10vgg0_01.pth',\n",
    "# 'r2_10vgg0_10.pth',\n",
    "# 'r2_10vgg0_21.pth',\n",
    "# 'r2_10vgg0_31.pth',\n",
    "# 'r2_10vgg0_41.pth',\n",
    "# 'r2_10vgg0_51.pth',\n",
    "# 'r2_10vgg0_60.pth',\n",
    "# 'r2_10vgg0_70.pth',\n",
    "# 'r2_10vgg0_81.pth',\n",
    "# 'r2_10vgg0_90.pth'\n",
    "# ]\n",
    "# waiting_list = ['r2_5vgg0_01.pth',\n",
    "#                 'r2_5vgg0_10.pth',\n",
    "#                 'r2_5vgg0_21.pth',\n",
    "#                 'r2_5vgg0_30.pth',\n",
    "#                 'r2_5vgg0_40.pth']\n",
    "waiting_list = [i for i in os.listdir('vgg_models') if 'r3' in i]\n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "net= vgg16_bn\n",
    "\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(test_loader):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715401</td>\n",
       "      <td>0.684430</td>\n",
       "      <td>0.793467</td>\n",
       "      <td>0.894659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.715401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888102</td>\n",
       "      <td>0.682418</td>\n",
       "      <td>0.741345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.684430</td>\n",
       "      <td>0.888102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685948</td>\n",
       "      <td>0.679946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.793467</td>\n",
       "      <td>0.682418</td>\n",
       "      <td>0.685948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.894659</td>\n",
       "      <td>0.741345</td>\n",
       "      <td>0.679946</td>\n",
       "      <td>0.698735</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  is_iceberg4\n",
       "is_iceberg0     1.000000     0.715401     0.684430     0.793467     0.894659\n",
       "is_iceberg1     0.715401     1.000000     0.888102     0.682418     0.741345\n",
       "is_iceberg2     0.684430     0.888102     1.000000     0.685948     0.679946\n",
       "is_iceberg3     0.793467     0.682418     0.685948     1.000000     0.698735\n",
       "is_iceberg4     0.894659     0.741345     0.679946     0.698735     1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()\n",
    "# [i for i in os.listdir('vgg_models') if 'r3' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s494ms\n",
      "[=================== 132/132 ================>]  Step: 160ms | Tot: 27s661ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s644ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s598ms\n",
      "[=================== 132/132 ================>]  Step: 161ms | Tot: 27s668ms\n"
     ]
    }
   ],
   "source": [
    "#result_hist\n",
    "\n",
    "temp11 = pd.DataFrame()\n",
    "\n",
    "for i in range(5):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load('resnet34_acc%d.pth'%i))\n",
    "    net.cuda()\n",
    "\n",
    "    test = pd.read_json(BASE_dir + 'test.json')\n",
    "    test_X = raw_to_numpy(test)\n",
    "    test_X.shape \n",
    "    fake_label = np.zeros(len(test_X))\n",
    "\n",
    "    test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' %i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11.median(axis=1)\n",
    "sub.shape\n",
    "sub.to_csv('submission23.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, 0:6].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, 0:6].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, 0:6].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg5']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg5']\n",
    "sub.shape\n",
    "sub.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s704ms\n"
     ]
    }
   ],
   "source": [
    "net = resnet.resnet34(num_classes=2)\n",
    "net.load_state_dict(torch.load('save_resnet34_acc117.pth'))\n",
    "net.cuda()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "prob = [] \n",
    "net.eval()\n",
    "for k, (val_x, val_y) in enumerate(test_loader):\n",
    "    if use_cuda:\n",
    "        val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    x = Variable(val_x)\n",
    "    y = Variable(val_y)\n",
    "    out = net(x)\n",
    "    #prevent overflow\n",
    "    temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "    ans= temp[:,1]/(temp.sum(axis=1))\n",
    "    prob.append(ans)\n",
    "    #print(out.size())\n",
    "    progress_bar(k, len(test_loader))\n",
    "msg = 'is_iceberg%d' %5\n",
    "temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.852644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.816630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.785453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.559032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.827514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.655435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.895989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.897011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.816630</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.559032</td>\n",
       "      <td>0.827514</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.655435</td>\n",
       "      <td>0.895989</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  \\\n",
       "is_iceberg0           1.000000     0.852644     0.822586     0.648968   \n",
       "is_iceberg1           0.852644     1.000000     0.905401     0.754710   \n",
       "is_iceberg2           0.822586     0.905401     1.000000     0.771766   \n",
       "is_iceberg3           0.648968     0.754710     0.771766     1.000000   \n",
       "is_iceberg4           0.883101     0.833295     0.774018     0.685649   \n",
       "is_iceberg5           0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg_max        0.682861     0.821258     0.847868     0.940914   \n",
       "is_iceberg_min        0.922862     0.777728     0.738630     0.592617   \n",
       "is_iceberg_median     0.942663     0.956190     0.918857     0.749656   \n",
       "is_iceberg_base       0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg            0.905900     0.816630     0.785453     0.559032   \n",
       "\n",
       "                   is_iceberg4  is_iceberg5  is_iceberg_max  is_iceberg_min  \\\n",
       "is_iceberg0           0.883101     0.905277        0.682861        0.922862   \n",
       "is_iceberg1           0.833295     0.815734        0.821258        0.777728   \n",
       "is_iceberg2           0.774018     0.784324        0.847868        0.738630   \n",
       "is_iceberg3           0.685649     0.556919        0.940914        0.592617   \n",
       "is_iceberg4           1.000000     0.826391        0.685683        0.920097   \n",
       "is_iceberg5           0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg_max        0.685683     0.653849        1.000000        0.583326   \n",
       "is_iceberg_min        0.920097     0.895245        0.583326        1.000000   \n",
       "is_iceberg_median     0.909537     0.896220        0.792055        0.875356   \n",
       "is_iceberg_base       0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg            0.827514     0.999683        0.655435        0.895989   \n",
       "\n",
       "                   is_iceberg_median  is_iceberg_base  is_iceberg  \n",
       "is_iceberg0                 0.942663         0.905277    0.905900  \n",
       "is_iceberg1                 0.956190         0.815734    0.816630  \n",
       "is_iceberg2                 0.918857         0.784324    0.785453  \n",
       "is_iceberg3                 0.749656         0.556919    0.559032  \n",
       "is_iceberg4                 0.909537         0.826391    0.827514  \n",
       "is_iceberg5                 0.896220         1.000000    0.999683  \n",
       "is_iceberg_max              0.792055         0.653849    0.655435  \n",
       "is_iceberg_min              0.875356         0.895245    0.895989  \n",
       "is_iceberg_median           1.000000         0.896220    0.897011  \n",
       "is_iceberg_base             0.896220         1.000000    0.999683  \n",
       "is_iceberg                  0.897011         0.999683    1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  5,  6,  7,  8,  9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22,\n",
      "       23, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42,\n",
      "       44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62,\n",
      "       63, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 84, 85,\n",
      "       86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99]), array([ 0,  3,  4, 11, 14, 17, 24, 29, 40, 43, 48, 59, 64, 66, 70, 79, 82,\n",
      "       83, 93, 97]))\n"
     ]
    }
   ],
   "source": [
    "seed= np.random.RandomState(67)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "for i in spliter.split(list(range(100))):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s222ms Step: 200ms | Tot: 2s492ms  Step: 199ms | Tot: 2s692ms  Step: 200ms | Tot: 7s692ms  Step: 199ms | Tot: 13s642ms  Step: 200ms | Tot: 14s841ms  Step: 200ms | Tot: 24s62ms\n",
      "[=================== 132/132 ================>]  Step: 152ms | Tot: 26s199ms Step: 199ms | Tot: 1s425ms  Step: 200ms | Tot: 1s626ms  Step: 200ms | Tot: 8s236ms  Step: 200ms | Tot: 12s639ms  Step: 200ms | Tot: 14s844ms  Step: 199ms | Tot: 16s449ms  Step: 199ms | Tot: 17s647ms  Step: 199ms | Tot: 20s857ms  Step: 200ms | Tot: 23s61ms  Step: 199ms | Tot: 25s648ms  Step: 201ms | Tot: 25s850ms\n",
      "[=================== 132/132 ================>]  Step: 151ms | Tot: 26s141ms Step: 199ms | Tot: 6s583ms  Step: 199ms | Tot: 6s982ms 41/132   Step: 200ms | Tot: 9s576ms  Step: 200ms | Tot: 11s185ms\n",
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s219ms Step: 200ms | Tot: 3s394ms  Step: 200ms | Tot: 4s393ms 52/132   Step: 200ms | Tot: 10s614ms  Step: 200ms | Tot: 10s814ms  Step: 200ms | Tot: 12s219ms  Step: 200ms | Tot: 14s624ms  Step: 200ms | Tot: 15s829ms  Step: 200ms | Tot: 18s640ms 99/132   Step: 200ms | Tot: 22s651ms  Step: 200ms | Tot: 23s456ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "for i,pth in enumerate([os.path.join('resnet34_save_model',i) for i in os.listdir(path='resnet34_save_model') if '.pth' in i]):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg']\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.027504e-03</td>\n",
       "      <td>9.244031e-02</td>\n",
       "      <td>1.784263e-02</td>\n",
       "      <td>5.578169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.931345e-03</td>\n",
       "      <td>3.659658e-01</td>\n",
       "      <td>2.564293e-01</td>\n",
       "      <td>1.571568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.239599e-10</td>\n",
       "      <td>1.970750e-21</td>\n",
       "      <td>3.803356e-08</td>\n",
       "      <td>2.089403e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.993261e-01</td>\n",
       "      <td>9.456407e-01</td>\n",
       "      <td>9.853242e-01</td>\n",
       "      <td>9.989353e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448082e-03</td>\n",
       "      <td>6.435396e-02</td>\n",
       "      <td>3.096765e-02</td>\n",
       "      <td>2.362306e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_iceberg0   is_iceberg1   is_iceberg2   is_iceberg3\n",
       "0  7.027504e-03  9.244031e-02  1.784263e-02  5.578169e-03\n",
       "1  3.931345e-03  3.659658e-01  2.564293e-01  1.571568e-02\n",
       "2  5.239599e-10  1.970750e-21  3.803356e-08  2.089403e-21\n",
       "3  9.993261e-01  9.456407e-01  9.853242e-01  9.989353e-01\n",
       "4  1.448082e-03  6.435396e-02  3.096765e-02  2.362306e-04"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = temp11.mean(1)\n",
    "temp11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, :4].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, :4].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, :4].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg3']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! cp vgg_fcn.ipynb vgg_angle.ipynb\n",
    "temp11.to_csv('others/vgg_10fold.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
