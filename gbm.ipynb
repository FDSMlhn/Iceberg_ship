{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/train.json\n",
      "data/processed/test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1604/1604 [00:00<00:00, 1127667.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8424/8424 [00:00<00:00, 1122710.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import datetime as dt\n",
    "#\n",
    "from random import choice, sample, shuffle, uniform, seed\n",
    "from math import exp, expm1, log1p, log10, log2, sqrt, ceil, floor, isfinite, isnan\n",
    "from itertools import combinations\n",
    "#import for image processing\n",
    "#import cv2\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.ndimage import laplace, sobel\n",
    "#evaluation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "###############################################################################\n",
    "def read_jason(file='', loc='../input/'):\n",
    "    print('{}{}'.format(loc, file))\n",
    "    df = pd.read_json('{}{}'.format(loc, file))\n",
    "    df['inc_angle'] = df['inc_angle'].replace('na', -1).astype(float)\n",
    "    #print(df['inc_angle'].value_counts())\n",
    "    \n",
    "    band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\n",
    "    band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\n",
    "    df = df.drop(['band_1', 'band_2'], axis=1)\n",
    "    \n",
    "    bands = np.stack((band1, band2,  0.5 * (band1 + band2)), axis=-1)  # -1 means add to the last dimension\n",
    "    del band1, band2\n",
    "    \n",
    "    return df, bands\n",
    "\n",
    "###############################################################################\n",
    "#forked from\n",
    "#https://www.kaggle.com/the1owl/planet-understanding-the-amazon-from-space/natural-growth-patterns-fractals-of-nature/notebook\n",
    "def img_to_stats(paths):\n",
    "    \n",
    "    img_id, img = paths[0], paths[1]\n",
    "    \n",
    "    #ignored error    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    bins = 20\n",
    "    scl_min, scl_max = -50, 50\n",
    "    opt_poly = True\n",
    "    #opt_poly = False\n",
    "    \n",
    "    try:\n",
    "        st = []\n",
    "        st_interv = []\n",
    "        hist_interv = []\n",
    "        for i in range(img.shape[2]):\n",
    "            img_sub = np.squeeze(img[:, :, i])\n",
    "            \n",
    "            #median, max and min\n",
    "            sub_st = []\n",
    "            sub_st += [np.mean(img_sub), np.std(img_sub), np.max(img_sub), np.median(img_sub), np.min(img_sub)]\n",
    "            sub_st += [(sub_st[2] - sub_st[3]), (sub_st[2] - sub_st[4]), (sub_st[3] - sub_st[4])] \n",
    "            sub_st += [(sub_st[-3] / sub_st[1]), (sub_st[-2] / sub_st[1]), (sub_st[-1] / sub_st[1])] #normalized by stdev\n",
    "            st += sub_st\n",
    "            #Laplacian, Sobel, kurtosis and skewness\n",
    "            st_trans = []\n",
    "            st_trans += [laplace(img_sub, mode='reflect', cval=0.0).ravel().var()] #blurr\n",
    "            sobel0 = sobel(img_sub, axis=0, mode='reflect', cval=0.0).ravel().var()\n",
    "            sobel1 = sobel(img_sub, axis=1, mode='reflect', cval=0.0).ravel().var()\n",
    "            st_trans += [sobel0, sobel1]\n",
    "            st_trans += [kurtosis(img_sub.ravel()), skew(img_sub.ravel())]\n",
    "            \n",
    "            if opt_poly:\n",
    "                st_interv.append(sub_st)\n",
    "                #\n",
    "                st += [x * y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x + y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x - y for x, y in combinations(st_trans, 2)]                \n",
    " \n",
    "            #hist\n",
    "            #hist = list(cv2.calcHist([img], [i], None, [bins], [0., 1.]).flatten())\n",
    "            hist = list(np.histogram(img_sub, bins=bins, range=(scl_min, scl_max))[0])\n",
    "            hist_interv.append(hist)\n",
    "            st += hist\n",
    "            st += [hist.index(max(hist))] #only the smallest index w/ max value would be incl\n",
    "            st += [np.std(hist), np.max(hist), np.median(hist), (np.max(hist) - np.median(hist))]\n",
    "\n",
    "        if opt_poly:\n",
    "            for x, y in combinations(st_interv, 2):\n",
    "                st += [float(x[j]) * float(y[j]) for j in range(len(st_interv[0]))]\n",
    "\n",
    "            for x, y in combinations(hist_interv, 2):\n",
    "                hist_diff = [x[j] * y[j] for j in range(len(hist_interv[0]))]\n",
    "                st += [hist_diff.index(max(hist_diff))] #only the smallest index w/ max value would be incl\n",
    "                st += [np.std(hist_diff), np.max(hist_diff), np.median(hist_diff), (np.max(hist_diff) - np.median(hist_diff))]\n",
    "                \n",
    "        #correction\n",
    "        nan = -999\n",
    "        for i in range(len(st)):\n",
    "            if isnan(st[i]) == True:\n",
    "                st[i] = nan\n",
    "                \n",
    "    except:\n",
    "        print('except: ')\n",
    "    \n",
    "    return [img_id, st]\n",
    "\n",
    "\n",
    "def extract_img_stats(paths):\n",
    "    imf_d = {}\n",
    "    p = Pool(8) #(cpu_count())\n",
    "    ret = p.map(img_to_stats, paths)   # list of pair of (id, bands) bands is np.array shape (75, 75, 3)\n",
    "    for i in tqdm(range(len(ret)), miniters=100):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "\n",
    "    ret = []\n",
    "    fdata = [imf_d[i] for i, j in paths]\n",
    "    return np.array(fdata, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process(df, bands):\n",
    "\n",
    "    data = extract_img_stats([(k, v) for k, v in zip(df['id'].tolist(), bands)]); gc.collect() #(N, 246)\n",
    "    data = np.concatenate([data, df['inc_angle'].values[:, np.newaxis]], axis=-1); gc.collect() #(N, 247)\n",
    "\n",
    "    print(data.shape)\n",
    "    return data\n",
    "\n",
    "###############################################################################\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    np.random.seed(1017)\n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    #Load data\n",
    "    train, train_bands = read_jason(file='train.json', loc='data/processed/')\n",
    "    test, test_bands = read_jason(file='test.json', loc='data/processed/')\n",
    "\n",
    "    train_X_full = process(df=train, bands=train_bands)\n",
    "    train_y_full = train[target].values    \n",
    "    test_X = process(df=test, bands=test_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 247) (1604,)\n"
     ]
    }
   ],
   "source": [
    "# train_X= train_X_full[train_X_full[:,-1]!=-1] #train_X[:,-1]==-1\n",
    "# train_y= train_y_full[train_X_full[:,-1]!=-1]\n",
    "\n",
    "train_X= train_X_full\n",
    "train_y= train_y_full\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The seed we are using is: 56491\n",
      "\n",
      "round 0001 of 0005, seed=<mtrand.RandomState object at 0x7f027409f750>\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-logloss:0.676736\tvalid-logloss:0.67773\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.312469\tvalid-logloss:0.366561\n",
      "[100]\ttrain-logloss:0.204191\tvalid-logloss:0.283033\n",
      "[150]\ttrain-logloss:0.14502\tvalid-logloss:0.247596\n",
      "[200]\ttrain-logloss:0.107122\tvalid-logloss:0.226796\n",
      "[250]\ttrain-logloss:0.084178\tvalid-logloss:0.213285\n",
      "[300]\ttrain-logloss:0.066561\tvalid-logloss:0.207285\n",
      "[350]\ttrain-logloss:0.053414\tvalid-logloss:0.205252\n",
      "[400]\ttrain-logloss:0.043374\tvalid-logloss:0.201968\n",
      "[450]\ttrain-logloss:0.035939\tvalid-logloss:0.19901\n",
      "[500]\ttrain-logloss:0.029915\tvalid-logloss:0.198545\n",
      "[550]\ttrain-logloss:0.025106\tvalid-logloss:0.197671\n",
      "[600]\ttrain-logloss:0.021369\tvalid-logloss:0.198225\n",
      "Stopping. Best iteration:\n",
      "[549]\ttrain-logloss:0.025179\tvalid-logloss:0.197658\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.344481\n",
      "[100]\tvalid_0's binary_logloss: 0.268986\n",
      "[150]\tvalid_0's binary_logloss: 0.236287\n",
      "[200]\tvalid_0's binary_logloss: 0.222603\n",
      "[250]\tvalid_0's binary_logloss: 0.213092\n",
      "[300]\tvalid_0's binary_logloss: 0.211505\n",
      "[350]\tvalid_0's binary_logloss: 0.209862\n",
      "[400]\tvalid_0's binary_logloss: 0.21083\n",
      "[450]\tvalid_0's binary_logloss: 0.210739\n",
      "Early stopping, best iteration is:\n",
      "[351]\tvalid_0's binary_logloss: 0.209666\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.428933\n",
      "[100]\tvalid_0's binary_logloss: 0.333077\n",
      "[150]\tvalid_0's binary_logloss: 0.323676\n",
      "[200]\tvalid_0's binary_logloss: 0.284914\n",
      "[250]\tvalid_0's binary_logloss: 0.27327\n",
      "[300]\tvalid_0's binary_logloss: 0.255086\n",
      "[350]\tvalid_0's binary_logloss: 0.251324\n",
      "[400]\tvalid_0's binary_logloss: 0.242645\n",
      "[450]\tvalid_0's binary_logloss: 0.237685\n",
      "[500]\tvalid_0's binary_logloss: 0.234519\n",
      "[550]\tvalid_0's binary_logloss: 0.225707\n",
      "[600]\tvalid_0's binary_logloss: 0.223399\n",
      "[650]\tvalid_0's binary_logloss: 0.22239\n",
      "[700]\tvalid_0's binary_logloss: 0.219259\n",
      "[750]\tvalid_0's binary_logloss: 0.218586\n",
      "[800]\tvalid_0's binary_logloss: 0.218023\n",
      "[850]\tvalid_0's binary_logloss: 0.216331\n",
      "[900]\tvalid_0's binary_logloss: 0.215256\n",
      "[950]\tvalid_0's binary_logloss: 0.214098\n",
      "[1000]\tvalid_0's binary_logloss: 0.214055\n",
      "[1050]\tvalid_0's binary_logloss: 0.215061\n",
      "Early stopping, best iteration is:\n",
      "[967]\tvalid_0's binary_logloss: 0.212047\n",
      "\n",
      "round 0002 of 0005, seed=<mtrand.RandomState object at 0x7f027409f750>\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-logloss:0.676941\tvalid-logloss:0.678191\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.320934\tvalid-logloss:0.358283\n",
      "[100]\ttrain-logloss:0.213713\tvalid-logloss:0.269922\n",
      "[150]\ttrain-logloss:0.153602\tvalid-logloss:0.231734\n",
      "[200]\ttrain-logloss:0.115141\tvalid-logloss:0.214383\n",
      "[250]\ttrain-logloss:0.089301\tvalid-logloss:0.200472\n",
      "[300]\ttrain-logloss:0.070641\tvalid-logloss:0.190667\n",
      "[350]\ttrain-logloss:0.056851\tvalid-logloss:0.188247\n",
      "[400]\ttrain-logloss:0.046103\tvalid-logloss:0.184665\n",
      "[450]\ttrain-logloss:0.037799\tvalid-logloss:0.182088\n",
      "[500]\ttrain-logloss:0.031346\tvalid-logloss:0.180322\n",
      "[550]\ttrain-logloss:0.026307\tvalid-logloss:0.179116\n",
      "[600]\ttrain-logloss:0.022539\tvalid-logloss:0.179152\n",
      "[650]\ttrain-logloss:0.019352\tvalid-logloss:0.178058\n",
      "[700]\ttrain-logloss:0.016971\tvalid-logloss:0.177386\n",
      "[750]\ttrain-logloss:0.015022\tvalid-logloss:0.178733\n",
      "Stopping. Best iteration:\n",
      "[699]\ttrain-logloss:0.017001\tvalid-logloss:0.177294\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.338944\n",
      "[100]\tvalid_0's binary_logloss: 0.241758\n",
      "[150]\tvalid_0's binary_logloss: 0.205968\n",
      "[200]\tvalid_0's binary_logloss: 0.189353\n",
      "[250]\tvalid_0's binary_logloss: 0.181523\n",
      "[300]\tvalid_0's binary_logloss: 0.178015\n",
      "[350]\tvalid_0's binary_logloss: 0.1762\n",
      "[400]\tvalid_0's binary_logloss: 0.176332\n",
      "[450]\tvalid_0's binary_logloss: 0.176936\n",
      "Early stopping, best iteration is:\n",
      "[376]\tvalid_0's binary_logloss: 0.17281\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.39322\n",
      "[100]\tvalid_0's binary_logloss: 0.314821\n",
      "[150]\tvalid_0's binary_logloss: 0.279994\n",
      "[200]\tvalid_0's binary_logloss: 0.251129\n",
      "[250]\tvalid_0's binary_logloss: 0.233546\n",
      "[300]\tvalid_0's binary_logloss: 0.226377\n",
      "[350]\tvalid_0's binary_logloss: 0.20992\n",
      "[400]\tvalid_0's binary_logloss: 0.202998\n",
      "[450]\tvalid_0's binary_logloss: 0.198473\n",
      "[500]\tvalid_0's binary_logloss: 0.193135\n",
      "[550]\tvalid_0's binary_logloss: 0.187565\n",
      "[600]\tvalid_0's binary_logloss: 0.186866\n",
      "[650]\tvalid_0's binary_logloss: 0.183791\n",
      "[700]\tvalid_0's binary_logloss: 0.185997\n",
      "Early stopping, best iteration is:\n",
      "[637]\tvalid_0's binary_logloss: 0.182716\n",
      "\n",
      "round 0003 of 0005, seed=<mtrand.RandomState object at 0x7f027409f750>\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-logloss:0.676085\tvalid-logloss:0.678733\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.309249\tvalid-logloss:0.388837\n",
      "[100]\ttrain-logloss:0.200007\tvalid-logloss:0.318256\n",
      "[150]\ttrain-logloss:0.1393\tvalid-logloss:0.284329\n",
      "[200]\ttrain-logloss:0.103196\tvalid-logloss:0.269311\n",
      "[250]\ttrain-logloss:0.079943\tvalid-logloss:0.259764\n",
      "[300]\ttrain-logloss:0.06317\tvalid-logloss:0.252013\n",
      "[350]\ttrain-logloss:0.050518\tvalid-logloss:0.248534\n",
      "[400]\ttrain-logloss:0.040922\tvalid-logloss:0.24651\n",
      "[450]\ttrain-logloss:0.033863\tvalid-logloss:0.246563\n",
      "[500]\ttrain-logloss:0.028013\tvalid-logloss:0.248332\n",
      "Stopping. Best iteration:\n",
      "[403]\ttrain-logloss:0.040416\tvalid-logloss:0.24583\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.366064\n",
      "[100]\tvalid_0's binary_logloss: 0.299886\n",
      "[150]\tvalid_0's binary_logloss: 0.276611\n",
      "[200]\tvalid_0's binary_logloss: 0.266038\n",
      "[250]\tvalid_0's binary_logloss: 0.260256\n",
      "[300]\tvalid_0's binary_logloss: 0.257601\n",
      "[350]\tvalid_0's binary_logloss: 0.258692\n",
      "Early stopping, best iteration is:\n",
      "[293]\tvalid_0's binary_logloss: 0.256391\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.450459\n",
      "[100]\tvalid_0's binary_logloss: 0.372626\n",
      "[150]\tvalid_0's binary_logloss: 0.322289\n",
      "[200]\tvalid_0's binary_logloss: 0.316524\n",
      "[250]\tvalid_0's binary_logloss: 0.296349\n",
      "[300]\tvalid_0's binary_logloss: 0.289197\n",
      "[350]\tvalid_0's binary_logloss: 0.280662\n",
      "[400]\tvalid_0's binary_logloss: 0.27587\n",
      "[450]\tvalid_0's binary_logloss: 0.275016\n",
      "[500]\tvalid_0's binary_logloss: 0.271631\n",
      "[550]\tvalid_0's binary_logloss: 0.266822\n",
      "[600]\tvalid_0's binary_logloss: 0.263246\n",
      "[650]\tvalid_0's binary_logloss: 0.255897\n",
      "[700]\tvalid_0's binary_logloss: 0.254785\n",
      "[750]\tvalid_0's binary_logloss: 0.251387\n",
      "[800]\tvalid_0's binary_logloss: 0.251649\n",
      "Early stopping, best iteration is:\n",
      "[749]\tvalid_0's binary_logloss: 0.251095\n",
      "\n",
      "round 0004 of 0005, seed=<mtrand.RandomState object at 0x7f027409f750>\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-logloss:0.676422\tvalid-logloss:0.677736\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.314661\tvalid-logloss:0.366334\n",
      "[100]\ttrain-logloss:0.204875\tvalid-logloss:0.287771\n",
      "[150]\ttrain-logloss:0.145176\tvalid-logloss:0.254719\n",
      "[200]\ttrain-logloss:0.111499\tvalid-logloss:0.236169\n",
      "[250]\ttrain-logloss:0.086321\tvalid-logloss:0.22567\n",
      "[300]\ttrain-logloss:0.068042\tvalid-logloss:0.219061\n",
      "[350]\ttrain-logloss:0.055527\tvalid-logloss:0.213259\n",
      "[400]\ttrain-logloss:0.046018\tvalid-logloss:0.211633\n",
      "[450]\ttrain-logloss:0.037245\tvalid-logloss:0.211019\n",
      "[500]\ttrain-logloss:0.030513\tvalid-logloss:0.212959\n",
      "[550]\ttrain-logloss:0.025586\tvalid-logloss:0.214774\n",
      "Stopping. Best iteration:\n",
      "[458]\ttrain-logloss:0.036083\tvalid-logloss:0.210751\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.34714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.265851\n",
      "[150]\tvalid_0's binary_logloss: 0.239584\n",
      "[200]\tvalid_0's binary_logloss: 0.228293\n",
      "[250]\tvalid_0's binary_logloss: 0.225166\n",
      "[300]\tvalid_0's binary_logloss: 0.221068\n",
      "[350]\tvalid_0's binary_logloss: 0.218408\n",
      "[400]\tvalid_0's binary_logloss: 0.218388\n",
      "[450]\tvalid_0's binary_logloss: 0.220411\n",
      "Early stopping, best iteration is:\n",
      "[382]\tvalid_0's binary_logloss: 0.215053\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.404745\n",
      "[100]\tvalid_0's binary_logloss: 0.344426\n",
      "[150]\tvalid_0's binary_logloss: 0.292273\n",
      "[200]\tvalid_0's binary_logloss: 0.266047\n",
      "[250]\tvalid_0's binary_logloss: 0.256549\n",
      "[300]\tvalid_0's binary_logloss: 0.245817\n",
      "[350]\tvalid_0's binary_logloss: 0.241352\n",
      "[400]\tvalid_0's binary_logloss: 0.230226\n",
      "[450]\tvalid_0's binary_logloss: 0.231839\n",
      "[500]\tvalid_0's binary_logloss: 0.224892\n",
      "[550]\tvalid_0's binary_logloss: 0.224361\n",
      "[600]\tvalid_0's binary_logloss: 0.223192\n",
      "[650]\tvalid_0's binary_logloss: 0.222426\n",
      "[700]\tvalid_0's binary_logloss: 0.218528\n",
      "[750]\tvalid_0's binary_logloss: 0.218161\n",
      "[800]\tvalid_0's binary_logloss: 0.215159\n",
      "[850]\tvalid_0's binary_logloss: 0.214472\n",
      "[900]\tvalid_0's binary_logloss: 0.215138\n",
      "[950]\tvalid_0's binary_logloss: 0.214233\n",
      "[1000]\tvalid_0's binary_logloss: 0.21326\n",
      "[1050]\tvalid_0's binary_logloss: 0.21404\n",
      "Early stopping, best iteration is:\n",
      "[979]\tvalid_0's binary_logloss: 0.211975\n",
      "\n",
      "round 0005 of 0005, seed=<mtrand.RandomState object at 0x7f027409f750>\n",
      "splitted: (1284, 247), (320, 247)\n",
      "[0]\ttrain-logloss:0.67558\tvalid-logloss:0.679896\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.307249\tvalid-logloss:0.395785\n",
      "[100]\ttrain-logloss:0.202345\tvalid-logloss:0.291203\n",
      "[150]\ttrain-logloss:0.142742\tvalid-logloss:0.250259\n",
      "[200]\ttrain-logloss:0.108871\tvalid-logloss:0.230611\n",
      "[250]\ttrain-logloss:0.085292\tvalid-logloss:0.222499\n",
      "[300]\ttrain-logloss:0.066914\tvalid-logloss:0.213423\n",
      "[350]\ttrain-logloss:0.053818\tvalid-logloss:0.210765\n",
      "[400]\ttrain-logloss:0.043358\tvalid-logloss:0.210721\n",
      "[450]\ttrain-logloss:0.035978\tvalid-logloss:0.210775\n",
      "[500]\ttrain-logloss:0.02981\tvalid-logloss:0.208908\n",
      "[550]\ttrain-logloss:0.02511\tvalid-logloss:0.211063\n",
      "Stopping. Best iteration:\n",
      "[497]\ttrain-logloss:0.030139\tvalid-logloss:0.208387\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.362142\n",
      "[100]\tvalid_0's binary_logloss: 0.265275\n",
      "[150]\tvalid_0's binary_logloss: 0.226501\n",
      "[200]\tvalid_0's binary_logloss: 0.211301\n",
      "[250]\tvalid_0's binary_logloss: 0.20355\n",
      "[300]\tvalid_0's binary_logloss: 0.19928\n",
      "[350]\tvalid_0's binary_logloss: 0.196006\n",
      "[400]\tvalid_0's binary_logloss: 0.197026\n",
      "Early stopping, best iteration is:\n",
      "[348]\tvalid_0's binary_logloss: 0.195955\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.447386\n",
      "[100]\tvalid_0's binary_logloss: 0.364772\n",
      "[150]\tvalid_0's binary_logloss: 0.316394\n",
      "[200]\tvalid_0's binary_logloss: 0.282697\n",
      "[250]\tvalid_0's binary_logloss: 0.258704\n",
      "[300]\tvalid_0's binary_logloss: 0.249283\n",
      "[350]\tvalid_0's binary_logloss: 0.235754\n",
      "[400]\tvalid_0's binary_logloss: 0.235171\n",
      "[450]\tvalid_0's binary_logloss: 0.227824\n",
      "[500]\tvalid_0's binary_logloss: 0.22826\n",
      "[550]\tvalid_0's binary_logloss: 0.223142\n",
      "[600]\tvalid_0's binary_logloss: 0.219224\n",
      "[650]\tvalid_0's binary_logloss: 0.214081\n",
      "[700]\tvalid_0's binary_logloss: 0.213218\n",
      "[750]\tvalid_0's binary_logloss: 0.210746\n",
      "[800]\tvalid_0's binary_logloss: 0.206535\n",
      "[850]\tvalid_0's binary_logloss: 0.203128\n",
      "[900]\tvalid_0's binary_logloss: 0.201414\n",
      "[950]\tvalid_0's binary_logloss: 0.198187\n",
      "[1000]\tvalid_0's binary_logloss: 0.199199\n",
      "[1050]\tvalid_0's binary_logloss: 0.199957\n",
      "Early stopping, best iteration is:\n",
      "[958]\tvalid_0's binary_logloss: 0.19748\n",
      "\n",
      "Blending...\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_dart_01.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_dart_04.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_gbdt_01.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_xgb_03.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_gbdt_05.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_gbdt_02.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_xgb_02.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_xgb_01.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_xgb_05.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_dart_03.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_gbdt_04.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_dart_02.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_dart_05.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_lgb_gbdt_03.csv, w=1.0\n",
      "load: gbm/subm_2017-12-24-20-07_xgb_04.csv, w=1.0\n",
      "\n",
      "                       -24-20-07_lgb_dart_01  -24-20-07_lgb_dart_04  \\\n",
      "-24-20-07_lgb_dart_01               1.000000               0.983439   \n",
      "-24-20-07_lgb_dart_04               0.983439               1.000000   \n",
      "-24-20-07_lgb_gbdt_01               0.991750               0.976968   \n",
      "-24-20-07_xgb_03                    0.964785               0.972128   \n",
      "-24-20-07_lgb_gbdt_05               0.975110               0.975037   \n",
      "-24-20-07_lgb_gbdt_02               0.973986               0.971883   \n",
      "-24-20-07_xgb_02                    0.961433               0.956684   \n",
      "-24-20-07_xgb_01                    0.984095               0.972423   \n",
      "-24-20-07_xgb_05                    0.970279               0.972561   \n",
      "-24-20-07_lgb_dart_03               0.946982               0.959217   \n",
      "-24-20-07_lgb_gbdt_04               0.979869               0.992760   \n",
      "-24-20-07_lgb_dart_02               0.975193               0.975460   \n",
      "-24-20-07_lgb_dart_05               0.976646               0.979943   \n",
      "-24-20-07_lgb_gbdt_03               0.924089               0.937372   \n",
      "-24-20-07_xgb_04                    0.971070               0.982182   \n",
      "\n",
      "                       -24-20-07_lgb_gbdt_01  -24-20-07_xgb_03  \\\n",
      "-24-20-07_lgb_dart_01               0.991750          0.964785   \n",
      "-24-20-07_lgb_dart_04               0.976968          0.972128   \n",
      "-24-20-07_lgb_gbdt_01               1.000000          0.957776   \n",
      "-24-20-07_xgb_03                    0.957776          1.000000   \n",
      "-24-20-07_lgb_gbdt_05               0.977672          0.964092   \n",
      "-24-20-07_lgb_gbdt_02               0.974898          0.958014   \n",
      "-24-20-07_xgb_02                    0.957500          0.950540   \n",
      "-24-20-07_xgb_01                    0.980519          0.971772   \n",
      "-24-20-07_xgb_05                    0.970370          0.975172   \n",
      "-24-20-07_lgb_dart_03               0.942292          0.968120   \n",
      "-24-20-07_lgb_gbdt_04               0.978090          0.970720   \n",
      "-24-20-07_lgb_dart_02               0.969843          0.955846   \n",
      "-24-20-07_lgb_dart_05               0.974625          0.971029   \n",
      "-24-20-07_lgb_gbdt_03               0.922196          0.951013   \n",
      "-24-20-07_xgb_04                    0.968198          0.975689   \n",
      "\n",
      "                       -24-20-07_lgb_gbdt_05  -24-20-07_lgb_gbdt_02  \\\n",
      "-24-20-07_lgb_dart_01               0.975110               0.973986   \n",
      "-24-20-07_lgb_dart_04               0.975037               0.971883   \n",
      "-24-20-07_lgb_gbdt_01               0.977672               0.974898   \n",
      "-24-20-07_xgb_03                    0.964092               0.958014   \n",
      "-24-20-07_lgb_gbdt_05               1.000000               0.974234   \n",
      "-24-20-07_lgb_gbdt_02               0.974234               1.000000   \n",
      "-24-20-07_xgb_02                    0.947073               0.965899   \n",
      "-24-20-07_xgb_01                    0.967398               0.965004   \n",
      "-24-20-07_xgb_05                    0.983561               0.966533   \n",
      "-24-20-07_lgb_dart_03               0.956819               0.945310   \n",
      "-24-20-07_lgb_gbdt_04               0.978908               0.977254   \n",
      "-24-20-07_lgb_dart_02               0.967551               0.987439   \n",
      "-24-20-07_lgb_dart_05               0.993332               0.970584   \n",
      "-24-20-07_lgb_gbdt_03               0.943380               0.930149   \n",
      "-24-20-07_xgb_04                    0.967835               0.959568   \n",
      "\n",
      "                       -24-20-07_xgb_02  -24-20-07_xgb_01  -24-20-07_xgb_05  \\\n",
      "-24-20-07_lgb_dart_01          0.961433          0.984095          0.970279   \n",
      "-24-20-07_lgb_dart_04          0.956684          0.972423          0.972561   \n",
      "-24-20-07_lgb_gbdt_01          0.957500          0.980519          0.970370   \n",
      "-24-20-07_xgb_03               0.950540          0.971772          0.975172   \n",
      "-24-20-07_lgb_gbdt_05          0.947073          0.967398          0.983561   \n",
      "-24-20-07_lgb_gbdt_02          0.965899          0.965004          0.966533   \n",
      "-24-20-07_xgb_02               1.000000          0.975357          0.965501   \n",
      "-24-20-07_xgb_01               0.975357          1.000000          0.978992   \n",
      "-24-20-07_xgb_05               0.965501          0.978992          1.000000   \n",
      "-24-20-07_lgb_dart_03          0.899825          0.928097          0.941783   \n",
      "-24-20-07_lgb_gbdt_04          0.954092          0.969942          0.973675   \n",
      "-24-20-07_lgb_dart_02          0.969973          0.965632          0.964116   \n",
      "-24-20-07_lgb_dart_05          0.946604          0.968508          0.984179   \n",
      "-24-20-07_lgb_gbdt_03          0.870983          0.902982          0.922179   \n",
      "-24-20-07_xgb_04               0.967031          0.981832          0.979983   \n",
      "\n",
      "                       -24-20-07_lgb_dart_03  -24-20-07_lgb_gbdt_04  \\\n",
      "-24-20-07_lgb_dart_01               0.946982               0.979869   \n",
      "-24-20-07_lgb_dart_04               0.959217               0.992760   \n",
      "-24-20-07_lgb_gbdt_01               0.942292               0.978090   \n",
      "-24-20-07_xgb_03                    0.968120               0.970720   \n",
      "-24-20-07_lgb_gbdt_05               0.956819               0.978908   \n",
      "-24-20-07_lgb_gbdt_02               0.945310               0.977254   \n",
      "-24-20-07_xgb_02                    0.899825               0.954092   \n",
      "-24-20-07_xgb_01                    0.928097               0.969942   \n",
      "-24-20-07_xgb_05                    0.941783               0.973675   \n",
      "-24-20-07_lgb_dart_03               1.000000               0.958564   \n",
      "-24-20-07_lgb_gbdt_04               0.958564               1.000000   \n",
      "-24-20-07_lgb_dart_02               0.943255               0.973178   \n",
      "-24-20-07_lgb_dart_05               0.965111               0.979079   \n",
      "-24-20-07_lgb_gbdt_03               0.991353               0.941874   \n",
      "-24-20-07_xgb_04                    0.935714               0.978956   \n",
      "\n",
      "                       -24-20-07_lgb_dart_02  -24-20-07_lgb_dart_05  \\\n",
      "-24-20-07_lgb_dart_01               0.975193               0.976646   \n",
      "-24-20-07_lgb_dart_04               0.975460               0.979943   \n",
      "-24-20-07_lgb_gbdt_01               0.969843               0.974625   \n",
      "-24-20-07_xgb_03                    0.955846               0.971029   \n",
      "-24-20-07_lgb_gbdt_05               0.967551               0.993332   \n",
      "-24-20-07_lgb_gbdt_02               0.987439               0.970584   \n",
      "-24-20-07_xgb_02                    0.969973               0.946604   \n",
      "-24-20-07_xgb_01                    0.965632               0.968508   \n",
      "-24-20-07_xgb_05                    0.964116               0.984179   \n",
      "-24-20-07_lgb_dart_03               0.943255               0.965111   \n",
      "-24-20-07_lgb_gbdt_04               0.973178               0.979079   \n",
      "-24-20-07_lgb_dart_02               1.000000               0.970538   \n",
      "-24-20-07_lgb_dart_05               0.970538               1.000000   \n",
      "-24-20-07_lgb_gbdt_03               0.921488               0.949623   \n",
      "-24-20-07_xgb_04                    0.959306               0.970800   \n",
      "\n",
      "                       -24-20-07_lgb_gbdt_03  -24-20-07_xgb_04  \n",
      "-24-20-07_lgb_dart_01               0.924089          0.971070  \n",
      "-24-20-07_lgb_dart_04               0.937372          0.982182  \n",
      "-24-20-07_lgb_gbdt_01               0.922196          0.968198  \n",
      "-24-20-07_xgb_03                    0.951013          0.975689  \n",
      "-24-20-07_lgb_gbdt_05               0.943380          0.967835  \n",
      "-24-20-07_lgb_gbdt_02               0.930149          0.959568  \n",
      "-24-20-07_xgb_02                    0.870983          0.967031  \n",
      "-24-20-07_xgb_01                    0.902982          0.981832  \n",
      "-24-20-07_xgb_05                    0.922179          0.979983  \n",
      "-24-20-07_lgb_dart_03               0.991353          0.935714  \n",
      "-24-20-07_lgb_gbdt_04               0.941874          0.978956  \n",
      "-24-20-07_lgb_dart_02               0.921488          0.959306  \n",
      "-24-20-07_lgb_dart_05               0.949623          0.970800  \n",
      "-24-20-07_lgb_gbdt_03               1.000000          0.909892  \n",
      "-24-20-07_xgb_04                    0.909892          1.000000  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview: \n",
      "         id  is_iceberg\n",
      "0  5941774d    0.100334\n",
      "1  4023181e    0.939969\n",
      "2  b20200e4    0.197311\n",
      "3  e7f018bb    0.983173\n",
      "4  4371c8c3    0.889080\n"
     ]
    }
   ],
   "source": [
    "# This will be the version changed based on my own understanding\n",
    "def save_blend(preds={}, loc='./'):\n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    w_total = 0.0\n",
    "    blend = None\n",
    "    df_corr = None\n",
    "    print('\\nBlending...')\n",
    "    for k, v in preds.items():\n",
    "        if blend is None:\n",
    "            blend = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            \n",
    "            df_corr = pd.DataFrame({'id': blend['id'].tolist()})\n",
    "            df_corr[k[16:-4]] = blend[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] = blend[target] * v\n",
    "                \n",
    "        else:\n",
    "            preds_tmp = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            preds_tmp = blend[['id']].merge(preds_tmp, how='left', on='id')\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            df_corr[k[16:-4]] = preds_tmp[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] += preds_tmp[target] * v\n",
    "            del preds_tmp\n",
    "            \n",
    "    print('\\n{}'.format(df_corr.corr()), flush=True)\n",
    "    #write submission\n",
    "    blend[target] = blend[target] / w_total\n",
    "    print('\\nPreview: \\n{}'.format(blend.head()), flush=True)\n",
    "    blend.to_csv('{}subm_blend{:03d}_{}.csv'.format(loc, len(preds), tmp), index=False, float_format='%.6f')\n",
    "\n",
    "def run_lgb(params={}, lgb_train=None, lgb_valid=None, lgb_test=None, test_ids=None, nr_round=2000, min_round=100, file=''):\n",
    "\n",
    "    print('\\nLightGBM: {}'.format(params['boosting'])) \n",
    "    model2 = lgb.train(params, \n",
    "                       lgb_train, \n",
    "                       nr_round, \n",
    "                       lgb_valid, \n",
    "                       verbose_eval=50, early_stopping_rounds=min_round)\n",
    "    \n",
    "    pred = model2.predict(lgb_test, num_iteration=model2.best_iteration)\n",
    "    #\n",
    "    subm = pd.DataFrame({'id': test_ids, 'is_iceberg': pred})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    #   \n",
    "    df = pd.DataFrame({'feature':model2.feature_name(), 'importances': model2.feature_importance()})\n",
    "    \n",
    "    return pred, df\n",
    "\n",
    "\n",
    "#results\n",
    "freq = pd.DataFrame()\n",
    "subms = []\n",
    "\n",
    "#training\n",
    "# test_ratio = 0.2\n",
    "# nr_runs = 3\n",
    "# split_seed = 25\n",
    "# kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "ran_num = np.random.randint(50000,60000,size=1)[0]\n",
    "split_seed= np.random.RandomState(ran_num)\n",
    "print('The seed we are using is: %d' % ran_num)\n",
    "nr_runs = 5\n",
    "kf = KFold(n_splits=nr_runs, random_state=split_seed)\n",
    "\n",
    "for r, (train_index, test_index) in enumerate(kf.split(train_X, train_y)):\n",
    "    print('\\nround {:04d} of {:04d}, seed={}'.format(r+1, nr_runs, split_seed))\n",
    "\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "    x1, x2 = train_X[train_index], train_X[test_index]\n",
    "    y1, y2 = train_y[train_index], train_y[test_index]\n",
    "    #x1, x2, y1, y2 = train_test_split(train_X, train_y, test_size=test_ratio, random_state=split_seed + r)\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    test_X_dup = test_X.copy()\n",
    "\n",
    "    #XGB\n",
    "    xgb_train = xgb.DMatrix(x1, y1)\n",
    "    xgb_valid = xgb.DMatrix(x2, y2)\n",
    "    #\n",
    "    watchlist = [(xgb_train, 'train'), (xgb_valid, 'valid')]\n",
    "    params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'seed': 99, 'silent': True}\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 4\n",
    "    params['subsample'] = 0.9\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['colsample_bytree'] = 0.8\n",
    "    params['colsample_bylevel'] = 0.8\n",
    "    params['max_delta_step'] = 3\n",
    "    #params['gamma'] = 5.0\n",
    "    #params['labmda'] = 1\n",
    "    params['scale_pos_weight'] = 1.0\n",
    "    params['seed'] = ran_num + r\n",
    "    nr_round = 2000\n",
    "    min_round = 100\n",
    "\n",
    "    model1 = xgb.train(params, \n",
    "                       xgb_train, \n",
    "                       nr_round,  \n",
    "                       watchlist, \n",
    "                       verbose_eval=50, \n",
    "                       early_stopping_rounds=min_round)\n",
    "\n",
    "    pred_xgb = model1.predict(xgb.DMatrix(test_X_dup), ntree_limit=model1.best_ntree_limit+45)\n",
    "\n",
    "    #\n",
    "    file = 'gbm/subm_{}_xgb_{:02d}.csv'.format(tmp, r+1)\n",
    "    subm = pd.DataFrame({'id': test['id'].values, target: pred_xgb})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    subms.append(file)    \n",
    "\n",
    "    ##LightGBM\n",
    "    lgb_train = lgb.Dataset(x1, label=y1, free_raw_data=False)\n",
    "    lgb_valid = lgb.Dataset(x2, label=y2, reference=lgb_train, free_raw_data=False)\n",
    "    #gbdt\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'gbdt'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = ran_num + r\n",
    "    #\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    file = 'gbm/subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "    ##LightGBM\n",
    "    #dart\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'dart'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.04\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = ran_num + r\n",
    "    #dart\n",
    "    params['drop_rate'] = 0.1\n",
    "    params['skip_drop'] = 0.5\n",
    "    params['max_drop'] = 10\n",
    "    params['verbose'] = -1 \n",
    "\n",
    "    file = 'gbm/subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "\n",
    "#blending\n",
    "preds = {k: 1.0 for k in subms}\n",
    "save_blend(preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m.\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34m..\u001b[m\u001b[m\r\n",
      ".DS_Store\r\n",
      "\u001b[1m\u001b[34m.git\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "36_plain_cnn.csv\r\n",
      "41_plain_cnn.csv\r\n",
      "50_plain_fcn.csv\r\n",
      "67_plain_cnn.csv\r\n",
      "6_retrain_inception.csv\r\n",
      "Image preprocess testing.ipynb\r\n",
      "README.md\r\n",
      "Ship-Iceberg Discrimination with Convolutional Neural Networks in High Resolution SAR Images.pdf\r\n",
      "The Effectiveness of Data Augmentation in Image Classification using Deep Learning.pdf\r\n",
      "Training_log.ipynb\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "all_14_inception.csv\r\n",
      "cnn.ipynb\r\n",
      "cnn.py\r\n",
      "cnn_angle.ipynb\r\n",
      "cnn_angle.py\r\n",
      "\u001b[1m\u001b[34mdata\u001b[m\u001b[m\r\n",
      "densenet.py\r\n",
      "densenet121.ipynb\r\n",
      "densenet121_pseudl.ipynb\r\n",
      "densenetBC.py\r\n",
      "densenetbc100.ipynb\r\n",
      "fcn.ipynb\r\n",
      "fcn.py\r\n",
      "final ensemble.ipynb\r\n",
      "gbm.ipynb\r\n",
      "inception.ipynb\r\n",
      "inception.py\r\n",
      "\u001b[1m\u001b[34mothers\u001b[m\u001b[m\r\n",
      "pre_resnet.py\r\n",
      "pre_vgg.py\r\n",
      "r2_11_plain_cnn.csv\r\n",
      "r2_fcn_11_models.csv\r\n",
      "resnet.py\r\n",
      "resnet101.ipynb\r\n",
      "resnet101_4feat.ipynb\r\n",
      "resnet152.ipynb\r\n",
      "resnet18.ipynb\r\n",
      "resnet34.ipynb\r\n",
      "resnet34_4feat.ipynb\r\n",
      "resnet34_onlygoodretrain.csv\r\n",
      "resnet34_pseudolb.ipynb\r\n",
      "resnet34_retrain_all.csv\r\n",
      "resnet50.ipynb\r\n",
      "resnet50_temp.ipynb\r\n",
      "save_for_use.csv\r\n",
      "squeezenet.py\r\n",
      "squeezenet1.ipynb\r\n",
      "submissionlasttd.csv\r\n",
      "test.txt\r\n",
      "utils.py\r\n",
      "vgg.ipynb\r\n",
      "vgg.py\r\n",
      "vgg_mobile.py\r\n"
     ]
    }
   ],
   "source": [
    "def save_blend(preds={}, loc='./'):\n",
    "    \n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    w_total = 0.0\n",
    "    blend = None\n",
    "    df_corr = None\n",
    "    print('\\nBlending...')\n",
    "    for k, v in preds.items():\n",
    "        if blend is None:\n",
    "            blend = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            \n",
    "            df_corr = pd.DataFrame({'id': blend['id'].tolist()})\n",
    "            df_corr[k[16:-4]] = blend[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] = blend[target] * v\n",
    "                \n",
    "        else:\n",
    "            preds_tmp = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            preds_tmp = blend[['id']].merge(preds_tmp, how='left', on='id')\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            df_corr[k[16:-4]] = preds_tmp[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] += preds_tmp[target] * v\n",
    "            del preds_tmp\n",
    "            \n",
    "    print('\\n{}'.format(df_corr.corr()), flush=True)\n",
    "    #write submission\n",
    "    blend[target] = blend[target] / w_total\n",
    "    print('\\nPreview: \\n{}'.format(blend.head()), flush=True)\n",
    "    blend.to_csv('{}subm_blend{:03d}_{}.csv'.format(loc, len(preds), tmp), index=False, float_format='%.6f')\n",
    "\n",
    "def run_lgb(params={}, lgb_train=None, lgb_valid=None, lgb_test=None, test_ids=None, nr_round=2000, min_round=100, file=''):\n",
    "\n",
    "    print('\\nLightGBM: {}'.format(params['boosting'])) \n",
    "    model2 = lgb.train(params, \n",
    "                       lgb_train, \n",
    "                       nr_round, \n",
    "                       lgb_valid, \n",
    "                       verbose_eval=50, early_stopping_rounds=min_round)\n",
    "    \n",
    "    pred = model2.predict(lgb_test, num_iteration=model2.best_iteration)\n",
    "    #\n",
    "    subm = pd.DataFrame({'id': test_ids, 'is_iceberg': pred})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    #   \n",
    "    df = pd.DataFrame({'feature':model2.feature_name(), 'importances': model2.feature_importance()})\n",
    "    \n",
    "    return pred, df\n",
    "\n",
    "\n",
    "#results\n",
    "freq = pd.DataFrame()\n",
    "subms = []\n",
    "\n",
    "#training\n",
    "test_ratio = 0.2\n",
    "nr_runs = 3\n",
    "split_seed = 25\n",
    "kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "\n",
    "for r, (train_index, test_index) in enumerate(kf.split(train_X, train_y)):\n",
    "    print('\\nround {:04d} of {:04d}, seed={}'.format(r+1, nr_runs, split_seed))\n",
    "\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "    x1, x2 = train_X[train_index], train_X[test_index]\n",
    "    y1, y2 = train_y[train_index], train_y[test_index]\n",
    "    #x1, x2, y1, y2 = train_test_split(train_X, train_y, test_size=test_ratio, random_state=split_seed + r)\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    test_X_dup = test_X.copy()\n",
    "\n",
    "    #XGB\n",
    "    xgb_train = xgb.DMatrix(x1, y1)\n",
    "    xgb_valid = xgb.DMatrix(x2, y2)\n",
    "    #\n",
    "    watchlist = [(xgb_train, 'train'), (xgb_valid, 'valid')]\n",
    "    params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'seed': 99, 'silent': True}\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 4\n",
    "    params['subsample'] = 0.9\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['colsample_bytree'] = 0.8\n",
    "    params['colsample_bylevel'] = 0.8\n",
    "    params['max_delta_step'] = 3\n",
    "    #params['gamma'] = 5.0\n",
    "    #params['labmda'] = 1\n",
    "    params['scale_pos_weight'] = 1.0\n",
    "    params['seed'] = split_seed + r\n",
    "    nr_round = 2000\n",
    "    min_round = 100\n",
    "\n",
    "    model1 = xgb.train(params, \n",
    "                       xgb_train, \n",
    "                       nr_round,  \n",
    "                       watchlist, \n",
    "                       verbose_eval=50, \n",
    "                       early_stopping_rounds=min_round)\n",
    "\n",
    "    pred_xgb = model1.predict(xgb.DMatrix(test_X_dup), ntree_limit=model1.best_ntree_limit+45)\n",
    "\n",
    "    #\n",
    "    file = 'subm_{}_xgb_{:02d}.csv'.format(tmp, r+1)\n",
    "    subm = pd.DataFrame({'id': test['id'].values, target: pred_xgb})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    subms.append(file)    \n",
    "\n",
    "    ##LightGBM\n",
    "    lgb_train = lgb.Dataset(x1, label=y1, free_raw_data=False)\n",
    "    lgb_valid = lgb.Dataset(x2, label=y2, reference=lgb_train, free_raw_data=False)\n",
    "    #gbdt\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'gbdt'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "    ##LightGBM\n",
    "    #dart\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'dart'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.04\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #dart\n",
    "    params['drop_rate'] = 0.1\n",
    "    params['skip_drop'] = 0.5\n",
    "    params['max_drop'] = 10\n",
    "    params['verbose'] = -1 \n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "\n",
    "#blending\n",
    "preds = {k: 1.0 for k in subms}\n",
    "save_blend(preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52161])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=  torch.Tensor([1,2,3])\n",
    "a.size()\n",
    "a=a.unsqueeze(1)\n",
    "a.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
