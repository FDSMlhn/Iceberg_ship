{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/train.json\n",
      "data/processed/test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1604/1604 [00:00<00:00, 1016570.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8424/8424 [00:00<00:00, 1073945.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 247)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import datetime as dt\n",
    "#\n",
    "from random import choice, sample, shuffle, uniform, seed\n",
    "from math import exp, expm1, log1p, log10, log2, sqrt, ceil, floor, isfinite, isnan\n",
    "from itertools import combinatcv\n",
    "ions\n",
    "#import for image processing\n",
    "#import cv2\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.ndimage import laplace, sobel\n",
    "#evaluation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "###############################################################################\n",
    "def read_jason(file='', loc='../input/'):\n",
    "    print('{}{}'.format(loc, file))\n",
    "    df = pd.read_json('{}{}'.format(loc, file))\n",
    "    df['inc_angle'] = df['inc_angle'].replace('na', -1).astype(float)\n",
    "    #print(df['inc_angle'].value_counts())\n",
    "    \n",
    "    band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\n",
    "    band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\n",
    "    df = df.drop(['band_1', 'band_2'], axis=1)\n",
    "    \n",
    "    bands = np.stack((band1, band2,  0.5 * (band1 + band2)), axis=-1)  # -1 means add to the last dimension\n",
    "    del band1, band2\n",
    "    \n",
    "    return df, bands\n",
    "\n",
    "###############################################################################\n",
    "#forked from\n",
    "#https://www.kaggle.com/the1owl/planet-understanding-the-amazon-from-space/natural-growth-patterns-fractals-of-nature/notebook\n",
    "def img_to_stats(paths):\n",
    "    \n",
    "    img_id, img = paths[0], paths[1]\n",
    "    \n",
    "    #ignored error    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    bins = 20\n",
    "    scl_min, scl_max = -50, 50\n",
    "    opt_poly = True\n",
    "    #opt_poly = False\n",
    "    \n",
    "    try:\n",
    "        st = []\n",
    "        st_interv = []\n",
    "        hist_interv = []\n",
    "        for i in range(img.shape[2]):\n",
    "            img_sub = np.squeeze(img[:, :, i])\n",
    "            \n",
    "            #median, max and min\n",
    "            sub_st = []\n",
    "            sub_st += [np.mean(img_sub), np.std(img_sub), np.max(img_sub), np.median(img_sub), np.min(img_sub)]\n",
    "            sub_st += [(sub_st[2] - sub_st[3]), (sub_st[2] - sub_st[4]), (sub_st[3] - sub_st[4])] \n",
    "            sub_st += [(sub_st[-3] / sub_st[1]), (sub_st[-2] / sub_st[1]), (sub_st[-1] / sub_st[1])] #normalized by stdev\n",
    "            st += sub_st\n",
    "            #Laplacian, Sobel, kurtosis and skewness\n",
    "            st_trans = []\n",
    "            st_trans += [laplace(img_sub, mode='reflect', cval=0.0).ravel().var()] #blurr\n",
    "            sobel0 = sobel(img_sub, axis=0, mode='reflect', cval=0.0).ravel().var()\n",
    "            sobel1 = sobel(img_sub, axis=1, mode='reflect', cval=0.0).ravel().var()\n",
    "            st_trans += [sobel0, sobel1]\n",
    "            st_trans += [kurtosis(img_sub.ravel()), skew(img_sub.ravel())]\n",
    "            \n",
    "            if opt_poly:\n",
    "                st_interv.append(sub_st)\n",
    "                #\n",
    "                st += [x * y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x + y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x - y for x, y in combinations(st_trans, 2)]                \n",
    " \n",
    "            #hist\n",
    "            #hist = list(cv2.calcHist([img], [i], None, [bins], [0., 1.]).flatten())\n",
    "            hist = list(np.histogram(img_sub, bins=bins, range=(scl_min, scl_max))[0])\n",
    "            hist_interv.append(hist)\n",
    "            st += hist\n",
    "            st += [hist.index(max(hist))] #only the smallest index w/ max value would be incl\n",
    "            st += [np.std(hist), np.max(hist), np.median(hist), (np.max(hist) - np.median(hist))]\n",
    "\n",
    "        if opt_poly:\n",
    "            for x, y in combinations(st_interv, 2):\n",
    "                st += [float(x[j]) * float(y[j]) for j in range(len(st_interv[0]))]\n",
    "\n",
    "            for x, y in combinations(hist_interv, 2):\n",
    "                hist_diff = [x[j] * y[j] for j in range(len(hist_interv[0]))]\n",
    "                st += [hist_diff.index(max(hist_diff))] #only the smallest index w/ max value would be incl\n",
    "                st += [np.std(hist_diff), np.max(hist_diff), np.median(hist_diff), (np.max(hist_diff) - np.median(hist_diff))]\n",
    "                \n",
    "        #correction\n",
    "        nan = -999\n",
    "        for i in range(len(st)):\n",
    "            if isnan(st[i]) == True:\n",
    "                st[i] = nan\n",
    "                \n",
    "    except:\n",
    "        print('except: ')\n",
    "    \n",
    "    return [img_id, st]\n",
    "\n",
    "\n",
    "def extract_img_stats(paths):\n",
    "    imf_d = {}\n",
    "    p = Pool(8) #(cpu_count())\n",
    "    ret = p.map(img_to_stats, paths)   # list of pair of (id, bands) bands is np.array shape (75, 75, 3)\n",
    "    for i in tqdm(range(len(ret)), miniters=100):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "\n",
    "    ret = []\n",
    "    fdata = [imf_d[i] for i, j in paths]\n",
    "    return np.array(fdata, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process(df, bands):\n",
    "\n",
    "    data = extract_img_stats([(k, v) for k, v in zip(df['id'].tolist(), bands)]); gc.collect() #(N, 246)\n",
    "    data = np.concatenate([data, df['inc_angle'].values[:, np.newaxis]], axis=-1); gc.collect() #(N, 247)\n",
    "\n",
    "    print(data.shape)\n",
    "    return data\n",
    "\n",
    "###############################################################################\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    np.random.seed(1017)\n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    #Load data\n",
    "    train, train_bands = read_jason(file='train.json', loc='data/processed/')\n",
    "    test, test_bands = read_jason(file='test.json', loc='data/processed/')\n",
    "\n",
    "    train_X_full = process(df=train, bands=train_bands)\n",
    "    train_y_full = train[target].values    \n",
    "    test_X = process(df=test, bands=test_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 247) (1604,)\n"
     ]
    }
   ],
   "source": [
    "# train_X= train_X_full[train_X_full[:,-1]!=-1] #train_X[:,-1]==-1\n",
    "# train_y= train_y_full[train_X_full[:,-1]!=-1]\n",
    "\n",
    "train_X= train_X_full\n",
    "train_y= train_y_full\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will be the version changed based on my own understanding\n",
    "def save_blend(preds={}, loc='./'):\n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    w_total = 0.0\n",
    "    blend = None\n",
    "    df_corr = None\n",
    "    print('\\nBlending...')\n",
    "    for k, v in preds.items():\n",
    "        if blend is None:\n",
    "            blend = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            \n",
    "            df_corr = pd.DataFrame({'id': blend['id'].tolist()})\n",
    "            df_corr[k[16:-4]] = blend[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] = blend[target] * v\n",
    "                \n",
    "        else:\n",
    "            preds_tmp = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            preds_tmp = blend[['id']].merge(preds_tmp, how='left', on='id')\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            df_corr[k[16:-4]] = preds_tmp[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] += preds_tmp[target] * v\n",
    "            del preds_tmp\n",
    "            \n",
    "    print('\\n{}'.format(df_corr.corr()), flush=True)\n",
    "    #write submission\n",
    "    blend[target] = blend[target] / w_total\n",
    "    print('\\nPreview: \\n{}'.format(blend.head()), flush=True)\n",
    "    blend.to_csv('{}subm_blend{:03d}_{}.csv'.format(loc, len(preds), tmp), index=False, float_format='%.6f')\n",
    "\n",
    "def run_lgb(params={}, lgb_train=None, lgb_valid=None, lgb_test=None, test_ids=None, nr_round=2000, min_round=100, file=''):\n",
    "\n",
    "    print('\\nLightGBM: {}'.format(params['boosting'])) \n",
    "    model2 = lgb.train(params, \n",
    "                       lgb_train, \n",
    "                       nr_round, \n",
    "                       lgb_valid, \n",
    "                       verbose_eval=50, early_stopping_rounds=min_round)\n",
    "    \n",
    "    pred = model2.predict(lgb_test, num_iteration=model2.best_iteration)\n",
    "    #\n",
    "    subm = pd.DataFrame({'id': test_ids, 'is_iceberg': pred})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    #   \n",
    "    df = pd.DataFrame({'feature':model2.feature_name(), 'importances': model2.feature_importance()})\n",
    "    \n",
    "    return pred, df\n",
    "\n",
    "\n",
    "#results\n",
    "freq = pd.DataFrame()\n",
    "subms = []\n",
    "\n",
    "#training\n",
    "# test_ratio = 0.2\n",
    "# nr_runs = 3\n",
    "# split_seed = 25\n",
    "# kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "split_seed = np.random.randint(50000,60000,size=1)\n",
    "print('The seed we are using is: %d' % split_seed)\n",
    "kf = Kfold(n_splits=5, random_state=split_seed)\n",
    "\n",
    "for r, (train_index, test_index) in enumerate(kf.split(train_X, train_y)):\n",
    "    print('\\nround {:04d} of {:04d}, seed={}'.format(r+1, nr_runs, split_seed))\n",
    "\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "    x1, x2 = train_X[train_index], train_X[test_index]\n",
    "    y1, y2 = train_y[train_index], train_y[test_index]\n",
    "    #x1, x2, y1, y2 = train_test_split(train_X, train_y, test_size=test_ratio, random_state=split_seed + r)\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    test_X_dup = test_X.copy()\n",
    "\n",
    "    #XGB\n",
    "    xgb_train = xgb.DMatrix(x1, y1)\n",
    "    xgb_valid = xgb.DMatrix(x2, y2)\n",
    "    #\n",
    "    watchlist = [(xgb_train, 'train'), (xgb_valid, 'valid')]\n",
    "    params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'seed': 99, 'silent': True}\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 4\n",
    "    params['subsample'] = 0.9\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['colsample_bytree'] = 0.8\n",
    "    params['colsample_bylevel'] = 0.8\n",
    "    params['max_delta_step'] = 3\n",
    "    #params['gamma'] = 5.0\n",
    "    #params['labmda'] = 1\n",
    "    params['scale_pos_weight'] = 1.0\n",
    "    params['seed'] = split_seed + r\n",
    "    nr_round = 2000\n",
    "    min_round = 100\n",
    "\n",
    "    model1 = xgb.train(params, \n",
    "                       xgb_train, \n",
    "                       nr_round,  \n",
    "                       watchlist, \n",
    "                       verbose_eval=50, \n",
    "                       early_stopping_rounds=min_round)\n",
    "\n",
    "    pred_xgb = model1.predict(xgb.DMatrix(test_X_dup), ntree_limit=model1.best_ntree_limit+45)\n",
    "\n",
    "    #\n",
    "    file = 'subm_{}_xgb_{:02d}.csv'.format(tmp, r+1)\n",
    "    subm = pd.DataFrame({'id': test['id'].values, target: pred_xgb})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    subms.append(file)    \n",
    "\n",
    "    ##LightGBM\n",
    "    lgb_train = lgb.Dataset(x1, label=y1, free_raw_data=False)\n",
    "    lgb_valid = lgb.Dataset(x2, label=y2, reference=lgb_train, free_raw_data=False)\n",
    "    #gbdt\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'gbdt'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "    ##LightGBM\n",
    "    #dart\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'dart'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.04\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #dart\n",
    "    params['drop_rate'] = 0.1\n",
    "    params['skip_drop'] = 0.5\n",
    "    params['max_drop'] = 10\n",
    "    params['verbose'] = -1 \n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "\n",
    "#blending\n",
    "preds = {k: 1.0 for k in subms}\n",
    "save_blend(preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m.\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34m..\u001b[m\u001b[m\r\n",
      ".DS_Store\r\n",
      "\u001b[1m\u001b[34m.git\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "36_plain_cnn.csv\r\n",
      "41_plain_cnn.csv\r\n",
      "50_plain_fcn.csv\r\n",
      "67_plain_cnn.csv\r\n",
      "6_retrain_inception.csv\r\n",
      "Image preprocess testing.ipynb\r\n",
      "README.md\r\n",
      "Ship-Iceberg Discrimination with Convolutional Neural Networks in High Resolution SAR Images.pdf\r\n",
      "The Effectiveness of Data Augmentation in Image Classification using Deep Learning.pdf\r\n",
      "Training_log.ipynb\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "all_14_inception.csv\r\n",
      "cnn.ipynb\r\n",
      "cnn.py\r\n",
      "cnn_angle.ipynb\r\n",
      "cnn_angle.py\r\n",
      "\u001b[1m\u001b[34mdata\u001b[m\u001b[m\r\n",
      "densenet.py\r\n",
      "densenet121.ipynb\r\n",
      "densenet121_pseudl.ipynb\r\n",
      "densenetBC.py\r\n",
      "densenetbc100.ipynb\r\n",
      "fcn.ipynb\r\n",
      "fcn.py\r\n",
      "final ensemble.ipynb\r\n",
      "gbm.ipynb\r\n",
      "inception.ipynb\r\n",
      "inception.py\r\n",
      "\u001b[1m\u001b[34mothers\u001b[m\u001b[m\r\n",
      "pre_resnet.py\r\n",
      "pre_vgg.py\r\n",
      "r2_11_plain_cnn.csv\r\n",
      "r2_fcn_11_models.csv\r\n",
      "resnet.py\r\n",
      "resnet101.ipynb\r\n",
      "resnet101_4feat.ipynb\r\n",
      "resnet152.ipynb\r\n",
      "resnet18.ipynb\r\n",
      "resnet34.ipynb\r\n",
      "resnet34_4feat.ipynb\r\n",
      "resnet34_onlygoodretrain.csv\r\n",
      "resnet34_pseudolb.ipynb\r\n",
      "resnet34_retrain_all.csv\r\n",
      "resnet50.ipynb\r\n",
      "resnet50_temp.ipynb\r\n",
      "save_for_use.csv\r\n",
      "squeezenet.py\r\n",
      "squeezenet1.ipynb\r\n",
      "submissionlasttd.csv\r\n",
      "test.txt\r\n",
      "utils.py\r\n",
      "vgg.ipynb\r\n",
      "vgg.py\r\n",
      "vgg_mobile.py\r\n"
     ]
    }
   ],
   "source": [
    "def save_blend(preds={}, loc='./'):\n",
    "    \n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    w_total = 0.0\n",
    "    blend = None\n",
    "    df_corr = None\n",
    "    print('\\nBlending...')\n",
    "    for k, v in preds.items():\n",
    "        if blend is None:\n",
    "            blend = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            \n",
    "            df_corr = pd.DataFrame({'id': blend['id'].tolist()})\n",
    "            df_corr[k[16:-4]] = blend[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] = blend[target] * v\n",
    "                \n",
    "        else:\n",
    "            preds_tmp = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            preds_tmp = blend[['id']].merge(preds_tmp, how='left', on='id')\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            df_corr[k[16:-4]] = preds_tmp[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] += preds_tmp[target] * v\n",
    "            del preds_tmp\n",
    "            \n",
    "    print('\\n{}'.format(df_corr.corr()), flush=True)\n",
    "    #write submission\n",
    "    blend[target] = blend[target] / w_total\n",
    "    print('\\nPreview: \\n{}'.format(blend.head()), flush=True)\n",
    "    blend.to_csv('{}subm_blend{:03d}_{}.csv'.format(loc, len(preds), tmp), index=False, float_format='%.6f')\n",
    "\n",
    "def run_lgb(params={}, lgb_train=None, lgb_valid=None, lgb_test=None, test_ids=None, nr_round=2000, min_round=100, file=''):\n",
    "\n",
    "    print('\\nLightGBM: {}'.format(params['boosting'])) \n",
    "    model2 = lgb.train(params, \n",
    "                       lgb_train, \n",
    "                       nr_round, \n",
    "                       lgb_valid, \n",
    "                       verbose_eval=50, early_stopping_rounds=min_round)\n",
    "    \n",
    "    pred = model2.predict(lgb_test, num_iteration=model2.best_iteration)\n",
    "    #\n",
    "    subm = pd.DataFrame({'id': test_ids, 'is_iceberg': pred})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    #   \n",
    "    df = pd.DataFrame({'feature':model2.feature_name(), 'importances': model2.feature_importance()})\n",
    "    \n",
    "    return pred, df\n",
    "\n",
    "\n",
    "#results\n",
    "freq = pd.DataFrame()\n",
    "subms = []\n",
    "\n",
    "#training\n",
    "test_ratio = 0.2\n",
    "nr_runs = 3\n",
    "split_seed = 25\n",
    "kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "\n",
    "for r, (train_index, test_index) in enumerate(kf.split(train_X, train_y)):\n",
    "    print('\\nround {:04d} of {:04d}, seed={}'.format(r+1, nr_runs, split_seed))\n",
    "\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "    x1, x2 = train_X[train_index], train_X[test_index]\n",
    "    y1, y2 = train_y[train_index], train_y[test_index]\n",
    "    #x1, x2, y1, y2 = train_test_split(train_X, train_y, test_size=test_ratio, random_state=split_seed + r)\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    test_X_dup = test_X.copy()\n",
    "\n",
    "    #XGB\n",
    "    xgb_train = xgb.DMatrix(x1, y1)\n",
    "    xgb_valid = xgb.DMatrix(x2, y2)\n",
    "    #\n",
    "    watchlist = [(xgb_train, 'train'), (xgb_valid, 'valid')]\n",
    "    params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'seed': 99, 'silent': True}\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 4\n",
    "    params['subsample'] = 0.9\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['colsample_bytree'] = 0.8\n",
    "    params['colsample_bylevel'] = 0.8\n",
    "    params['max_delta_step'] = 3\n",
    "    #params['gamma'] = 5.0\n",
    "    #params['labmda'] = 1\n",
    "    params['scale_pos_weight'] = 1.0\n",
    "    params['seed'] = split_seed + r\n",
    "    nr_round = 2000\n",
    "    min_round = 100\n",
    "\n",
    "    model1 = xgb.train(params, \n",
    "                       xgb_train, \n",
    "                       nr_round,  \n",
    "                       watchlist, \n",
    "                       verbose_eval=50, \n",
    "                       early_stopping_rounds=min_round)\n",
    "\n",
    "    pred_xgb = model1.predict(xgb.DMatrix(test_X_dup), ntree_limit=model1.best_ntree_limit+45)\n",
    "\n",
    "    #\n",
    "    file = 'subm_{}_xgb_{:02d}.csv'.format(tmp, r+1)\n",
    "    subm = pd.DataFrame({'id': test['id'].values, target: pred_xgb})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    subms.append(file)    \n",
    "\n",
    "    ##LightGBM\n",
    "    lgb_train = lgb.Dataset(x1, label=y1, free_raw_data=False)\n",
    "    lgb_valid = lgb.Dataset(x2, label=y2, reference=lgb_train, free_raw_data=False)\n",
    "    #gbdt\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'gbdt'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "    ##LightGBM\n",
    "    #dart\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'dart'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['learning_rate'] = 0.04\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #dart\n",
    "    params['drop_rate'] = 0.1\n",
    "    params['skip_drop'] = 0.5\n",
    "    params['max_drop'] = 10\n",
    "    params['verbose'] = -1 \n",
    "\n",
    "    file = 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "\n",
    "#blending\n",
    "preds = {k: 1.0 for k in subms}\n",
    "save_blend(preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=  torch.Tensor([1,2,3])\n",
    "a.size()\n",
    "a=a.unsqueeze(1)\n",
    "a.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
