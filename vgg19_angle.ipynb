{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau,StepLR\n",
    "#torch.multiprocessing.set_start_method(\"spawn\")\n",
    "import vgg_fcn\n",
    "import vgg\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import progress_bar\n",
    "from skimage import transform as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_dir = 'data/processed/'\n",
    "\n",
    "train = pd.read_json(BASE_dir + 'train.json')\n",
    "#test = pd.read_json(BASE_dir + 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data\n",
    "use_cuda= True if torch.cuda.is_available() else False\n",
    "#use_cuda =False\n",
    "#dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor \n",
    "dtype = torch.FloatTensor \n",
    "data=  pd.read_json(BASE_dir + 'train.json')\n",
    "\n",
    "class iceberg_dataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label=  self.data[idx], self.label[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "                rotate = np.random.randint(4, size=1)\n",
    "                if rotate:\n",
    "                    img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                \n",
    "                scale1 = np.exp(np.random.uniform(np.log(1/1.1), np.log(1.1)))\n",
    "                tran = np.random.uniform(-5, 5)\n",
    "                aug = tf.AffineTransform(translation=tran, scale= (scale1, scale1))\n",
    "                img = tf.warp(img, inverse_map=aug)\n",
    "                pass\n",
    "#             temp = []\n",
    "#             for i in img:\n",
    "#                 temp.append(tf.rescale(i,224/75,mode='constant'))\n",
    "#             img = np.stack(temp)\n",
    "            img = torch.from_numpy(img).type(dtype)\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "class iceberg_angle_dataset(Dataset):\n",
    "    def __init__(self, data,angle,label,size=None, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "#         self.angle=torch.cat( (torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1),torch.from_numpy(size).type(torch.FloatTensor).unsqueeze(1)),1)\n",
    "        self.angle=torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1)\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label, angle=  self.data[idx], self.label[idx], self.angle[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            \n",
    "            #rotate, scale, shear, translation\n",
    "#             if self.test is False:\n",
    "#                 angle = np.random.uniform(0,360)\n",
    "#                 img = tf.rotate(img,angle=angle,resize=False)\n",
    "#                 scale1 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.2)))\n",
    "#                 scale2 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.1)))\n",
    "#                 #shear = np.random.uniform(-np.pi/18, np.pi/18)\n",
    "#                 #tran = np.random.uniform(-5, 5)\n",
    "#                 #aug = tf.AffineTransform(shear = shear, translation=tran, scale= (scale1, scale2))\n",
    "#                 aug = tf.AffineTransform(scale= (scale1, scale2))\n",
    "#                 img = tf.warp(img, inverse_map=aug)\n",
    "            \n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "            \n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "                rotate = np.random.randint(4, size=1)\n",
    "                if rotate:\n",
    "                    img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "            pass\n",
    "        img = torch.from_numpy(img).type(dtype)\n",
    "#         img = self.transform(img)\n",
    "\n",
    "        return img, angle,label    \n",
    "    \n",
    "    \n",
    "def stack(row):\n",
    "    return np.stack(row[['c1','c2','c3']]).reshape(3,75,75)\n",
    "\n",
    "def raw_to_numpy(data):\n",
    "    img = []\n",
    "    data['c1'] = data['band_1'].apply(np.array)\n",
    "    data['c2'] = data['band_2'].apply(np.array)\n",
    "    data['c3'] = (data['c1'] + data['c2'])/2\n",
    "#     data['c3'] = (data['c1'] + data['c2'])/2\n",
    "    for _, row in data.iterrows():\n",
    "        img.append(stack(row))\n",
    "    return np.stack(img)\n",
    "\n",
    "def transform_compute(img):\n",
    "    train_mean = img.mean(axis=(0,2,3))\n",
    "    train_std = img.std(axis=(0,2,3))\n",
    "    return train_mean, train_std\n",
    "\n",
    "def data_aug(X, y):    \n",
    "    X_rot_30 = []\n",
    "    X_rot_60 = [] \n",
    "    X_h = np.flip(X, 3)\n",
    "    X_v = np.flip(X, 2)\n",
    "    for i in X:\n",
    "        X_rot_30.append(tf.rotate(i,angle=90,resize=False))\n",
    "        X_rot_60.append(tf.rotate(i,angle=270,resize=False))\n",
    "        \n",
    "    X_rot_30 = np.stack(X_rot_30)\n",
    "    X_rot_60 = np.stack(X_rot_60)\n",
    "    ch_y = np.concatenate((y,y,y,y,y))\n",
    "    ch_X = np.concatenate((X, X_h, X_v, X_rot_30, X_rot_60))\n",
    "    return ch_X, ch_y\n",
    "\n",
    "train_X = raw_to_numpy(data)#.transpose(0,2,3,1)\n",
    "train_X.shape     #1604 * 3 *75* 75   N*c*H*W\n",
    "train_y = data['is_iceberg'].values # if iceberg then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are 0\n",
      "We are 50\n",
      "We are 100\n",
      "We are 150\n",
      "We are 200\n",
      "We are 250\n",
      "We are 300\n",
      "We are 350\n",
      "We are 400\n",
      "We are 450\n",
      "We are 500\n",
      "We are 550\n",
      "We are 600\n",
      "We are 650\n",
      "We are 700\n",
      "We are 750\n",
      "We are 800\n",
      "We are 850\n",
      "We are 900\n",
      "We are 950\n",
      "We are 1000\n",
      "We are 1050\n",
      "We are 1100\n",
      "We are 1150\n",
      "We are 1200\n",
      "We are 1250\n",
      "We are 1300\n",
      "We are 1350\n",
      "We are 1400\n",
      "We are 1450\n",
      "We are 1500\n",
      "We are 1550\n",
      "We are 1600\n"
     ]
    }
   ],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "result = []\n",
    "for num,i in enumerate(train_X_del):\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(tf.rescale(j,224/75,mode='constant'))\n",
    "    img = np.stack(temp)\n",
    "    result.append(img)\n",
    "    if num%50==0:\n",
    "        print('We are %d'%num)\n",
    "train_X_del = np.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_index=list(range(1300))\n",
    "# val_index= list(range(1300,1604))\n",
    "# train_index=list(range(304,1604)) \n",
    "# val_index= list(range(304))\n",
    "# # train_X[train_index].shape\n",
    "\n",
    "# # data.inc_angle = data.inc_angle.map(lambda x: 0.0 if x == 'na' else x)\n",
    "# # train_index = np.where(data.inc_angle > 0)[0]\n",
    "# # val_index = np.where(data.inc_angle <= 0)[0]\n",
    "\n",
    "# # seed= np.random.RandomState(123)\n",
    "# # spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# # train_index, val_index = next(spliter.split(train_X))\n",
    "# train_mean, train_std = transform_compute(train_X[train_index])\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X[train_index], label=train_y[train_index], transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "## add augmentation \n",
    "# seed= np.random.RandomState(123)\n",
    "# spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# train_index, val_index = next(spliter.split(train_X))\n",
    "\n",
    "# train_X_af,train_y_af = data_aug(train_X[train_index], train_y[train_index])\n",
    "# train_mean, train_std = transform_compute(train_X_af)\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X_af, label=train_y_af, transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "# train_X_del = train_X[data.inc_angle!='na',:,:,:]\n",
    "# train_y_del = train_y[data.inc_angle!='na']\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "# af_train_X, af_train_y = data_aug(train_X_del[train_index], train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "af_train_X, af_train_y = train_X_del[train_index], train_y_del[train_index]\n",
    "\n",
    "train_dataset = iceberg_dataset(data= af_train_X, label=af_train_y, transform=train_transform)\n",
    "val_dataset = iceberg_dataset(data= train_X_del[val_index], label=train_y_del[val_index], transform=train_transform, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## angle and size\n",
    "\n",
    "#data['inc_angle'][data['inc_angle']=='na']=0\n",
    "data.loc[data['inc_angle']=='na', 'inc_angle'] = 0\n",
    "\n",
    "train_X = train_X\n",
    "train_angle_del = data['inc_angle'].values\n",
    "train_angle = train_angle_del.astype(np.float)\n",
    "#train_size = train['size'].values\n",
    "train_y = train_y\n",
    "\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "#af_train_X,af_train_angle, af_train_y = data_aug(train_X_del[train_index], train_angle_del[train_index],train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "\n",
    "\n",
    "train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                    label=train_y[train_index],\n",
    "                                    transform=train_transform, test=False)\n",
    "\n",
    "val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                    label=train_y[val_index],\n",
    "                                    transform=train_transform, test=True)\n",
    "\n",
    "# train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],size=train_size[train_index],\n",
    "#                                     label=train_y[train_index],\n",
    "#                                     transform=train_transform)\n",
    "\n",
    "# val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],size= train_size[val_index],\n",
    "#                                     label=train_y[val_index],\n",
    "#                                     transform=train_transform, test=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.from_numpy(train_X).type(torch.FloatTensor)[1].shape\n",
    "train_X[1]\n",
    "use_cuda\n",
    "# for i in train_loader:\n",
    "#     print(i.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "#         scheduler.step()\n",
    "        scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        net.train()\n",
    "        for j,(batch_x, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            y = Variable(batch_y)\n",
    "            out = net(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                acc= best_acc+ 0.01\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total > val_loss:\n",
    "                e_s = 0\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x, val_y) in enumerate(val_load):\n",
    "        #len(val_x.size())==1\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        \n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        if len(out.size())==1: #in case it's one dimensional\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####train with angle and other\n",
    "\n",
    "\n",
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = pre_vgg.vgg16_bn(pretrained=True)\n",
    "for param in vgg16.parameters():\n",
    "    print(param.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for  i in net.features:\n",
    "#     print(i)\n",
    "#     break\n",
    "# for i in i.parameters():\n",
    "#     print(i)\n",
    "len(net.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.577 | Acc: 67.108% (861/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.518 | Acc: 74.143% (238/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.422 | Acc: 82.463% (1058/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.680 | Acc: 68.224% (219/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.356 | Acc: 85.659% (1099/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 3.991 | Acc: 50.467% (162/321)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.398 | Acc: 83.164% (1067/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.369 | Acc: 83.178% (267/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.335 | Acc: 86.750% (1113/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.362 | Acc: 85.047% (273/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.329 | Acc: 85.892% (1102/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.327 | Acc: 82.866% (266/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.283 | Acc: 88.075% (1130/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.371 | Acc: 84.424% (271/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.284 | Acc: 87.685% (1125/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.322 | Acc: 85.047% (273/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.289 | Acc: 88.309% (1133/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.312 | Acc: 87.850% (282/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.269 | Acc: 88.542% (1136/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.609 | Acc: 73.209% (235/321)\n",
      "\n",
      "This is epoch:11\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.251 | Acc: 91.348% (1172/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.247 | Acc: 89.408% (287/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.239 | Acc: 91.348% (1172/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.244 | Acc: 89.097% (286/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.215 | Acc: 91.816% (1178/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.242 | Acc: 88.785% (285/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.215 | Acc: 92.050% (1181/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.257 | Acc: 89.097% (286/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.214 | Acc: 92.206% (1183/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 89.408% (287/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.213 | Acc: 92.362% (1185/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 90.343% (290/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.219 | Acc: 91.193% (1170/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.265 | Acc: 88.474% (284/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.204 | Acc: 92.673% (1189/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.272 | Acc: 87.539% (281/321)\n",
      "\n",
      "This is epoch:19\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.218 | Acc: 91.270% (1171/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.244 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.212 | Acc: 92.284% (1184/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.253 | Acc: 89.097% (286/321)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.196 | Acc: 92.907% (1192/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.240 | Acc: 90.031% (289/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.206 | Acc: 91.894% (1179/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.240 | Acc: 89.408% (287/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.214 | Acc: 91.816% (1178/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 90.654% (291/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.210 | Acc: 92.128% (1182/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 90.966% (292/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.204 | Acc: 92.128% (1182/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 89.408% (287/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.217 | Acc: 92.673% (1189/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.343% (290/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.186 | Acc: 93.297% (1197/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.210 | Acc: 91.660% (1176/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.242 | Acc: 88.785% (285/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.216 | Acc: 92.206% (1183/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.205 | Acc: 92.206% (1183/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 89.720% (288/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:31\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.198 | Acc: 92.595% (1188/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.200 | Acc: 92.673% (1189/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.205 | Acc: 91.972% (1180/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.247 | Acc: 88.162% (283/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.199 | Acc: 91.816% (1178/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.210 | Acc: 91.816% (1178/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.240 | Acc: 89.097% (286/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.199 | Acc: 92.440% (1186/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.235 | Acc: 90.343% (290/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.214 | Acc: 92.128% (1182/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.216 | Acc: 91.504% (1174/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 90.031% (289/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.202 | Acc: 92.050% (1181/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.199 | Acc: 92.751% (1190/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.190 | Acc: 93.141% (1195/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.238 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.199 | Acc: 93.063% (1194/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.233 | Acc: 91.504% (1174/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.208 | Acc: 92.128% (1182/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.204 | Acc: 91.816% (1178/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.238 | Acc: 90.654% (291/321)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.201 | Acc: 92.362% (1185/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.236 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.195 | Acc: 93.063% (1194/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.233 | Acc: 90.031% (289/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:48\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.202 | Acc: 92.050% (1181/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.197 | Acc: 93.141% (1195/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.211 | Acc: 92.518% (1187/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 89.408% (287/321)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.215 | Acc: 92.128% (1182/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.238 | Acc: 89.408% (287/321)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.197 | Acc: 92.985% (1193/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.238 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.202 | Acc: 93.141% (1195/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.209 | Acc: 92.907% (1192/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.191 | Acc: 93.141% (1195/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.202 | Acc: 92.673% (1189/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.237 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:57\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.198 | Acc: 92.985% (1193/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 90.343% (290/321)\n",
      "\n",
      "This is epoch:58\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.200 | Acc: 92.518% (1187/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.240 | Acc: 91.589% (294/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:59\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.219 | Acc: 92.206% (1183/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.245 | Acc: 88.785% (285/321)\n",
      "\n",
      "This is epoch:60\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.202 | Acc: 92.907% (1192/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.235 | Acc: 90.343% (290/321)\n"
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "result=[]\n",
    "for i in range(1):\n",
    "    vgg19_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "    num = 256\n",
    "    vgg19_bn.classifier = nn.Sequential(\n",
    "                nn.Linear(512+1, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(num, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(num, 2)\n",
    "            )\n",
    "\n",
    "    net= vgg19_bn\n",
    "    # net.load_state_dict(torch.load('vgg_fcn_loss.pth'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # #Adam does not perform so good here   \n",
    "    # #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "    # optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "    scheduler = MultiStepLR(optimizer, [10,20,30], gamma=0.1)\n",
    "#     scheduler = MultiStepLR(optimizer, [10,18,26], gamma=0.1)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'max', patience =3,min_lr= 0.00001)\n",
    "    #5e-3 86\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "        net.cuda()\n",
    "    #     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "    #     cudnn.benchmark = True   \n",
    "\n",
    "    a = train(epoch=60,early_stopping =20)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21936874226246297, 91.27725856697819, 39),\n",
       " (0.21782404165773006, 91.58878504672897, 55),\n",
       " (0.22525541061924254, 90.96573208722741, 53)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24811193430535147, 90.03115264797508, 29),\n",
       " (0.21092650229314405, 91.58878504672897, 28),\n",
       " (0.22480700989007207, 90.65420560747664, 53)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.218 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 93.146% (299/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.082 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.225 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.086 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:11\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.091 | Acc: 96.648% (1240/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.084 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:16\n",
      "lr change from 0.000001 to 0.000000\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.038% (1245/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.090 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:21\n",
      "[========>  29/ 81 .......]Step: 0ms| Tot: 1s|Loss: 0.075 | Acc: 96.552% (448/464))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8923:\n",
      "Process Process-8924:\n",
      "Process Process-8925:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-77835431faca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e8875581f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "net.load_state_dict(torch.load('cnn_ang_loss.pth'))\n",
    "for i in vgg16_bn.features:\n",
    "    i.requires_grad = False\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #Adam does not perform so good here   \n",
    "# #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "# optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "scheduler = MultiStepLR(optimizer, [5,10,15], gamma=0.1)\n",
    "# scheduler = MultiStepLR(optimizer, [8,18], gamma=0.1)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'max', patience =10,min_lr= 0.0001)\n",
    "#5e-3 86\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "    net.cuda()\n",
    "#     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "#     cudnn.benchmark = True   \n",
    "\n",
    "train(epoch=250,early_stopping =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 3, 75, 75)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test_set)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k =np.stack(result).mean(axis=0)\n",
    "# #sub.shape\n",
    "# result[1].shape\n",
    "# np.concatenate(prob).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test_set['id']\n",
    "sub['is_iceberg'] =  np.concatenate(prob)\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>is_iceberg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.886197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg  is_iceberg2\n",
       "is_iceberg     1.000000     0.886197\n",
       "is_iceberg2    0.886197     1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp= pd.read_csv('submission3.csv') #0.0001 wd one\n",
    "sub['is_iceberg2'] = temp['is_iceberg']\n",
    "sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    ran_num = 1024\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        \n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=False)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(2):\n",
    "            vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [10,20,30], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            with open(\"vgg19_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase1,with aug, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg19_loss.pth vgg19_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "        \n",
    "        for g in range(2):\n",
    "            cmd = 'cp vgg19_loss{}.pth vgg19_models/r1_10vgg_aug{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    ran_num = 1290\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        \n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(2):\n",
    "            vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [5,11,20], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            with open(\"vgg19_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase2, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg19_loss.pth vgg19_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "        \n",
    "        for g in range(2):\n",
    "            cmd = 'cp vgg19_loss{}.pth vgg19_models/r2_10vgg{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vgg_models/r1_5vgg0_00.pth', 'vgg_models/r1_5vgg0_11.pth', 'vgg_models/r1_5vgg0_20.pth', 'vgg_models/r1_5vgg0_31.pth', 'vgg_models/r1_5vgg0_40.pth']\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6mss\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "waiting_list=  \n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "vgg16_bn = vgg_fcn.vgg16_bn(pretrained=True)#copy.deepcopy(vgg16)\n",
    "# vgg16_bn.avg= nn.Conv2d(512, 512, kernel_size=2,\n",
    "#                                bias=False)\n",
    "\n",
    "# vgg16_bn.classifier = nn.Sequential(\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.6),\n",
    "#             nn.Linear(512, 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Conv2d(512,512, kernel_size= 3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Conv2d(512, 2, kernel_size=3, padding=1,\n",
    "                               bias=False),\n",
    "            nn.AvgPool2d(3)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "\n",
    "print(waiting_list)\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s5ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s5ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s5ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 6s4ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle=test.inc_angle.values.astype(np.float), transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "num = 256\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(num, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "# waiting_list = ['r1_10vgg0_00.pth',\n",
    "#     'r1_10vgg0_10.pth',\n",
    "#     'r1_10vgg0_21.pth',\n",
    "#     'r1_10vgg0_31.pth',\n",
    "#                 'r1_10vgg0_41.pth',\n",
    "#                 'r1_10vgg0_51.pth',\n",
    "#                 'r1_10vgg0_60.pth',\n",
    "#                 'r1_10vgg0_70.pth',\n",
    "#                 'r1_10vgg0_81.pth',\n",
    "#                 'r1_10vgg0_90.pth']\n",
    "\n",
    "waiting_list = ['r2_10vgg0_00.pth',\n",
    "    'r2_10vgg0_10.pth',\n",
    "    'r2_10vgg0_21.pth',\n",
    "    'r2_10vgg0_30.pth',\n",
    "                'r2_10vgg0_41.pth',\n",
    "                'r2_10vgg0_50.pth',\n",
    "                'r2_10vgg0_61.pth',\n",
    "                'r2_10vgg0_71.pth',\n",
    "                'r2_10vgg0_81.pth',\n",
    "                'r2_10vgg0_90.pth']\n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models') if 'r3' in i]\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg19_models', i) for i in waiting_list] \n",
    "net= vgg16_bn\n",
    "\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(test_loader):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg6</th>\n",
       "      <th>is_iceberg7</th>\n",
       "      <th>is_iceberg8</th>\n",
       "      <th>is_iceberg9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849149</td>\n",
       "      <td>0.868695</td>\n",
       "      <td>0.881311</td>\n",
       "      <td>0.840994</td>\n",
       "      <td>0.840279</td>\n",
       "      <td>0.911320</td>\n",
       "      <td>0.913760</td>\n",
       "      <td>0.862159</td>\n",
       "      <td>0.877924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.849149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843577</td>\n",
       "      <td>0.885951</td>\n",
       "      <td>0.904536</td>\n",
       "      <td>0.903572</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.903304</td>\n",
       "      <td>0.877240</td>\n",
       "      <td>0.900565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.868695</td>\n",
       "      <td>0.843577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856969</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.875933</td>\n",
       "      <td>0.859779</td>\n",
       "      <td>0.839132</td>\n",
       "      <td>0.807777</td>\n",
       "      <td>0.892925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.881311</td>\n",
       "      <td>0.885951</td>\n",
       "      <td>0.856969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865349</td>\n",
       "      <td>0.849464</td>\n",
       "      <td>0.900896</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.890439</td>\n",
       "      <td>0.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.840994</td>\n",
       "      <td>0.904536</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.865349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919091</td>\n",
       "      <td>0.892540</td>\n",
       "      <td>0.846711</td>\n",
       "      <td>0.831919</td>\n",
       "      <td>0.925033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.840279</td>\n",
       "      <td>0.903572</td>\n",
       "      <td>0.875933</td>\n",
       "      <td>0.849464</td>\n",
       "      <td>0.919091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897984</td>\n",
       "      <td>0.857921</td>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.920130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg6</th>\n",
       "      <td>0.911320</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.859779</td>\n",
       "      <td>0.900896</td>\n",
       "      <td>0.892540</td>\n",
       "      <td>0.897984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.924744</td>\n",
       "      <td>0.882315</td>\n",
       "      <td>0.919925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg7</th>\n",
       "      <td>0.913760</td>\n",
       "      <td>0.903304</td>\n",
       "      <td>0.839132</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.846711</td>\n",
       "      <td>0.857921</td>\n",
       "      <td>0.924744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.883764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg8</th>\n",
       "      <td>0.862159</td>\n",
       "      <td>0.877240</td>\n",
       "      <td>0.807777</td>\n",
       "      <td>0.890439</td>\n",
       "      <td>0.831919</td>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.882315</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg9</th>\n",
       "      <td>0.877924</td>\n",
       "      <td>0.900565</td>\n",
       "      <td>0.892925</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.925033</td>\n",
       "      <td>0.920130</td>\n",
       "      <td>0.919925</td>\n",
       "      <td>0.883764</td>\n",
       "      <td>0.846793</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  is_iceberg4  \\\n",
       "is_iceberg0     1.000000     0.849149     0.868695     0.881311     0.840994   \n",
       "is_iceberg1     0.849149     1.000000     0.843577     0.885951     0.904536   \n",
       "is_iceberg2     0.868695     0.843577     1.000000     0.856969     0.873840   \n",
       "is_iceberg3     0.881311     0.885951     0.856969     1.000000     0.865349   \n",
       "is_iceberg4     0.840994     0.904536     0.873840     0.865349     1.000000   \n",
       "is_iceberg5     0.840279     0.903572     0.875933     0.849464     0.919091   \n",
       "is_iceberg6     0.911320     0.924978     0.859779     0.900896     0.892540   \n",
       "is_iceberg7     0.913760     0.903304     0.839132     0.903965     0.846711   \n",
       "is_iceberg8     0.862159     0.877240     0.807777     0.890439     0.831919   \n",
       "is_iceberg9     0.877924     0.900565     0.892925     0.897196     0.925033   \n",
       "\n",
       "             is_iceberg5  is_iceberg6  is_iceberg7  is_iceberg8  is_iceberg9  \n",
       "is_iceberg0     0.840279     0.911320     0.913760     0.862159     0.877924  \n",
       "is_iceberg1     0.903572     0.924978     0.903304     0.877240     0.900565  \n",
       "is_iceberg2     0.875933     0.859779     0.839132     0.807777     0.892925  \n",
       "is_iceberg3     0.849464     0.900896     0.903965     0.890439     0.897196  \n",
       "is_iceberg4     0.919091     0.892540     0.846711     0.831919     0.925033  \n",
       "is_iceberg5     1.000000     0.897984     0.857921     0.800030     0.920130  \n",
       "is_iceberg6     0.897984     1.000000     0.924744     0.882315     0.919925  \n",
       "is_iceberg7     0.857921     0.924744     1.000000     0.884420     0.883764  \n",
       "is_iceberg8     0.800030     0.882315     0.884420     1.000000     0.846793  \n",
       "is_iceberg9     0.920130     0.919925     0.883764     0.846793     1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()\n",
    "# [i for i in os.listdir('vgg_models') if 'r3' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s494ms\n",
      "[=================== 132/132 ================>]  Step: 160ms | Tot: 27s661ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s644ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s598ms\n",
      "[=================== 132/132 ================>]  Step: 161ms | Tot: 27s668ms\n"
     ]
    }
   ],
   "source": [
    "#result_hist\n",
    "\n",
    "temp11 = pd.DataFrame()\n",
    "\n",
    "for i in range(5):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load('resnet34_acc%d.pth'%i))\n",
    "    net.cuda()\n",
    "\n",
    "    test = pd.read_json(BASE_dir + 'test.json')\n",
    "    test_X = raw_to_numpy(test)\n",
    "    test_X.shape \n",
    "    fake_label = np.zeros(len(test_X))\n",
    "\n",
    "    test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' %i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11.median(axis=1)\n",
    "sub.shape\n",
    "sub.to_csv('submission23.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, 0:6].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, 0:6].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, 0:6].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg5']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg5']\n",
    "sub.shape\n",
    "sub.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s704ms\n"
     ]
    }
   ],
   "source": [
    "net = resnet.resnet34(num_classes=2)\n",
    "net.load_state_dict(torch.load('save_resnet34_acc117.pth'))\n",
    "net.cuda()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "prob = [] \n",
    "net.eval()\n",
    "for k, (val_x, val_y) in enumerate(test_loader):\n",
    "    if use_cuda:\n",
    "        val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    x = Variable(val_x)\n",
    "    y = Variable(val_y)\n",
    "    out = net(x)\n",
    "    #prevent overflow\n",
    "    temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "    ans= temp[:,1]/(temp.sum(axis=1))\n",
    "    prob.append(ans)\n",
    "    #print(out.size())\n",
    "    progress_bar(k, len(test_loader))\n",
    "msg = 'is_iceberg%d' %5\n",
    "temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.852644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.816630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.785453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.559032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.827514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.655435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.895989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.897011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.816630</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.559032</td>\n",
       "      <td>0.827514</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.655435</td>\n",
       "      <td>0.895989</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  \\\n",
       "is_iceberg0           1.000000     0.852644     0.822586     0.648968   \n",
       "is_iceberg1           0.852644     1.000000     0.905401     0.754710   \n",
       "is_iceberg2           0.822586     0.905401     1.000000     0.771766   \n",
       "is_iceberg3           0.648968     0.754710     0.771766     1.000000   \n",
       "is_iceberg4           0.883101     0.833295     0.774018     0.685649   \n",
       "is_iceberg5           0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg_max        0.682861     0.821258     0.847868     0.940914   \n",
       "is_iceberg_min        0.922862     0.777728     0.738630     0.592617   \n",
       "is_iceberg_median     0.942663     0.956190     0.918857     0.749656   \n",
       "is_iceberg_base       0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg            0.905900     0.816630     0.785453     0.559032   \n",
       "\n",
       "                   is_iceberg4  is_iceberg5  is_iceberg_max  is_iceberg_min  \\\n",
       "is_iceberg0           0.883101     0.905277        0.682861        0.922862   \n",
       "is_iceberg1           0.833295     0.815734        0.821258        0.777728   \n",
       "is_iceberg2           0.774018     0.784324        0.847868        0.738630   \n",
       "is_iceberg3           0.685649     0.556919        0.940914        0.592617   \n",
       "is_iceberg4           1.000000     0.826391        0.685683        0.920097   \n",
       "is_iceberg5           0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg_max        0.685683     0.653849        1.000000        0.583326   \n",
       "is_iceberg_min        0.920097     0.895245        0.583326        1.000000   \n",
       "is_iceberg_median     0.909537     0.896220        0.792055        0.875356   \n",
       "is_iceberg_base       0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg            0.827514     0.999683        0.655435        0.895989   \n",
       "\n",
       "                   is_iceberg_median  is_iceberg_base  is_iceberg  \n",
       "is_iceberg0                 0.942663         0.905277    0.905900  \n",
       "is_iceberg1                 0.956190         0.815734    0.816630  \n",
       "is_iceberg2                 0.918857         0.784324    0.785453  \n",
       "is_iceberg3                 0.749656         0.556919    0.559032  \n",
       "is_iceberg4                 0.909537         0.826391    0.827514  \n",
       "is_iceberg5                 0.896220         1.000000    0.999683  \n",
       "is_iceberg_max              0.792055         0.653849    0.655435  \n",
       "is_iceberg_min              0.875356         0.895245    0.895989  \n",
       "is_iceberg_median           1.000000         0.896220    0.897011  \n",
       "is_iceberg_base             0.896220         1.000000    0.999683  \n",
       "is_iceberg                  0.897011         0.999683    1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  5,  6,  7,  8,  9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22,\n",
      "       23, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42,\n",
      "       44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62,\n",
      "       63, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 84, 85,\n",
      "       86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99]), array([ 0,  3,  4, 11, 14, 17, 24, 29, 40, 43, 48, 59, 64, 66, 70, 79, 82,\n",
      "       83, 93, 97]))\n"
     ]
    }
   ],
   "source": [
    "seed= np.random.RandomState(67)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "for i in spliter.split(list(range(100))):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s222ms Step: 200ms | Tot: 2s492ms  Step: 199ms | Tot: 2s692ms  Step: 200ms | Tot: 7s692ms  Step: 199ms | Tot: 13s642ms  Step: 200ms | Tot: 14s841ms  Step: 200ms | Tot: 24s62ms\n",
      "[=================== 132/132 ================>]  Step: 152ms | Tot: 26s199ms Step: 199ms | Tot: 1s425ms  Step: 200ms | Tot: 1s626ms  Step: 200ms | Tot: 8s236ms  Step: 200ms | Tot: 12s639ms  Step: 200ms | Tot: 14s844ms  Step: 199ms | Tot: 16s449ms  Step: 199ms | Tot: 17s647ms  Step: 199ms | Tot: 20s857ms  Step: 200ms | Tot: 23s61ms  Step: 199ms | Tot: 25s648ms  Step: 201ms | Tot: 25s850ms\n",
      "[=================== 132/132 ================>]  Step: 151ms | Tot: 26s141ms Step: 199ms | Tot: 6s583ms  Step: 199ms | Tot: 6s982ms 41/132   Step: 200ms | Tot: 9s576ms  Step: 200ms | Tot: 11s185ms\n",
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s219ms Step: 200ms | Tot: 3s394ms  Step: 200ms | Tot: 4s393ms 52/132   Step: 200ms | Tot: 10s614ms  Step: 200ms | Tot: 10s814ms  Step: 200ms | Tot: 12s219ms  Step: 200ms | Tot: 14s624ms  Step: 200ms | Tot: 15s829ms  Step: 200ms | Tot: 18s640ms 99/132   Step: 200ms | Tot: 22s651ms  Step: 200ms | Tot: 23s456ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "for i,pth in enumerate([os.path.join('resnet34_save_model',i) for i in os.listdir(path='resnet34_save_model') if '.pth' in i]):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg']\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.027504e-03</td>\n",
       "      <td>9.244031e-02</td>\n",
       "      <td>1.784263e-02</td>\n",
       "      <td>5.578169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.931345e-03</td>\n",
       "      <td>3.659658e-01</td>\n",
       "      <td>2.564293e-01</td>\n",
       "      <td>1.571568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.239599e-10</td>\n",
       "      <td>1.970750e-21</td>\n",
       "      <td>3.803356e-08</td>\n",
       "      <td>2.089403e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.993261e-01</td>\n",
       "      <td>9.456407e-01</td>\n",
       "      <td>9.853242e-01</td>\n",
       "      <td>9.989353e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448082e-03</td>\n",
       "      <td>6.435396e-02</td>\n",
       "      <td>3.096765e-02</td>\n",
       "      <td>2.362306e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_iceberg0   is_iceberg1   is_iceberg2   is_iceberg3\n",
       "0  7.027504e-03  9.244031e-02  1.784263e-02  5.578169e-03\n",
       "1  3.931345e-03  3.659658e-01  2.564293e-01  1.571568e-02\n",
       "2  5.239599e-10  1.970750e-21  3.803356e-08  2.089403e-21\n",
       "3  9.993261e-01  9.456407e-01  9.853242e-01  9.989353e-01\n",
       "4  1.448082e-03  6.435396e-02  3.096765e-02  2.362306e-04"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = temp11.mean(1)\n",
    "temp11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, :4].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, :4].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, :4].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg3']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! cp vgg_fcn.ipynb vgg_angle.ipynb\n",
    "temp11.to_csv('others/vgg19_10fold2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
