{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau,StepLR\n",
    "#torch.multiprocessing.set_start_method(\"spawn\")\n",
    "import vgg_fcn\n",
    "import vgg\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import progress_bar\n",
    "from skimage import transform as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_dir = 'data/processed/'\n",
    "\n",
    "train = pd.read_json(BASE_dir + 'train.json')\n",
    "#test = pd.read_json(BASE_dir + 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data\n",
    "use_cuda= True if torch.cuda.is_available() else False\n",
    "#use_cuda =False\n",
    "#dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor \n",
    "dtype = torch.FloatTensor \n",
    "data=  pd.read_json(BASE_dir + 'train.json')\n",
    "\n",
    "class iceberg_dataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label=  self.data[idx], self.label[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "                rotate = np.random.randint(4, size=1)\n",
    "                if rotate:\n",
    "                    img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                \n",
    "                scale1 = np.exp(np.random.uniform(np.log(1/1.1), np.log(1.1)))\n",
    "                tran = np.random.uniform(-5, 5)\n",
    "                aug = tf.AffineTransform(translation=tran, scale= (scale1, scale1))\n",
    "                img = tf.warp(img, inverse_map=aug)\n",
    "                pass\n",
    "#             temp = []\n",
    "#             for i in img:\n",
    "#                 temp.append(tf.rescale(i,224/75,mode='constant'))\n",
    "#             img = np.stack(temp)\n",
    "            img = torch.from_numpy(img).type(dtype)\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "class iceberg_angle_dataset(Dataset):\n",
    "    def __init__(self, data,angle,label,size=None, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "#         self.angle=torch.cat( (torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1),torch.from_numpy(size).type(torch.FloatTensor).unsqueeze(1)),1)\n",
    "        self.angle=torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1)\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label, angle=  self.data[idx], self.label[idx], self.angle[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            \n",
    "            #rotate, scale, shear, translation\n",
    "#             if self.test is False:\n",
    "#                 angle = np.random.uniform(0,360)\n",
    "#                 img = tf.rotate(img,angle=angle,resize=False)\n",
    "#                 scale1 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.2)))\n",
    "#                 scale2 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.1)))\n",
    "#                 #shear = np.random.uniform(-np.pi/18, np.pi/18)\n",
    "#                 #tran = np.random.uniform(-5, 5)\n",
    "#                 #aug = tf.AffineTransform(shear = shear, translation=tran, scale= (scale1, scale2))\n",
    "#                 aug = tf.AffineTransform(scale= (scale1, scale2))\n",
    "#                 img = tf.warp(img, inverse_map=aug)\n",
    "            \n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "            \n",
    "            if self.test is False:\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "#                 rotate = np.random.randint(4, size=1)\n",
    "#                 if rotate:\n",
    "#                     img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                pass\n",
    "        img = torch.from_numpy(img).type(dtype)\n",
    "#         img = self.transform(img)\n",
    "\n",
    "        return img, angle,label    \n",
    "    \n",
    "    \n",
    "def stack(row):\n",
    "    return np.stack(row[['c1','c2','c3']]).reshape(3,75,75)\n",
    "\n",
    "def raw_to_numpy(data):\n",
    "    img = []\n",
    "    data['c1'] = data['band_1'].apply(np.array)\n",
    "    data['c2'] = data['band_2'].apply(np.array)\n",
    "    data['c3'] = (data['c1'] + data['c2'])/2\n",
    "#     data['c3'] = (data['c1'] + data['c2'])/2\n",
    "    for _, row in data.iterrows():\n",
    "        img.append(stack(row))\n",
    "    return np.stack(img)\n",
    "\n",
    "def transform_compute(img):\n",
    "    train_mean = img.mean(axis=(0,2,3))\n",
    "    train_std = img.std(axis=(0,2,3))\n",
    "    return train_mean, train_std\n",
    "\n",
    "def data_aug(X, y):    \n",
    "    X_rot_30 = []\n",
    "    X_rot_60 = [] \n",
    "    X_h = np.flip(X, 3)\n",
    "    X_v = np.flip(X, 2)\n",
    "    for i in X:\n",
    "        X_rot_30.append(tf.rotate(i,angle=90,resize=False))\n",
    "        X_rot_60.append(tf.rotate(i,angle=270,resize=False))\n",
    "        \n",
    "    X_rot_30 = np.stack(X_rot_30)\n",
    "    X_rot_60 = np.stack(X_rot_60)\n",
    "    ch_y = np.concatenate((y,y,y,y,y))\n",
    "    ch_X = np.concatenate((X, X_h, X_v, X_rot_30, X_rot_60))\n",
    "    return ch_X, ch_y\n",
    "\n",
    "train_X = raw_to_numpy(data)#.transpose(0,2,3,1)\n",
    "train_X.shape     #1604 * 3 *75* 75   N*c*H*W\n",
    "train_y = data['is_iceberg'].values # if iceberg then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are 0\n",
      "We are 50\n",
      "We are 100\n",
      "We are 150\n",
      "We are 200\n",
      "We are 250\n",
      "We are 300\n",
      "We are 350\n",
      "We are 400\n",
      "We are 450\n",
      "We are 500\n",
      "We are 550\n",
      "We are 600\n",
      "We are 650\n",
      "We are 700\n",
      "We are 750\n",
      "We are 800\n",
      "We are 850\n",
      "We are 900\n",
      "We are 950\n",
      "We are 1000\n",
      "We are 1050\n",
      "We are 1100\n",
      "We are 1150\n",
      "We are 1200\n",
      "We are 1250\n",
      "We are 1300\n",
      "We are 1350\n",
      "We are 1400\n",
      "We are 1450\n",
      "We are 1500\n",
      "We are 1550\n",
      "We are 1600\n"
     ]
    }
   ],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "result = []\n",
    "for num,i in enumerate(train_X_del):\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(tf.rescale(j,224/75,mode='constant'))\n",
    "    img = np.stack(temp)\n",
    "    result.append(img)\n",
    "    if num%50==0:\n",
    "        print('We are %d'%num)\n",
    "train_X_del = np.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_index=list(range(1300))\n",
    "# val_index= list(range(1300,1604))\n",
    "# train_index=list(range(304,1604)) \n",
    "# val_index= list(range(304))\n",
    "# # train_X[train_index].shape\n",
    "\n",
    "# # data.inc_angle = data.inc_angle.map(lambda x: 0.0 if x == 'na' else x)\n",
    "# # train_index = np.where(data.inc_angle > 0)[0]\n",
    "# # val_index = np.where(data.inc_angle <= 0)[0]\n",
    "\n",
    "# # seed= np.random.RandomState(123)\n",
    "# # spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# # train_index, val_index = next(spliter.split(train_X))\n",
    "# train_mean, train_std = transform_compute(train_X[train_index])\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X[train_index], label=train_y[train_index], transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "## add augmentation \n",
    "# seed= np.random.RandomState(123)\n",
    "# spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# train_index, val_index = next(spliter.split(train_X))\n",
    "\n",
    "# train_X_af,train_y_af = data_aug(train_X[train_index], train_y[train_index])\n",
    "# train_mean, train_std = transform_compute(train_X_af)\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X_af, label=train_y_af, transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "# train_X_del = train_X[data.inc_angle!='na',:,:,:]\n",
    "# train_y_del = train_y[data.inc_angle!='na']\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "# af_train_X, af_train_y = data_aug(train_X_del[train_index], train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "af_train_X, af_train_y = train_X_del[train_index], train_y_del[train_index]\n",
    "\n",
    "train_dataset = iceberg_dataset(data= af_train_X, label=af_train_y, transform=train_transform)\n",
    "val_dataset = iceberg_dataset(data= train_X_del[val_index], label=train_y_del[val_index], transform=train_transform, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## angle and size\n",
    "\n",
    "#data['inc_angle'][data['inc_angle']=='na']=0\n",
    "data.loc[data['inc_angle']=='na', 'inc_angle'] = 0\n",
    "\n",
    "train_X = train_X\n",
    "train_angle_del = data['inc_angle'].values\n",
    "train_angle = train_angle_del.astype(np.float)\n",
    "#train_size = train['size'].values\n",
    "train_y = train_y\n",
    "\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "#af_train_X,af_train_angle, af_train_y = data_aug(train_X_del[train_index], train_angle_del[train_index],train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "\n",
    "\n",
    "train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                    label=train_y[train_index],\n",
    "                                    transform=train_transform, test=False)\n",
    "\n",
    "val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                    label=train_y[val_index],\n",
    "                                    transform=train_transform, test=True)\n",
    "\n",
    "# train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],size=train_size[train_index],\n",
    "#                                     label=train_y[train_index],\n",
    "#                                     transform=train_transform)\n",
    "\n",
    "# val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],size= train_size[val_index],\n",
    "#                                     label=train_y[val_index],\n",
    "#                                     transform=train_transform, test=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.from_numpy(train_X).type(torch.FloatTensor)[1].shape\n",
    "train_X[1]\n",
    "use_cuda\n",
    "# for i in train_loader:\n",
    "#     print(i.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "#         scheduler.step()\n",
    "        scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        net.train()\n",
    "        for j,(batch_x, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            y = Variable(batch_y)\n",
    "            out = net(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                acc= best_acc+ 0.01\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total > val_loss:\n",
    "                e_s = 0\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x, val_y) in enumerate(val_load):\n",
    "        #len(val_x.size())==1\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        \n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        if len(out.size())==1: #in case it's one dimensional\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####train with angle and other\n",
    "\n",
    "\n",
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = pre_vgg.vgg16_bn(pretrained=True)\n",
    "for param in vgg16.parameters():\n",
    "    print(param.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for  i in net.features:\n",
    "#     print(i)\n",
    "#     break\n",
    "# for i in i.parameters():\n",
    "#     print(i)\n",
    "len(net.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.487 | Acc: 75.604% (970/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 1.643 | Acc: 50.467% (162/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.314 | Acc: 87.062% (1117/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.774 | Acc: 69.159% (222/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.283 | Acc: 87.373% (1121/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.388 | Acc: 80.685% (259/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s1ms|Loss: 0.276 | Acc: 88.854% (1140/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.378 | Acc: 80.062% (257/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.220 | Acc: 91.037% (1168/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.233 | Acc: 91.277% (293/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.165 | Acc: 93.920% (1205/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.188 | Acc: 93.453% (1199/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.230 | Acc: 89.720% (288/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.148 | Acc: 94.778% (1216/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.223 | Acc: 90.654% (291/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.142 | Acc: 95.168% (1221/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.222 | Acc: 90.966% (292/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.132 | Acc: 96.025% (1232/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.900% (295/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.133 | Acc: 96.259% (1235/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s1ms|Loss: 0.131 | Acc: 96.103% (1233/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.223 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.227 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.115 | Acc: 96.804% (1242/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.126 | Acc: 96.493% (1238/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 92.212% (296/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.117 | Acc: 96.493% (1238/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.221 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 95.869% (1230/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 96.415% (1237/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.221 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.493% (1238/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.118 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.113 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.224 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 96.103% (1233/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.118 | Acc: 96.882% (1243/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.103 | Acc: 97.038% (1245/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.102 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.109 | Acc: 97.506% (1251/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.226 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.112 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.108 | Acc: 97.116% (1246/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:31\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.104 | Acc: 97.272% (1248/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.116% (1246/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.102 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.123 | Acc: 96.337% (1236/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.104 | Acc: 97.350% (1249/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 92.212% (296/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.119 | Acc: 96.571% (1239/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.226 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.108 | Acc: 96.960% (1244/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.101 | Acc: 97.194% (1247/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.107 | Acc: 96.726% (1241/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.272% (1248/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.648% (1240/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.804% (1242/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.107 | Acc: 97.038% (1245/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.100 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.222 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.230 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.415% (1237/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.101 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.224 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.098 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.337% (1236/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.105 | Acc: 96.882% (1243/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  75/ 81 ====>..]Step: 0ms| Tot: 3s7ms|Loss: 0.102 | Acc: 97.417% (1169/1200)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1417:\n",
      "KeyboardInterrupt\n",
      "Process Process-1419:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f6aa1aed4b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e3449cb2b121>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Process Process-1418:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "result=[]\n",
    "for i in range(1):\n",
    "    vgg19_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "    num = 256\n",
    "    vgg19_bn.classifier = nn.Sequential(\n",
    "                nn.Linear(512+1, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(num, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(num, 2)\n",
    "            )\n",
    "\n",
    "    net= vgg19_bn\n",
    "    # net.load_state_dict(torch.load('vgg_fcn_loss.pth'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # #Adam does not perform so good here   \n",
    "    # #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "    # optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "    scheduler = MultiStepLR(optimizer, [5,11,30], gamma=0.1)\n",
    "#     scheduler = MultiStepLR(optimizer, [10,18,26], gamma=0.1)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'max', patience =3,min_lr= 0.00001)\n",
    "    #5e-3 86\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "        net.cuda()\n",
    "    #     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "    #     cudnn.benchmark = True   \n",
    "\n",
    "    a = train(epoch=80,early_stopping =20)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21936874226246297, 91.27725856697819, 39),\n",
       " (0.21782404165773006, 91.58878504672897, 55),\n",
       " (0.22525541061924254, 90.96573208722741, 53)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result 5* 2* 10*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24811193430535147, 90.03115264797508, 29),\n",
       " (0.21092650229314405, 91.58878504672897, 28),\n",
       " (0.22480700989007207, 90.65420560747664, 53)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.218 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 93.146% (299/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.082 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.225 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.086 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:11\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.091 | Acc: 96.648% (1240/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.084 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:16\n",
      "lr change from 0.000001 to 0.000000\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.038% (1245/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.090 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:21\n",
      "[========>  29/ 81 .......]Step: 0ms| Tot: 1s|Loss: 0.075 | Acc: 96.552% (448/464))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8923:\n",
      "Process Process-8924:\n",
      "Process Process-8925:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-77835431faca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e8875581f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "net.load_state_dict(torch.load('cnn_ang_loss.pth'))\n",
    "for i in vgg16_bn.features:\n",
    "    i.requires_grad = False\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #Adam does not perform so good here   \n",
    "# #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "# optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "scheduler = MultiStepLR(optimizer, [5,10,15], gamma=0.1)\n",
    "# scheduler = MultiStepLR(optimizer, [8,18], gamma=0.1)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'max', patience =10,min_lr= 0.0001)\n",
    "#5e-3 86\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "    net.cuda()\n",
    "#     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "#     cudnn.benchmark = True   \n",
    "\n",
    "train(epoch=250,early_stopping =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 3, 75, 75)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test_set)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k =np.stack(result).mean(axis=0)\n",
    "# #sub.shape\n",
    "# result[1].shape\n",
    "# np.concatenate(prob).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test_set['id']\n",
    "sub['is_iceberg'] =  np.concatenate(prob)\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>is_iceberg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.886197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg  is_iceberg2\n",
       "is_iceberg     1.000000     0.886197\n",
       "is_iceberg2    0.886197     1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp= pd.read_csv('submission3.csv') #0.0001 wd one\n",
    "sub['is_iceberg2'] = temp['is_iceberg']\n",
    "sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(4):\n",
    "    ran_num = np.random.randint(50000,60000,size=1)[0]\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        \n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(2):\n",
    "            vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [10,20,30], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            with open(\"vgg19_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase1,with aug, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg19_loss.pth vgg19_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "        \n",
    "        for g in range(2):\n",
    "            cmd = 'cp vgg19_loss{}.pth vgg19_models/r1_10vgg_aug{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass 0:0\n",
      "pass 0:1\n",
      "pass 0:2\n",
      "pass 0:3\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.512 | Acc: 73.615% (1063/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.347 | Acc: 58.125% (93/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.366 | Acc: 85.457% (1234/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.417 | Acc: 81.875% (131/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.293 | Acc: 88.227% (1274/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.340 | Acc: 86.250% (138/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.241 | Acc: 90.305% (1304/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.364 | Acc: 85.625% (137/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.265 | Acc: 88.989% (1285/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.003 | Acc: 58.125% (93/160)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.208 | Acc: 91.690% (1324/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.292 | Acc: 85.625% (137/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.183 | Acc: 93.421% (1349/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.288 | Acc: 86.875% (139/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.191 | Acc: 92.382% (1334/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 86.250% (138/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.184 | Acc: 93.352% (1348/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 88.125% (141/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.156 | Acc: 94.945% (1371/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.125% (141/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.157 | Acc: 94.044% (1358/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 89.375% (143/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.137 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.257 | Acc: 88.750% (142/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.149 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.135 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.137 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.141 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.129 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.139 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 90.000% (144/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.137 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.133 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:21\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.147 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.275 | Acc: 87.500% (140/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.131 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.142 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 87.500% (140/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.134 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.127 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.264 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.147 | Acc: 95.152% (1374/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.265 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.142 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.130 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.264 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.126 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.144 | Acc: 95.360% (1377/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.129 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.122 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.266 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.126 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.137 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.131 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.120 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.147 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s4ms|Loss: 0.144 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.123 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.139 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.136 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.135 | Acc: 95.152% (1374/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.132 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.265 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.130 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.137 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.138 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.128 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.153 | Acc: 94.945% (1371/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.132 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.493 | Acc: 76.247% (1101/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.385 | Acc: 80.000% (128/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.317 | Acc: 86.011% (1242/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.377 | Acc: 84.375% (135/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.277 | Acc: 88.989% (1285/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.298 | Acc: 86.250% (138/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.235 | Acc: 90.374% (1305/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.178 | Acc: 60.000% (96/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.221 | Acc: 91.967% (1328/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.306 | Acc: 88.125% (141/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.176 | Acc: 94.044% (1358/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.144 | Acc: 94.668% (1367/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.265 | Acc: 91.250% (146/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s4ms|Loss: 0.144 | Acc: 94.737% (1368/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.133 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.283 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.129 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.120 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.316 | Acc: 87.500% (140/160)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.115 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.110 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.298 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.111 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.284 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.104 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.292 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.095 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.115 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.101 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.293 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.110 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.096 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.303 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:21\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.114 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.300 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.094 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.296 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.098 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.289 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.091 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.297 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.093 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.294 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.097 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.095 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.291 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.509 | Acc: 75.000% (1083/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.454 | Acc: 77.500% (124/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.314 | Acc: 87.673% (1266/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.329 | Acc: 83.750% (134/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.255 | Acc: 89.266% (1289/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.750% (142/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.244 | Acc: 90.097% (1301/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.319 | Acc: 86.875% (139/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.207 | Acc: 91.898% (1327/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.342 | Acc: 83.750% (134/160)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.162 | Acc: 93.767% (1354/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.156 | Acc: 93.767% (1354/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.238 | Acc: 92.500% (148/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.135 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.247 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.126 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.130 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.116 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.251 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.110 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.095 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.115 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.257 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.111 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.256 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.111 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.252 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.112 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.105 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.109 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.108 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:21\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.119 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.105 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.107 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.257 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.111 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.110 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s5ms|Loss: 0.101 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 4s6ms|Loss: 0.114 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 91.250% (146/160)\n",
      "pass 0:5\n",
      "pass 0:6\n",
      "pass 0:7\n",
      "pass 0:8\n",
      "pass 0:9\n"
     ]
    }
   ],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    if rou ==0:\n",
    "        ran_num = 59790\n",
    "    else:\n",
    "        ran_num = np.random.randint(50000,60000, size=1)[0]\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        if rou ==0 and k not in [4]:\n",
    "            print('pass %d:%d' %(rou,k))\n",
    "            continue\n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(3):\n",
    "            vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [5,11,20], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            candidate.append(result[0])\n",
    "            with open(\"vgg19_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase3, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg19_loss.pth vgg19_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "            if rep ==1 and np.sum(np.array(candidate)>0.20)>=1:\n",
    "                continue\n",
    "            elif rep==1:\n",
    "                break\n",
    "            \n",
    "        for g in range(rep+1):\n",
    "            cmd = 'cp vgg19_loss{}.pth vgg19_models/r3_10vgg{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vgg_models/r1_5vgg0_00.pth', 'vgg_models/r1_5vgg0_11.pth', 'vgg_models/r1_5vgg0_20.pth', 'vgg_models/r1_5vgg0_31.pth', 'vgg_models/r1_5vgg0_40.pth']\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6mss\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "waiting_list=  \n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "vgg16_bn = vgg_fcn.vgg16_bn(pretrained=True)#copy.deepcopy(vgg16)\n",
    "# vgg16_bn.avg= nn.Conv2d(512, 512, kernel_size=2,\n",
    "#                                bias=False)\n",
    "\n",
    "# vgg16_bn.classifier = nn.Sequential(\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.6),\n",
    "#             nn.Linear(512, 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Conv2d(512,512, kernel_size= 3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Conv2d(512, 2, kernel_size=3, padding=1,\n",
    "                               bias=False),\n",
    "            nn.AvgPool2d(3)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "\n",
    "print(waiting_list)\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s1ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n",
      "[=========  26/ 26 ======>]Step: 0ms| Tot: 1s2ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'train.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "# test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle=test.inc_angle.values.astype(np.float), transform=train_transform,test=True)\n",
    "\n",
    "data = test\n",
    "data.loc[data.inc_angle=='na', 'inc_angle']=0\n",
    "train_angle_del = data['inc_angle'].values\n",
    "train_angle_del = train_angle_del.astype(np.float)\n",
    "test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle= train_angle_del, transform=train_transform,test=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "num = 256\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(num, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "waiting_list = ['r1_10vgg0_00.pth',\n",
    "    'r1_10vgg0_10.pth',\n",
    "    'r1_10vgg0_21.pth',\n",
    "    'r1_10vgg0_31.pth',\n",
    "                'r1_10vgg0_41.pth',\n",
    "                'r1_10vgg0_51.pth',\n",
    "                'r1_10vgg0_60.pth',\n",
    "                'r1_10vgg0_70.pth',\n",
    "                'r1_10vgg0_81.pth',\n",
    "                'r1_10vgg0_90.pth']\n",
    "\n",
    "# waiting_list = ['r2_10vgg0_00.pth',\n",
    "#     'r2_10vgg0_10.pth',\n",
    "#     'r2_10vgg0_21.pth',\n",
    "#     'r2_10vgg0_30.pth',\n",
    "#                 'r2_10vgg0_41.pth',\n",
    "#                 'r2_10vgg0_50.pth',\n",
    "#                 'r2_10vgg0_61.pth',\n",
    "#                 'r2_10vgg0_71.pth',\n",
    "#                 'r2_10vgg0_81.pth',\n",
    "#                 'r2_10vgg0_90.pth']\n",
    "\n",
    "waiting_list += ['r3_10vgg0_00.pth','r3_10vgg0_50.pth',\n",
    "                'r3_10vgg0_11.pth','r3_10vgg0_60.pth',\n",
    "                'r3_10vgg0_20.pth','r3_10vgg0_71.pth',\n",
    "                'r3_10vgg0_31.pth','r3_10vgg0_80.pth',\n",
    "                'r3_10vgg0_42.pth','r3_10vgg0_91.pth',\n",
    "                'r3_10vgg1_00.pth','r3_10vgg1_51.pth',\n",
    "                'r3_10vgg1_11.pth','r3_10vgg1_60.pth',\n",
    "                'r3_10vgg1_20.pth','r3_10vgg1_70.pth',\n",
    "                'r3_10vgg1_31.pth','r3_10vgg1_81.pth',\n",
    "                'r3_10vgg1_40.pth','r3_10vgg1_91.pth',\n",
    "                'r3_10vgg2_01.pth','r3_10vgg2_52.pth',\n",
    "                'r3_10vgg2_11.pth','r3_10vgg2_60.pth',\n",
    "                'r3_10vgg2_21.pth','r3_10vgg2_71.pth',\n",
    "                'r3_10vgg2_30.pth','r3_10vgg2_80.pth',\n",
    "                'r3_10vgg2_40.pth','r3_10vgg2_91.pth',\n",
    "                'r3_10vgg3_01.pth','r3_10vgg3_51.pth',\n",
    "                'r3_10vgg3_12.pth','r3_10vgg3_60.pth',\n",
    "                'r3_10vgg3_20.pth','r3_10vgg3_70.pth',\n",
    "                'r3_10vgg3_30.pth','r3_10vgg3_80.pth',\n",
    "                'r3_10vgg3_40.pth','r3_10vgg3_91.pth'\n",
    "]\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models') if 'r3' in i]\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg19_models', i) for i in waiting_list] \n",
    "net= vgg16_bn\n",
    "\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(test_loader):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg6</th>\n",
       "      <th>is_iceberg7</th>\n",
       "      <th>is_iceberg8</th>\n",
       "      <th>is_iceberg9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_iceberg40</th>\n",
       "      <th>is_iceberg41</th>\n",
       "      <th>is_iceberg42</th>\n",
       "      <th>is_iceberg43</th>\n",
       "      <th>is_iceberg44</th>\n",
       "      <th>is_iceberg45</th>\n",
       "      <th>is_iceberg46</th>\n",
       "      <th>is_iceberg47</th>\n",
       "      <th>is_iceberg48</th>\n",
       "      <th>is_iceberg49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980554</td>\n",
       "      <td>0.981650</td>\n",
       "      <td>0.970135</td>\n",
       "      <td>0.978177</td>\n",
       "      <td>0.976217</td>\n",
       "      <td>0.978550</td>\n",
       "      <td>0.980526</td>\n",
       "      <td>0.980679</td>\n",
       "      <td>0.979944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980512</td>\n",
       "      <td>0.977156</td>\n",
       "      <td>0.977947</td>\n",
       "      <td>0.981513</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.981483</td>\n",
       "      <td>0.983084</td>\n",
       "      <td>0.982341</td>\n",
       "      <td>0.979847</td>\n",
       "      <td>0.976667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.980554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980287</td>\n",
       "      <td>0.972908</td>\n",
       "      <td>0.976180</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.971239</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>0.982309</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981852</td>\n",
       "      <td>0.974734</td>\n",
       "      <td>0.973112</td>\n",
       "      <td>0.978877</td>\n",
       "      <td>0.979431</td>\n",
       "      <td>0.975573</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>0.976373</td>\n",
       "      <td>0.979191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.981650</td>\n",
       "      <td>0.980287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976590</td>\n",
       "      <td>0.978487</td>\n",
       "      <td>0.976332</td>\n",
       "      <td>0.977972</td>\n",
       "      <td>0.977608</td>\n",
       "      <td>0.983201</td>\n",
       "      <td>0.973851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981455</td>\n",
       "      <td>0.978083</td>\n",
       "      <td>0.972824</td>\n",
       "      <td>0.979892</td>\n",
       "      <td>0.979012</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.977123</td>\n",
       "      <td>0.980226</td>\n",
       "      <td>0.981219</td>\n",
       "      <td>0.975678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.970135</td>\n",
       "      <td>0.972908</td>\n",
       "      <td>0.976590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969545</td>\n",
       "      <td>0.962886</td>\n",
       "      <td>0.966389</td>\n",
       "      <td>0.967832</td>\n",
       "      <td>0.974243</td>\n",
       "      <td>0.967090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975202</td>\n",
       "      <td>0.971817</td>\n",
       "      <td>0.965979</td>\n",
       "      <td>0.974335</td>\n",
       "      <td>0.971078</td>\n",
       "      <td>0.970410</td>\n",
       "      <td>0.969232</td>\n",
       "      <td>0.971781</td>\n",
       "      <td>0.976994</td>\n",
       "      <td>0.967691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.978177</td>\n",
       "      <td>0.976180</td>\n",
       "      <td>0.978487</td>\n",
       "      <td>0.969545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970950</td>\n",
       "      <td>0.979420</td>\n",
       "      <td>0.976911</td>\n",
       "      <td>0.978729</td>\n",
       "      <td>0.978526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980442</td>\n",
       "      <td>0.976273</td>\n",
       "      <td>0.976257</td>\n",
       "      <td>0.980883</td>\n",
       "      <td>0.979364</td>\n",
       "      <td>0.976211</td>\n",
       "      <td>0.981681</td>\n",
       "      <td>0.984610</td>\n",
       "      <td>0.979794</td>\n",
       "      <td>0.970683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.976217</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.976332</td>\n",
       "      <td>0.962886</td>\n",
       "      <td>0.970950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973648</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.974152</td>\n",
       "      <td>0.970282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970019</td>\n",
       "      <td>0.969217</td>\n",
       "      <td>0.972377</td>\n",
       "      <td>0.976065</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>0.975768</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.972738</td>\n",
       "      <td>0.973628</td>\n",
       "      <td>0.979958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg6</th>\n",
       "      <td>0.978550</td>\n",
       "      <td>0.971239</td>\n",
       "      <td>0.977972</td>\n",
       "      <td>0.966389</td>\n",
       "      <td>0.979420</td>\n",
       "      <td>0.973648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976894</td>\n",
       "      <td>0.974675</td>\n",
       "      <td>0.978224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975146</td>\n",
       "      <td>0.979981</td>\n",
       "      <td>0.975245</td>\n",
       "      <td>0.978846</td>\n",
       "      <td>0.977729</td>\n",
       "      <td>0.978267</td>\n",
       "      <td>0.977926</td>\n",
       "      <td>0.983776</td>\n",
       "      <td>0.981586</td>\n",
       "      <td>0.971734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg7</th>\n",
       "      <td>0.980526</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>0.977608</td>\n",
       "      <td>0.967832</td>\n",
       "      <td>0.976911</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.976894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977910</td>\n",
       "      <td>0.978892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976238</td>\n",
       "      <td>0.976621</td>\n",
       "      <td>0.973131</td>\n",
       "      <td>0.978654</td>\n",
       "      <td>0.977455</td>\n",
       "      <td>0.982510</td>\n",
       "      <td>0.977932</td>\n",
       "      <td>0.982038</td>\n",
       "      <td>0.978247</td>\n",
       "      <td>0.975423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg8</th>\n",
       "      <td>0.980679</td>\n",
       "      <td>0.982309</td>\n",
       "      <td>0.983201</td>\n",
       "      <td>0.974243</td>\n",
       "      <td>0.978729</td>\n",
       "      <td>0.974152</td>\n",
       "      <td>0.974675</td>\n",
       "      <td>0.977910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984046</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.974839</td>\n",
       "      <td>0.980695</td>\n",
       "      <td>0.978954</td>\n",
       "      <td>0.979101</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.982733</td>\n",
       "      <td>0.983106</td>\n",
       "      <td>0.976887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg9</th>\n",
       "      <td>0.979944</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.973851</td>\n",
       "      <td>0.967090</td>\n",
       "      <td>0.978526</td>\n",
       "      <td>0.970282</td>\n",
       "      <td>0.978224</td>\n",
       "      <td>0.978892</td>\n",
       "      <td>0.976175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977241</td>\n",
       "      <td>0.975077</td>\n",
       "      <td>0.976420</td>\n",
       "      <td>0.979723</td>\n",
       "      <td>0.982016</td>\n",
       "      <td>0.979522</td>\n",
       "      <td>0.978882</td>\n",
       "      <td>0.983858</td>\n",
       "      <td>0.981084</td>\n",
       "      <td>0.969335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg10</th>\n",
       "      <td>0.978986</td>\n",
       "      <td>0.984075</td>\n",
       "      <td>0.980254</td>\n",
       "      <td>0.972012</td>\n",
       "      <td>0.976464</td>\n",
       "      <td>0.973951</td>\n",
       "      <td>0.974297</td>\n",
       "      <td>0.974471</td>\n",
       "      <td>0.984482</td>\n",
       "      <td>0.974546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979781</td>\n",
       "      <td>0.976626</td>\n",
       "      <td>0.974983</td>\n",
       "      <td>0.980009</td>\n",
       "      <td>0.980750</td>\n",
       "      <td>0.975109</td>\n",
       "      <td>0.974379</td>\n",
       "      <td>0.978494</td>\n",
       "      <td>0.979830</td>\n",
       "      <td>0.980154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg11</th>\n",
       "      <td>0.979575</td>\n",
       "      <td>0.975238</td>\n",
       "      <td>0.979147</td>\n",
       "      <td>0.969264</td>\n",
       "      <td>0.981215</td>\n",
       "      <td>0.971910</td>\n",
       "      <td>0.979551</td>\n",
       "      <td>0.981440</td>\n",
       "      <td>0.979617</td>\n",
       "      <td>0.977286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977945</td>\n",
       "      <td>0.980906</td>\n",
       "      <td>0.972673</td>\n",
       "      <td>0.981311</td>\n",
       "      <td>0.979035</td>\n",
       "      <td>0.978884</td>\n",
       "      <td>0.980034</td>\n",
       "      <td>0.985089</td>\n",
       "      <td>0.978149</td>\n",
       "      <td>0.971345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg12</th>\n",
       "      <td>0.981202</td>\n",
       "      <td>0.972606</td>\n",
       "      <td>0.977144</td>\n",
       "      <td>0.961507</td>\n",
       "      <td>0.978763</td>\n",
       "      <td>0.973707</td>\n",
       "      <td>0.977479</td>\n",
       "      <td>0.975604</td>\n",
       "      <td>0.973427</td>\n",
       "      <td>0.976692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976770</td>\n",
       "      <td>0.968605</td>\n",
       "      <td>0.979018</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>0.978515</td>\n",
       "      <td>0.979780</td>\n",
       "      <td>0.982978</td>\n",
       "      <td>0.979854</td>\n",
       "      <td>0.980345</td>\n",
       "      <td>0.971089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg13</th>\n",
       "      <td>0.980246</td>\n",
       "      <td>0.978761</td>\n",
       "      <td>0.977328</td>\n",
       "      <td>0.972912</td>\n",
       "      <td>0.974763</td>\n",
       "      <td>0.970470</td>\n",
       "      <td>0.976463</td>\n",
       "      <td>0.978040</td>\n",
       "      <td>0.979838</td>\n",
       "      <td>0.980304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977470</td>\n",
       "      <td>0.974608</td>\n",
       "      <td>0.974753</td>\n",
       "      <td>0.976007</td>\n",
       "      <td>0.979122</td>\n",
       "      <td>0.979669</td>\n",
       "      <td>0.978801</td>\n",
       "      <td>0.979062</td>\n",
       "      <td>0.980619</td>\n",
       "      <td>0.974206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg14</th>\n",
       "      <td>0.978273</td>\n",
       "      <td>0.976951</td>\n",
       "      <td>0.980468</td>\n",
       "      <td>0.974728</td>\n",
       "      <td>0.980902</td>\n",
       "      <td>0.970702</td>\n",
       "      <td>0.977764</td>\n",
       "      <td>0.978983</td>\n",
       "      <td>0.979580</td>\n",
       "      <td>0.978848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.978756</td>\n",
       "      <td>0.974007</td>\n",
       "      <td>0.979532</td>\n",
       "      <td>0.983198</td>\n",
       "      <td>0.978079</td>\n",
       "      <td>0.977663</td>\n",
       "      <td>0.984110</td>\n",
       "      <td>0.982507</td>\n",
       "      <td>0.970620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg15</th>\n",
       "      <td>0.985174</td>\n",
       "      <td>0.981018</td>\n",
       "      <td>0.982273</td>\n",
       "      <td>0.971233</td>\n",
       "      <td>0.983265</td>\n",
       "      <td>0.974631</td>\n",
       "      <td>0.982356</td>\n",
       "      <td>0.983191</td>\n",
       "      <td>0.984228</td>\n",
       "      <td>0.980477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.980913</td>\n",
       "      <td>0.979607</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.981217</td>\n",
       "      <td>0.984682</td>\n",
       "      <td>0.981548</td>\n",
       "      <td>0.987385</td>\n",
       "      <td>0.982556</td>\n",
       "      <td>0.976271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg16</th>\n",
       "      <td>0.979138</td>\n",
       "      <td>0.971491</td>\n",
       "      <td>0.975766</td>\n",
       "      <td>0.970864</td>\n",
       "      <td>0.980416</td>\n",
       "      <td>0.969756</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>0.977642</td>\n",
       "      <td>0.979585</td>\n",
       "      <td>0.980629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978988</td>\n",
       "      <td>0.978734</td>\n",
       "      <td>0.977197</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>0.979735</td>\n",
       "      <td>0.980635</td>\n",
       "      <td>0.982613</td>\n",
       "      <td>0.983361</td>\n",
       "      <td>0.981088</td>\n",
       "      <td>0.967152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg17</th>\n",
       "      <td>0.980151</td>\n",
       "      <td>0.974752</td>\n",
       "      <td>0.979734</td>\n",
       "      <td>0.970800</td>\n",
       "      <td>0.979910</td>\n",
       "      <td>0.974921</td>\n",
       "      <td>0.982106</td>\n",
       "      <td>0.979602</td>\n",
       "      <td>0.978986</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979980</td>\n",
       "      <td>0.980738</td>\n",
       "      <td>0.978686</td>\n",
       "      <td>0.981618</td>\n",
       "      <td>0.977398</td>\n",
       "      <td>0.977357</td>\n",
       "      <td>0.979474</td>\n",
       "      <td>0.983191</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg18</th>\n",
       "      <td>0.979617</td>\n",
       "      <td>0.981959</td>\n",
       "      <td>0.979238</td>\n",
       "      <td>0.972482</td>\n",
       "      <td>0.979961</td>\n",
       "      <td>0.976690</td>\n",
       "      <td>0.976394</td>\n",
       "      <td>0.976194</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980131</td>\n",
       "      <td>0.974745</td>\n",
       "      <td>0.976469</td>\n",
       "      <td>0.984767</td>\n",
       "      <td>0.977213</td>\n",
       "      <td>0.975581</td>\n",
       "      <td>0.974887</td>\n",
       "      <td>0.979773</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.983515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg19</th>\n",
       "      <td>0.976182</td>\n",
       "      <td>0.980686</td>\n",
       "      <td>0.980751</td>\n",
       "      <td>0.974096</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.973926</td>\n",
       "      <td>0.972146</td>\n",
       "      <td>0.977715</td>\n",
       "      <td>0.982241</td>\n",
       "      <td>0.971330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977314</td>\n",
       "      <td>0.977817</td>\n",
       "      <td>0.970053</td>\n",
       "      <td>0.978719</td>\n",
       "      <td>0.975640</td>\n",
       "      <td>0.975223</td>\n",
       "      <td>0.975143</td>\n",
       "      <td>0.975156</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>0.975776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg20</th>\n",
       "      <td>0.980535</td>\n",
       "      <td>0.979512</td>\n",
       "      <td>0.983286</td>\n",
       "      <td>0.975853</td>\n",
       "      <td>0.977627</td>\n",
       "      <td>0.966512</td>\n",
       "      <td>0.974041</td>\n",
       "      <td>0.975017</td>\n",
       "      <td>0.980611</td>\n",
       "      <td>0.977106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980603</td>\n",
       "      <td>0.976269</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.976274</td>\n",
       "      <td>0.978065</td>\n",
       "      <td>0.977063</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>0.979648</td>\n",
       "      <td>0.980803</td>\n",
       "      <td>0.969379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg21</th>\n",
       "      <td>0.980398</td>\n",
       "      <td>0.976910</td>\n",
       "      <td>0.981272</td>\n",
       "      <td>0.971025</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>0.971998</td>\n",
       "      <td>0.974597</td>\n",
       "      <td>0.976223</td>\n",
       "      <td>0.975602</td>\n",
       "      <td>0.972418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976673</td>\n",
       "      <td>0.972058</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>0.975692</td>\n",
       "      <td>0.974533</td>\n",
       "      <td>0.976880</td>\n",
       "      <td>0.976745</td>\n",
       "      <td>0.976353</td>\n",
       "      <td>0.977553</td>\n",
       "      <td>0.974348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg22</th>\n",
       "      <td>0.977810</td>\n",
       "      <td>0.977470</td>\n",
       "      <td>0.981252</td>\n",
       "      <td>0.967719</td>\n",
       "      <td>0.975638</td>\n",
       "      <td>0.979125</td>\n",
       "      <td>0.977523</td>\n",
       "      <td>0.978012</td>\n",
       "      <td>0.978343</td>\n",
       "      <td>0.975156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975210</td>\n",
       "      <td>0.976087</td>\n",
       "      <td>0.975903</td>\n",
       "      <td>0.981497</td>\n",
       "      <td>0.977796</td>\n",
       "      <td>0.977386</td>\n",
       "      <td>0.975383</td>\n",
       "      <td>0.978850</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg23</th>\n",
       "      <td>0.979774</td>\n",
       "      <td>0.971745</td>\n",
       "      <td>0.977382</td>\n",
       "      <td>0.971195</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.968722</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.982571</td>\n",
       "      <td>0.980019</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981981</td>\n",
       "      <td>0.979787</td>\n",
       "      <td>0.975292</td>\n",
       "      <td>0.976935</td>\n",
       "      <td>0.981279</td>\n",
       "      <td>0.979929</td>\n",
       "      <td>0.981815</td>\n",
       "      <td>0.985030</td>\n",
       "      <td>0.984432</td>\n",
       "      <td>0.967857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg24</th>\n",
       "      <td>0.975199</td>\n",
       "      <td>0.979567</td>\n",
       "      <td>0.977285</td>\n",
       "      <td>0.975938</td>\n",
       "      <td>0.975102</td>\n",
       "      <td>0.971606</td>\n",
       "      <td>0.971162</td>\n",
       "      <td>0.972511</td>\n",
       "      <td>0.980439</td>\n",
       "      <td>0.973978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980086</td>\n",
       "      <td>0.975712</td>\n",
       "      <td>0.969112</td>\n",
       "      <td>0.981199</td>\n",
       "      <td>0.973824</td>\n",
       "      <td>0.973179</td>\n",
       "      <td>0.971002</td>\n",
       "      <td>0.977272</td>\n",
       "      <td>0.978442</td>\n",
       "      <td>0.981068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg25</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.974524</td>\n",
       "      <td>0.978592</td>\n",
       "      <td>0.971959</td>\n",
       "      <td>0.980907</td>\n",
       "      <td>0.972770</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>0.981130</td>\n",
       "      <td>0.979064</td>\n",
       "      <td>0.982183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981520</td>\n",
       "      <td>0.976244</td>\n",
       "      <td>0.978641</td>\n",
       "      <td>0.982150</td>\n",
       "      <td>0.982006</td>\n",
       "      <td>0.980915</td>\n",
       "      <td>0.984354</td>\n",
       "      <td>0.984553</td>\n",
       "      <td>0.985436</td>\n",
       "      <td>0.969882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg26</th>\n",
       "      <td>0.978488</td>\n",
       "      <td>0.973647</td>\n",
       "      <td>0.975995</td>\n",
       "      <td>0.966335</td>\n",
       "      <td>0.975487</td>\n",
       "      <td>0.968666</td>\n",
       "      <td>0.977082</td>\n",
       "      <td>0.977958</td>\n",
       "      <td>0.976694</td>\n",
       "      <td>0.975397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972479</td>\n",
       "      <td>0.972733</td>\n",
       "      <td>0.971378</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>0.975360</td>\n",
       "      <td>0.980145</td>\n",
       "      <td>0.978108</td>\n",
       "      <td>0.978540</td>\n",
       "      <td>0.978295</td>\n",
       "      <td>0.968063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg27</th>\n",
       "      <td>0.975484</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.979975</td>\n",
       "      <td>0.976297</td>\n",
       "      <td>0.975666</td>\n",
       "      <td>0.969134</td>\n",
       "      <td>0.975988</td>\n",
       "      <td>0.974176</td>\n",
       "      <td>0.979257</td>\n",
       "      <td>0.972638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975928</td>\n",
       "      <td>0.978428</td>\n",
       "      <td>0.968327</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>0.971323</td>\n",
       "      <td>0.974422</td>\n",
       "      <td>0.972015</td>\n",
       "      <td>0.975333</td>\n",
       "      <td>0.978451</td>\n",
       "      <td>0.969353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg28</th>\n",
       "      <td>0.977063</td>\n",
       "      <td>0.966890</td>\n",
       "      <td>0.975265</td>\n",
       "      <td>0.965835</td>\n",
       "      <td>0.977735</td>\n",
       "      <td>0.970907</td>\n",
       "      <td>0.980122</td>\n",
       "      <td>0.974990</td>\n",
       "      <td>0.972780</td>\n",
       "      <td>0.975398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974970</td>\n",
       "      <td>0.973461</td>\n",
       "      <td>0.978636</td>\n",
       "      <td>0.975532</td>\n",
       "      <td>0.976573</td>\n",
       "      <td>0.977293</td>\n",
       "      <td>0.979804</td>\n",
       "      <td>0.982276</td>\n",
       "      <td>0.980682</td>\n",
       "      <td>0.969289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg29</th>\n",
       "      <td>0.978981</td>\n",
       "      <td>0.968113</td>\n",
       "      <td>0.973388</td>\n",
       "      <td>0.968518</td>\n",
       "      <td>0.979027</td>\n",
       "      <td>0.969310</td>\n",
       "      <td>0.978030</td>\n",
       "      <td>0.977616</td>\n",
       "      <td>0.974384</td>\n",
       "      <td>0.981294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977308</td>\n",
       "      <td>0.972648</td>\n",
       "      <td>0.977508</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.980017</td>\n",
       "      <td>0.983753</td>\n",
       "      <td>0.983728</td>\n",
       "      <td>0.979775</td>\n",
       "      <td>0.965018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg30</th>\n",
       "      <td>0.980160</td>\n",
       "      <td>0.981101</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.977610</td>\n",
       "      <td>0.976785</td>\n",
       "      <td>0.971012</td>\n",
       "      <td>0.976410</td>\n",
       "      <td>0.979940</td>\n",
       "      <td>0.981882</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980744</td>\n",
       "      <td>0.975761</td>\n",
       "      <td>0.974394</td>\n",
       "      <td>0.976357</td>\n",
       "      <td>0.981496</td>\n",
       "      <td>0.978985</td>\n",
       "      <td>0.979276</td>\n",
       "      <td>0.979603</td>\n",
       "      <td>0.983375</td>\n",
       "      <td>0.972428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg31</th>\n",
       "      <td>0.978830</td>\n",
       "      <td>0.978720</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972315</td>\n",
       "      <td>0.977831</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.974851</td>\n",
       "      <td>0.984478</td>\n",
       "      <td>0.975228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981450</td>\n",
       "      <td>0.984038</td>\n",
       "      <td>0.970956</td>\n",
       "      <td>0.981514</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.972613</td>\n",
       "      <td>0.974541</td>\n",
       "      <td>0.985062</td>\n",
       "      <td>0.978859</td>\n",
       "      <td>0.970016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg32</th>\n",
       "      <td>0.980099</td>\n",
       "      <td>0.973159</td>\n",
       "      <td>0.976691</td>\n",
       "      <td>0.969323</td>\n",
       "      <td>0.975915</td>\n",
       "      <td>0.966437</td>\n",
       "      <td>0.976178</td>\n",
       "      <td>0.979744</td>\n",
       "      <td>0.980953</td>\n",
       "      <td>0.981192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.976968</td>\n",
       "      <td>0.973230</td>\n",
       "      <td>0.974030</td>\n",
       "      <td>0.978268</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.980055</td>\n",
       "      <td>0.981326</td>\n",
       "      <td>0.980945</td>\n",
       "      <td>0.964193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg33</th>\n",
       "      <td>0.983825</td>\n",
       "      <td>0.975642</td>\n",
       "      <td>0.980142</td>\n",
       "      <td>0.971703</td>\n",
       "      <td>0.979720</td>\n",
       "      <td>0.975587</td>\n",
       "      <td>0.979841</td>\n",
       "      <td>0.983248</td>\n",
       "      <td>0.976962</td>\n",
       "      <td>0.980822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979434</td>\n",
       "      <td>0.976436</td>\n",
       "      <td>0.976646</td>\n",
       "      <td>0.980531</td>\n",
       "      <td>0.979617</td>\n",
       "      <td>0.983158</td>\n",
       "      <td>0.984743</td>\n",
       "      <td>0.984784</td>\n",
       "      <td>0.981337</td>\n",
       "      <td>0.974034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg34</th>\n",
       "      <td>0.977294</td>\n",
       "      <td>0.983350</td>\n",
       "      <td>0.982073</td>\n",
       "      <td>0.974763</td>\n",
       "      <td>0.972153</td>\n",
       "      <td>0.969437</td>\n",
       "      <td>0.968124</td>\n",
       "      <td>0.973041</td>\n",
       "      <td>0.983394</td>\n",
       "      <td>0.972768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981647</td>\n",
       "      <td>0.972599</td>\n",
       "      <td>0.967924</td>\n",
       "      <td>0.974770</td>\n",
       "      <td>0.974886</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.972016</td>\n",
       "      <td>0.973732</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.974813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg35</th>\n",
       "      <td>0.981513</td>\n",
       "      <td>0.972820</td>\n",
       "      <td>0.978480</td>\n",
       "      <td>0.969130</td>\n",
       "      <td>0.980045</td>\n",
       "      <td>0.971161</td>\n",
       "      <td>0.980785</td>\n",
       "      <td>0.982754</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978319</td>\n",
       "      <td>0.975999</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0.980465</td>\n",
       "      <td>0.979615</td>\n",
       "      <td>0.981488</td>\n",
       "      <td>0.982773</td>\n",
       "      <td>0.982102</td>\n",
       "      <td>0.983273</td>\n",
       "      <td>0.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg36</th>\n",
       "      <td>0.959926</td>\n",
       "      <td>0.972520</td>\n",
       "      <td>0.968959</td>\n",
       "      <td>0.967115</td>\n",
       "      <td>0.956762</td>\n",
       "      <td>0.957916</td>\n",
       "      <td>0.953069</td>\n",
       "      <td>0.958243</td>\n",
       "      <td>0.971972</td>\n",
       "      <td>0.951983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965703</td>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.954205</td>\n",
       "      <td>0.963673</td>\n",
       "      <td>0.960924</td>\n",
       "      <td>0.959348</td>\n",
       "      <td>0.957557</td>\n",
       "      <td>0.956403</td>\n",
       "      <td>0.959001</td>\n",
       "      <td>0.966705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg37</th>\n",
       "      <td>0.948254</td>\n",
       "      <td>0.962102</td>\n",
       "      <td>0.954508</td>\n",
       "      <td>0.953977</td>\n",
       "      <td>0.949053</td>\n",
       "      <td>0.943044</td>\n",
       "      <td>0.941682</td>\n",
       "      <td>0.944875</td>\n",
       "      <td>0.956574</td>\n",
       "      <td>0.941832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951279</td>\n",
       "      <td>0.948812</td>\n",
       "      <td>0.941369</td>\n",
       "      <td>0.954374</td>\n",
       "      <td>0.946690</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.939590</td>\n",
       "      <td>0.949203</td>\n",
       "      <td>0.944329</td>\n",
       "      <td>0.957393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg38</th>\n",
       "      <td>0.985305</td>\n",
       "      <td>0.981622</td>\n",
       "      <td>0.982033</td>\n",
       "      <td>0.974212</td>\n",
       "      <td>0.982944</td>\n",
       "      <td>0.977582</td>\n",
       "      <td>0.981726</td>\n",
       "      <td>0.980896</td>\n",
       "      <td>0.983220</td>\n",
       "      <td>0.983186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985553</td>\n",
       "      <td>0.983958</td>\n",
       "      <td>0.979961</td>\n",
       "      <td>0.984881</td>\n",
       "      <td>0.984573</td>\n",
       "      <td>0.984456</td>\n",
       "      <td>0.986868</td>\n",
       "      <td>0.985766</td>\n",
       "      <td>0.986081</td>\n",
       "      <td>0.974653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg39</th>\n",
       "      <td>0.978317</td>\n",
       "      <td>0.974139</td>\n",
       "      <td>0.976162</td>\n",
       "      <td>0.965029</td>\n",
       "      <td>0.976933</td>\n",
       "      <td>0.971070</td>\n",
       "      <td>0.977114</td>\n",
       "      <td>0.980960</td>\n",
       "      <td>0.975434</td>\n",
       "      <td>0.976842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977988</td>\n",
       "      <td>0.976606</td>\n",
       "      <td>0.972390</td>\n",
       "      <td>0.978788</td>\n",
       "      <td>0.978835</td>\n",
       "      <td>0.980028</td>\n",
       "      <td>0.979721</td>\n",
       "      <td>0.980876</td>\n",
       "      <td>0.978489</td>\n",
       "      <td>0.969089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg40</th>\n",
       "      <td>0.980512</td>\n",
       "      <td>0.981852</td>\n",
       "      <td>0.981455</td>\n",
       "      <td>0.975202</td>\n",
       "      <td>0.980442</td>\n",
       "      <td>0.970019</td>\n",
       "      <td>0.975146</td>\n",
       "      <td>0.976238</td>\n",
       "      <td>0.984046</td>\n",
       "      <td>0.977241</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979380</td>\n",
       "      <td>0.975087</td>\n",
       "      <td>0.979068</td>\n",
       "      <td>0.983169</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>0.979913</td>\n",
       "      <td>0.981925</td>\n",
       "      <td>0.981380</td>\n",
       "      <td>0.973082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg41</th>\n",
       "      <td>0.977156</td>\n",
       "      <td>0.974734</td>\n",
       "      <td>0.978083</td>\n",
       "      <td>0.971817</td>\n",
       "      <td>0.976273</td>\n",
       "      <td>0.969217</td>\n",
       "      <td>0.979981</td>\n",
       "      <td>0.976621</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.975077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971119</td>\n",
       "      <td>0.980264</td>\n",
       "      <td>0.977636</td>\n",
       "      <td>0.976241</td>\n",
       "      <td>0.972490</td>\n",
       "      <td>0.981556</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.968645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg42</th>\n",
       "      <td>0.977947</td>\n",
       "      <td>0.973112</td>\n",
       "      <td>0.972824</td>\n",
       "      <td>0.965979</td>\n",
       "      <td>0.976257</td>\n",
       "      <td>0.972377</td>\n",
       "      <td>0.975245</td>\n",
       "      <td>0.973131</td>\n",
       "      <td>0.974839</td>\n",
       "      <td>0.976420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975087</td>\n",
       "      <td>0.971119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978793</td>\n",
       "      <td>0.976311</td>\n",
       "      <td>0.973618</td>\n",
       "      <td>0.977640</td>\n",
       "      <td>0.977262</td>\n",
       "      <td>0.976730</td>\n",
       "      <td>0.973711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg43</th>\n",
       "      <td>0.981513</td>\n",
       "      <td>0.978877</td>\n",
       "      <td>0.979892</td>\n",
       "      <td>0.974335</td>\n",
       "      <td>0.980883</td>\n",
       "      <td>0.976065</td>\n",
       "      <td>0.978846</td>\n",
       "      <td>0.978654</td>\n",
       "      <td>0.980695</td>\n",
       "      <td>0.979723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979068</td>\n",
       "      <td>0.980264</td>\n",
       "      <td>0.978793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.979058</td>\n",
       "      <td>0.977407</td>\n",
       "      <td>0.982142</td>\n",
       "      <td>0.979574</td>\n",
       "      <td>0.977050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg44</th>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.979431</td>\n",
       "      <td>0.979012</td>\n",
       "      <td>0.971078</td>\n",
       "      <td>0.979364</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>0.977729</td>\n",
       "      <td>0.977455</td>\n",
       "      <td>0.978954</td>\n",
       "      <td>0.982016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983169</td>\n",
       "      <td>0.977636</td>\n",
       "      <td>0.976311</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981949</td>\n",
       "      <td>0.981666</td>\n",
       "      <td>0.982942</td>\n",
       "      <td>0.982375</td>\n",
       "      <td>0.970050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg45</th>\n",
       "      <td>0.981483</td>\n",
       "      <td>0.975573</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.970410</td>\n",
       "      <td>0.976211</td>\n",
       "      <td>0.975768</td>\n",
       "      <td>0.978267</td>\n",
       "      <td>0.982510</td>\n",
       "      <td>0.979101</td>\n",
       "      <td>0.979522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>0.976241</td>\n",
       "      <td>0.973618</td>\n",
       "      <td>0.979058</td>\n",
       "      <td>0.981949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982429</td>\n",
       "      <td>0.982078</td>\n",
       "      <td>0.982583</td>\n",
       "      <td>0.973140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg46</th>\n",
       "      <td>0.983084</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.977123</td>\n",
       "      <td>0.969232</td>\n",
       "      <td>0.981681</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.977926</td>\n",
       "      <td>0.977932</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.978882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979913</td>\n",
       "      <td>0.972490</td>\n",
       "      <td>0.977640</td>\n",
       "      <td>0.977407</td>\n",
       "      <td>0.981666</td>\n",
       "      <td>0.982429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981509</td>\n",
       "      <td>0.981774</td>\n",
       "      <td>0.969387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg47</th>\n",
       "      <td>0.982341</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>0.980226</td>\n",
       "      <td>0.971781</td>\n",
       "      <td>0.984610</td>\n",
       "      <td>0.972738</td>\n",
       "      <td>0.983776</td>\n",
       "      <td>0.982038</td>\n",
       "      <td>0.982733</td>\n",
       "      <td>0.983858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981925</td>\n",
       "      <td>0.981556</td>\n",
       "      <td>0.977262</td>\n",
       "      <td>0.982142</td>\n",
       "      <td>0.982942</td>\n",
       "      <td>0.982078</td>\n",
       "      <td>0.981509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983787</td>\n",
       "      <td>0.972968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg48</th>\n",
       "      <td>0.979847</td>\n",
       "      <td>0.976373</td>\n",
       "      <td>0.981219</td>\n",
       "      <td>0.976994</td>\n",
       "      <td>0.979794</td>\n",
       "      <td>0.973628</td>\n",
       "      <td>0.981586</td>\n",
       "      <td>0.978247</td>\n",
       "      <td>0.983106</td>\n",
       "      <td>0.981084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981380</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.976730</td>\n",
       "      <td>0.979574</td>\n",
       "      <td>0.982375</td>\n",
       "      <td>0.982583</td>\n",
       "      <td>0.981774</td>\n",
       "      <td>0.983787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg49</th>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.979191</td>\n",
       "      <td>0.975678</td>\n",
       "      <td>0.967691</td>\n",
       "      <td>0.970683</td>\n",
       "      <td>0.979958</td>\n",
       "      <td>0.971734</td>\n",
       "      <td>0.975423</td>\n",
       "      <td>0.976887</td>\n",
       "      <td>0.969335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.973711</td>\n",
       "      <td>0.977050</td>\n",
       "      <td>0.970050</td>\n",
       "      <td>0.973140</td>\n",
       "      <td>0.969387</td>\n",
       "      <td>0.972968</td>\n",
       "      <td>0.972400</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  is_iceberg4  \\\n",
       "is_iceberg0      1.000000     0.980554     0.981650     0.970135     0.978177   \n",
       "is_iceberg1      0.980554     1.000000     0.980287     0.972908     0.976180   \n",
       "is_iceberg2      0.981650     0.980287     1.000000     0.976590     0.978487   \n",
       "is_iceberg3      0.970135     0.972908     0.976590     1.000000     0.969545   \n",
       "is_iceberg4      0.978177     0.976180     0.978487     0.969545     1.000000   \n",
       "is_iceberg5      0.976217     0.972847     0.976332     0.962886     0.970950   \n",
       "is_iceberg6      0.978550     0.971239     0.977972     0.966389     0.979420   \n",
       "is_iceberg7      0.980526     0.974874     0.977608     0.967832     0.976911   \n",
       "is_iceberg8      0.980679     0.982309     0.983201     0.974243     0.978729   \n",
       "is_iceberg9      0.979944     0.970819     0.973851     0.967090     0.978526   \n",
       "is_iceberg10     0.978986     0.984075     0.980254     0.972012     0.976464   \n",
       "is_iceberg11     0.979575     0.975238     0.979147     0.969264     0.981215   \n",
       "is_iceberg12     0.981202     0.972606     0.977144     0.961507     0.978763   \n",
       "is_iceberg13     0.980246     0.978761     0.977328     0.972912     0.974763   \n",
       "is_iceberg14     0.978273     0.976951     0.980468     0.974728     0.980902   \n",
       "is_iceberg15     0.985174     0.981018     0.982273     0.971233     0.983265   \n",
       "is_iceberg16     0.979138     0.971491     0.975766     0.970864     0.980416   \n",
       "is_iceberg17     0.980151     0.974752     0.979734     0.970800     0.979910   \n",
       "is_iceberg18     0.979617     0.981959     0.979238     0.972482     0.979961   \n",
       "is_iceberg19     0.976182     0.980686     0.980751     0.974096     0.973465   \n",
       "is_iceberg20     0.980535     0.979512     0.983286     0.975853     0.977627   \n",
       "is_iceberg21     0.980398     0.976910     0.981272     0.971025     0.975294   \n",
       "is_iceberg22     0.977810     0.977470     0.981252     0.967719     0.975638   \n",
       "is_iceberg23     0.979774     0.971745     0.977382     0.971195     0.980283   \n",
       "is_iceberg24     0.975199     0.979567     0.977285     0.975938     0.975102   \n",
       "is_iceberg25     0.982456     0.974524     0.978592     0.971959     0.980907   \n",
       "is_iceberg26     0.978488     0.973647     0.975995     0.966335     0.975487   \n",
       "is_iceberg27     0.975484     0.975023     0.979975     0.976297     0.975666   \n",
       "is_iceberg28     0.977063     0.966890     0.975265     0.965835     0.977735   \n",
       "is_iceberg29     0.978981     0.968113     0.973388     0.968518     0.979027   \n",
       "is_iceberg30     0.980160     0.981101     0.980358     0.977610     0.976785   \n",
       "is_iceberg31     0.978830     0.978720     0.978740     0.972315     0.977831   \n",
       "is_iceberg32     0.980099     0.973159     0.976691     0.969323     0.975915   \n",
       "is_iceberg33     0.983825     0.975642     0.980142     0.971703     0.979720   \n",
       "is_iceberg34     0.977294     0.983350     0.982073     0.974763     0.972153   \n",
       "is_iceberg35     0.981513     0.972820     0.978480     0.969130     0.980045   \n",
       "is_iceberg36     0.959926     0.972520     0.968959     0.967115     0.956762   \n",
       "is_iceberg37     0.948254     0.962102     0.954508     0.953977     0.949053   \n",
       "is_iceberg38     0.985305     0.981622     0.982033     0.974212     0.982944   \n",
       "is_iceberg39     0.978317     0.974139     0.976162     0.965029     0.976933   \n",
       "is_iceberg40     0.980512     0.981852     0.981455     0.975202     0.980442   \n",
       "is_iceberg41     0.977156     0.974734     0.978083     0.971817     0.976273   \n",
       "is_iceberg42     0.977947     0.973112     0.972824     0.965979     0.976257   \n",
       "is_iceberg43     0.981513     0.978877     0.979892     0.974335     0.980883   \n",
       "is_iceberg44     0.978365     0.979431     0.979012     0.971078     0.979364   \n",
       "is_iceberg45     0.981483     0.975573     0.980943     0.970410     0.976211   \n",
       "is_iceberg46     0.983084     0.975816     0.977123     0.969232     0.981681   \n",
       "is_iceberg47     0.982341     0.974478     0.980226     0.971781     0.984610   \n",
       "is_iceberg48     0.979847     0.976373     0.981219     0.976994     0.979794   \n",
       "is_iceberg49     0.976667     0.979191     0.975678     0.967691     0.970683   \n",
       "\n",
       "              is_iceberg5  is_iceberg6  is_iceberg7  is_iceberg8  is_iceberg9  \\\n",
       "is_iceberg0      0.976217     0.978550     0.980526     0.980679     0.979944   \n",
       "is_iceberg1      0.972847     0.971239     0.974874     0.982309     0.970819   \n",
       "is_iceberg2      0.976332     0.977972     0.977608     0.983201     0.973851   \n",
       "is_iceberg3      0.962886     0.966389     0.967832     0.974243     0.967090   \n",
       "is_iceberg4      0.970950     0.979420     0.976911     0.978729     0.978526   \n",
       "is_iceberg5      1.000000     0.973648     0.974714     0.974152     0.970282   \n",
       "is_iceberg6      0.973648     1.000000     0.976894     0.974675     0.978224   \n",
       "is_iceberg7      0.974714     0.976894     1.000000     0.977910     0.978892   \n",
       "is_iceberg8      0.974152     0.974675     0.977910     1.000000     0.976175   \n",
       "is_iceberg9      0.970282     0.978224     0.978892     0.976175     1.000000   \n",
       "is_iceberg10     0.973951     0.974297     0.974471     0.984482     0.974546   \n",
       "is_iceberg11     0.971910     0.979551     0.981440     0.979617     0.977286   \n",
       "is_iceberg12     0.973707     0.977479     0.975604     0.973427     0.976692   \n",
       "is_iceberg13     0.970470     0.976463     0.978040     0.979838     0.980304   \n",
       "is_iceberg14     0.970702     0.977764     0.978983     0.979580     0.978848   \n",
       "is_iceberg15     0.974631     0.982356     0.983191     0.984228     0.980477   \n",
       "is_iceberg16     0.969756     0.979263     0.977642     0.979585     0.980629   \n",
       "is_iceberg17     0.974921     0.982106     0.979602     0.978986     0.978475   \n",
       "is_iceberg18     0.976690     0.976394     0.976194     0.982011     0.975816   \n",
       "is_iceberg19     0.973926     0.972146     0.977715     0.982241     0.971330   \n",
       "is_iceberg20     0.966512     0.974041     0.975017     0.980611     0.977106   \n",
       "is_iceberg21     0.971998     0.974597     0.976223     0.975602     0.972418   \n",
       "is_iceberg22     0.979125     0.977523     0.978012     0.978343     0.975156   \n",
       "is_iceberg23     0.968722     0.980282     0.982571     0.980019     0.982716   \n",
       "is_iceberg24     0.971606     0.971162     0.972511     0.980439     0.973978   \n",
       "is_iceberg25     0.972770     0.980930     0.981130     0.979064     0.982183   \n",
       "is_iceberg26     0.968666     0.977082     0.977958     0.976694     0.975397   \n",
       "is_iceberg27     0.969134     0.975988     0.974176     0.979257     0.972638   \n",
       "is_iceberg28     0.970907     0.980122     0.974990     0.972780     0.975398   \n",
       "is_iceberg29     0.969310     0.978030     0.977616     0.974384     0.981294   \n",
       "is_iceberg30     0.971012     0.976410     0.979940     0.981882     0.977216   \n",
       "is_iceberg31     0.965006     0.976670     0.974851     0.984478     0.975228   \n",
       "is_iceberg32     0.966437     0.976178     0.979744     0.980953     0.981192   \n",
       "is_iceberg33     0.975587     0.979841     0.983248     0.976962     0.980822   \n",
       "is_iceberg34     0.969437     0.968124     0.973041     0.983394     0.972768   \n",
       "is_iceberg35     0.971161     0.980785     0.982754     0.979582     0.981266   \n",
       "is_iceberg36     0.957916     0.953069     0.958243     0.971972     0.951983   \n",
       "is_iceberg37     0.943044     0.941682     0.944875     0.956574     0.941832   \n",
       "is_iceberg38     0.977582     0.981726     0.980896     0.983220     0.983186   \n",
       "is_iceberg39     0.971070     0.977114     0.980960     0.975434     0.976842   \n",
       "is_iceberg40     0.970019     0.975146     0.976238     0.984046     0.977241   \n",
       "is_iceberg41     0.969217     0.979981     0.976621     0.980900     0.975077   \n",
       "is_iceberg42     0.972377     0.975245     0.973131     0.974839     0.976420   \n",
       "is_iceberg43     0.976065     0.978846     0.978654     0.980695     0.979723   \n",
       "is_iceberg44     0.971558     0.977729     0.977455     0.978954     0.982016   \n",
       "is_iceberg45     0.975768     0.978267     0.982510     0.979101     0.979522   \n",
       "is_iceberg46     0.973931     0.977926     0.977932     0.976608     0.978882   \n",
       "is_iceberg47     0.972738     0.983776     0.982038     0.982733     0.983858   \n",
       "is_iceberg48     0.973628     0.981586     0.978247     0.983106     0.981084   \n",
       "is_iceberg49     0.979958     0.971734     0.975423     0.976887     0.969335   \n",
       "\n",
       "                  ...       is_iceberg40  is_iceberg41  is_iceberg42  \\\n",
       "is_iceberg0       ...           0.980512      0.977156      0.977947   \n",
       "is_iceberg1       ...           0.981852      0.974734      0.973112   \n",
       "is_iceberg2       ...           0.981455      0.978083      0.972824   \n",
       "is_iceberg3       ...           0.975202      0.971817      0.965979   \n",
       "is_iceberg4       ...           0.980442      0.976273      0.976257   \n",
       "is_iceberg5       ...           0.970019      0.969217      0.972377   \n",
       "is_iceberg6       ...           0.975146      0.979981      0.975245   \n",
       "is_iceberg7       ...           0.976238      0.976621      0.973131   \n",
       "is_iceberg8       ...           0.984046      0.980900      0.974839   \n",
       "is_iceberg9       ...           0.977241      0.975077      0.976420   \n",
       "is_iceberg10      ...           0.979781      0.976626      0.974983   \n",
       "is_iceberg11      ...           0.977945      0.980906      0.972673   \n",
       "is_iceberg12      ...           0.976770      0.968605      0.979018   \n",
       "is_iceberg13      ...           0.977470      0.974608      0.974753   \n",
       "is_iceberg14      ...           0.983954      0.978756      0.974007   \n",
       "is_iceberg15      ...           0.983172      0.980913      0.979607   \n",
       "is_iceberg16      ...           0.978988      0.978734      0.977197   \n",
       "is_iceberg17      ...           0.979980      0.980738      0.978686   \n",
       "is_iceberg18      ...           0.980131      0.974745      0.976469   \n",
       "is_iceberg19      ...           0.977314      0.977817      0.970053   \n",
       "is_iceberg20      ...           0.980603      0.976269      0.973333   \n",
       "is_iceberg21      ...           0.976673      0.972058      0.974478   \n",
       "is_iceberg22      ...           0.975210      0.976087      0.975903   \n",
       "is_iceberg23      ...           0.981981      0.979787      0.975292   \n",
       "is_iceberg24      ...           0.980086      0.975712      0.969112   \n",
       "is_iceberg25      ...           0.981520      0.976244      0.978641   \n",
       "is_iceberg26      ...           0.972479      0.972733      0.971378   \n",
       "is_iceberg27      ...           0.975928      0.978428      0.968327   \n",
       "is_iceberg28      ...           0.974970      0.973461      0.978636   \n",
       "is_iceberg29      ...           0.977308      0.972648      0.977508   \n",
       "is_iceberg30      ...           0.980744      0.975761      0.974394   \n",
       "is_iceberg31      ...           0.981450      0.984038      0.970956   \n",
       "is_iceberg32      ...           0.975309      0.976968      0.973230   \n",
       "is_iceberg33      ...           0.979434      0.976436      0.976646   \n",
       "is_iceberg34      ...           0.981647      0.972599      0.967924   \n",
       "is_iceberg35      ...           0.978319      0.975999      0.976053   \n",
       "is_iceberg36      ...           0.965703      0.961583      0.954205   \n",
       "is_iceberg37      ...           0.951279      0.948812      0.941369   \n",
       "is_iceberg38      ...           0.985553      0.983958      0.979961   \n",
       "is_iceberg39      ...           0.977988      0.976606      0.972390   \n",
       "is_iceberg40      ...           1.000000      0.979380      0.975087   \n",
       "is_iceberg41      ...           0.979380      1.000000      0.971119   \n",
       "is_iceberg42      ...           0.975087      0.971119      1.000000   \n",
       "is_iceberg43      ...           0.979068      0.980264      0.978793   \n",
       "is_iceberg44      ...           0.983169      0.977636      0.976311   \n",
       "is_iceberg45      ...           0.977216      0.976241      0.973618   \n",
       "is_iceberg46      ...           0.979913      0.972490      0.977640   \n",
       "is_iceberg47      ...           0.981925      0.981556      0.977262   \n",
       "is_iceberg48      ...           0.981380      0.978052      0.976730   \n",
       "is_iceberg49      ...           0.973082      0.968645      0.973711   \n",
       "\n",
       "              is_iceberg43  is_iceberg44  is_iceberg45  is_iceberg46  \\\n",
       "is_iceberg0       0.981513      0.978365      0.981483      0.983084   \n",
       "is_iceberg1       0.978877      0.979431      0.975573      0.975816   \n",
       "is_iceberg2       0.979892      0.979012      0.980943      0.977123   \n",
       "is_iceberg3       0.974335      0.971078      0.970410      0.969232   \n",
       "is_iceberg4       0.980883      0.979364      0.976211      0.981681   \n",
       "is_iceberg5       0.976065      0.971558      0.975768      0.973931   \n",
       "is_iceberg6       0.978846      0.977729      0.978267      0.977926   \n",
       "is_iceberg7       0.978654      0.977455      0.982510      0.977932   \n",
       "is_iceberg8       0.980695      0.978954      0.979101      0.976608   \n",
       "is_iceberg9       0.979723      0.982016      0.979522      0.978882   \n",
       "is_iceberg10      0.980009      0.980750      0.975109      0.974379   \n",
       "is_iceberg11      0.981311      0.979035      0.978884      0.980034   \n",
       "is_iceberg12      0.974965      0.978515      0.979780      0.982978   \n",
       "is_iceberg13      0.976007      0.979122      0.979669      0.978801   \n",
       "is_iceberg14      0.979532      0.983198      0.978079      0.977663   \n",
       "is_iceberg15      0.985294      0.981217      0.984682      0.981548   \n",
       "is_iceberg16      0.978901      0.979735      0.980635      0.982613   \n",
       "is_iceberg17      0.981618      0.977398      0.977357      0.979474   \n",
       "is_iceberg18      0.984767      0.977213      0.975581      0.974887   \n",
       "is_iceberg19      0.978719      0.975640      0.975223      0.975143   \n",
       "is_iceberg20      0.976274      0.978065      0.977063      0.977590   \n",
       "is_iceberg21      0.975692      0.974533      0.976880      0.976745   \n",
       "is_iceberg22      0.981497      0.977796      0.977386      0.975383   \n",
       "is_iceberg23      0.976935      0.981279      0.979929      0.981815   \n",
       "is_iceberg24      0.981199      0.973824      0.973179      0.971002   \n",
       "is_iceberg25      0.982150      0.982006      0.980915      0.984354   \n",
       "is_iceberg26      0.973430      0.975360      0.980145      0.978108   \n",
       "is_iceberg27      0.979392      0.971323      0.974422      0.972015   \n",
       "is_iceberg28      0.975532      0.976573      0.977293      0.979804   \n",
       "is_iceberg29      0.974975      0.980358      0.980017      0.983753   \n",
       "is_iceberg30      0.976357      0.981496      0.978985      0.979276   \n",
       "is_iceberg31      0.981514      0.976160      0.972613      0.974541   \n",
       "is_iceberg32      0.974030      0.978268      0.979352      0.980055   \n",
       "is_iceberg33      0.980531      0.979617      0.983158      0.984743   \n",
       "is_iceberg34      0.974770      0.974886      0.973913      0.972016   \n",
       "is_iceberg35      0.980465      0.979615      0.981488      0.982773   \n",
       "is_iceberg36      0.963673      0.960924      0.959348      0.957557   \n",
       "is_iceberg37      0.954374      0.946690      0.942774      0.939590   \n",
       "is_iceberg38      0.984881      0.984573      0.984456      0.986868   \n",
       "is_iceberg39      0.978788      0.978835      0.980028      0.979721   \n",
       "is_iceberg40      0.979068      0.983169      0.977216      0.979913   \n",
       "is_iceberg41      0.980264      0.977636      0.976241      0.972490   \n",
       "is_iceberg42      0.978793      0.976311      0.973618      0.977640   \n",
       "is_iceberg43      1.000000      0.979300      0.979058      0.977407   \n",
       "is_iceberg44      0.979300      1.000000      0.981949      0.981666   \n",
       "is_iceberg45      0.979058      0.981949      1.000000      0.982429   \n",
       "is_iceberg46      0.977407      0.981666      0.982429      1.000000   \n",
       "is_iceberg47      0.982142      0.982942      0.982078      0.981509   \n",
       "is_iceberg48      0.979574      0.982375      0.982583      0.981774   \n",
       "is_iceberg49      0.977050      0.970050      0.973140      0.969387   \n",
       "\n",
       "              is_iceberg47  is_iceberg48  is_iceberg49  \n",
       "is_iceberg0       0.982341      0.979847      0.976667  \n",
       "is_iceberg1       0.974478      0.976373      0.979191  \n",
       "is_iceberg2       0.980226      0.981219      0.975678  \n",
       "is_iceberg3       0.971781      0.976994      0.967691  \n",
       "is_iceberg4       0.984610      0.979794      0.970683  \n",
       "is_iceberg5       0.972738      0.973628      0.979958  \n",
       "is_iceberg6       0.983776      0.981586      0.971734  \n",
       "is_iceberg7       0.982038      0.978247      0.975423  \n",
       "is_iceberg8       0.982733      0.983106      0.976887  \n",
       "is_iceberg9       0.983858      0.981084      0.969335  \n",
       "is_iceberg10      0.978494      0.979830      0.980154  \n",
       "is_iceberg11      0.985089      0.978149      0.971345  \n",
       "is_iceberg12      0.979854      0.980345      0.971089  \n",
       "is_iceberg13      0.979062      0.980619      0.974206  \n",
       "is_iceberg14      0.984110      0.982507      0.970620  \n",
       "is_iceberg15      0.987385      0.982556      0.976271  \n",
       "is_iceberg16      0.983361      0.981088      0.967152  \n",
       "is_iceberg17      0.983191      0.981928      0.972400  \n",
       "is_iceberg18      0.979773      0.977417      0.983515  \n",
       "is_iceberg19      0.975156      0.975895      0.975776  \n",
       "is_iceberg20      0.979648      0.980803      0.969379  \n",
       "is_iceberg21      0.976353      0.977553      0.974348  \n",
       "is_iceberg22      0.978850      0.977402      0.981983  \n",
       "is_iceberg23      0.985030      0.984432      0.967857  \n",
       "is_iceberg24      0.977272      0.978442      0.981068  \n",
       "is_iceberg25      0.984553      0.985436      0.969882  \n",
       "is_iceberg26      0.978540      0.978295      0.968063  \n",
       "is_iceberg27      0.975333      0.978451      0.969353  \n",
       "is_iceberg28      0.982276      0.980682      0.969289  \n",
       "is_iceberg29      0.983728      0.979775      0.965018  \n",
       "is_iceberg30      0.979603      0.983375      0.972428  \n",
       "is_iceberg31      0.985062      0.978859      0.970016  \n",
       "is_iceberg32      0.981326      0.980945      0.964193  \n",
       "is_iceberg33      0.984784      0.981337      0.974034  \n",
       "is_iceberg34      0.973732      0.974400      0.974813  \n",
       "is_iceberg35      0.982102      0.983273      0.969500  \n",
       "is_iceberg36      0.956403      0.959001      0.966705  \n",
       "is_iceberg37      0.949203      0.944329      0.957393  \n",
       "is_iceberg38      0.985766      0.986081      0.974653  \n",
       "is_iceberg39      0.980876      0.978489      0.969089  \n",
       "is_iceberg40      0.981925      0.981380      0.973082  \n",
       "is_iceberg41      0.981556      0.978052      0.968645  \n",
       "is_iceberg42      0.977262      0.976730      0.973711  \n",
       "is_iceberg43      0.982142      0.979574      0.977050  \n",
       "is_iceberg44      0.982942      0.982375      0.970050  \n",
       "is_iceberg45      0.982078      0.982583      0.973140  \n",
       "is_iceberg46      0.981509      0.981774      0.969387  \n",
       "is_iceberg47      1.000000      0.983787      0.972968  \n",
       "is_iceberg48      0.983787      1.000000      0.972400  \n",
       "is_iceberg49      0.972968      0.972400      1.000000  \n",
       "\n",
       "[50 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()\n",
    "# [i for i in os.listdir('vgg_models') if 'r3' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s494ms\n",
      "[=================== 132/132 ================>]  Step: 160ms | Tot: 27s661ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s644ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s598ms\n",
      "[=================== 132/132 ================>]  Step: 161ms | Tot: 27s668ms\n"
     ]
    }
   ],
   "source": [
    "#result_hist\n",
    "\n",
    "temp11 = pd.DataFrame()\n",
    "\n",
    "for i in range(5):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load('resnet34_acc%d.pth'%i))\n",
    "    net.cuda()\n",
    "\n",
    "    test = pd.read_json(BASE_dir + 'test.json')\n",
    "    test_X = raw_to_numpy(test)\n",
    "    test_X.shape \n",
    "    fake_label = np.zeros(len(test_X))\n",
    "\n",
    "    test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' %i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11.median(axis=1)\n",
    "sub.shape\n",
    "sub.to_csv('submission23.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, 0:6].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, 0:6].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, 0:6].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg5']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg5']\n",
    "sub.shape\n",
    "sub.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s704ms\n"
     ]
    }
   ],
   "source": [
    "net = resnet.resnet34(num_classes=2)\n",
    "net.load_state_dict(torch.load('save_resnet34_acc117.pth'))\n",
    "net.cuda()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "prob = [] \n",
    "net.eval()\n",
    "for k, (val_x, val_y) in enumerate(test_loader):\n",
    "    if use_cuda:\n",
    "        val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    x = Variable(val_x)\n",
    "    y = Variable(val_y)\n",
    "    out = net(x)\n",
    "    #prevent overflow\n",
    "    temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "    ans= temp[:,1]/(temp.sum(axis=1))\n",
    "    prob.append(ans)\n",
    "    #print(out.size())\n",
    "    progress_bar(k, len(test_loader))\n",
    "msg = 'is_iceberg%d' %5\n",
    "temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.852644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.816630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.785453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.559032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.827514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.655435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.895989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.897011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.816630</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.559032</td>\n",
       "      <td>0.827514</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.655435</td>\n",
       "      <td>0.895989</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  \\\n",
       "is_iceberg0           1.000000     0.852644     0.822586     0.648968   \n",
       "is_iceberg1           0.852644     1.000000     0.905401     0.754710   \n",
       "is_iceberg2           0.822586     0.905401     1.000000     0.771766   \n",
       "is_iceberg3           0.648968     0.754710     0.771766     1.000000   \n",
       "is_iceberg4           0.883101     0.833295     0.774018     0.685649   \n",
       "is_iceberg5           0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg_max        0.682861     0.821258     0.847868     0.940914   \n",
       "is_iceberg_min        0.922862     0.777728     0.738630     0.592617   \n",
       "is_iceberg_median     0.942663     0.956190     0.918857     0.749656   \n",
       "is_iceberg_base       0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg            0.905900     0.816630     0.785453     0.559032   \n",
       "\n",
       "                   is_iceberg4  is_iceberg5  is_iceberg_max  is_iceberg_min  \\\n",
       "is_iceberg0           0.883101     0.905277        0.682861        0.922862   \n",
       "is_iceberg1           0.833295     0.815734        0.821258        0.777728   \n",
       "is_iceberg2           0.774018     0.784324        0.847868        0.738630   \n",
       "is_iceberg3           0.685649     0.556919        0.940914        0.592617   \n",
       "is_iceberg4           1.000000     0.826391        0.685683        0.920097   \n",
       "is_iceberg5           0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg_max        0.685683     0.653849        1.000000        0.583326   \n",
       "is_iceberg_min        0.920097     0.895245        0.583326        1.000000   \n",
       "is_iceberg_median     0.909537     0.896220        0.792055        0.875356   \n",
       "is_iceberg_base       0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg            0.827514     0.999683        0.655435        0.895989   \n",
       "\n",
       "                   is_iceberg_median  is_iceberg_base  is_iceberg  \n",
       "is_iceberg0                 0.942663         0.905277    0.905900  \n",
       "is_iceberg1                 0.956190         0.815734    0.816630  \n",
       "is_iceberg2                 0.918857         0.784324    0.785453  \n",
       "is_iceberg3                 0.749656         0.556919    0.559032  \n",
       "is_iceberg4                 0.909537         0.826391    0.827514  \n",
       "is_iceberg5                 0.896220         1.000000    0.999683  \n",
       "is_iceberg_max              0.792055         0.653849    0.655435  \n",
       "is_iceberg_min              0.875356         0.895245    0.895989  \n",
       "is_iceberg_median           1.000000         0.896220    0.897011  \n",
       "is_iceberg_base             0.896220         1.000000    0.999683  \n",
       "is_iceberg                  0.897011         0.999683    1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  5,  6,  7,  8,  9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22,\n",
      "       23, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42,\n",
      "       44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62,\n",
      "       63, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 84, 85,\n",
      "       86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99]), array([ 0,  3,  4, 11, 14, 17, 24, 29, 40, 43, 48, 59, 64, 66, 70, 79, 82,\n",
      "       83, 93, 97]))\n"
     ]
    }
   ],
   "source": [
    "seed= np.random.RandomState(67)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "for i in spliter.split(list(range(100))):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s222ms Step: 200ms | Tot: 2s492ms  Step: 199ms | Tot: 2s692ms  Step: 200ms | Tot: 7s692ms  Step: 199ms | Tot: 13s642ms  Step: 200ms | Tot: 14s841ms  Step: 200ms | Tot: 24s62ms\n",
      "[=================== 132/132 ================>]  Step: 152ms | Tot: 26s199ms Step: 199ms | Tot: 1s425ms  Step: 200ms | Tot: 1s626ms  Step: 200ms | Tot: 8s236ms  Step: 200ms | Tot: 12s639ms  Step: 200ms | Tot: 14s844ms  Step: 199ms | Tot: 16s449ms  Step: 199ms | Tot: 17s647ms  Step: 199ms | Tot: 20s857ms  Step: 200ms | Tot: 23s61ms  Step: 199ms | Tot: 25s648ms  Step: 201ms | Tot: 25s850ms\n",
      "[=================== 132/132 ================>]  Step: 151ms | Tot: 26s141ms Step: 199ms | Tot: 6s583ms  Step: 199ms | Tot: 6s982ms 41/132   Step: 200ms | Tot: 9s576ms  Step: 200ms | Tot: 11s185ms\n",
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s219ms Step: 200ms | Tot: 3s394ms  Step: 200ms | Tot: 4s393ms 52/132   Step: 200ms | Tot: 10s614ms  Step: 200ms | Tot: 10s814ms  Step: 200ms | Tot: 12s219ms  Step: 200ms | Tot: 14s624ms  Step: 200ms | Tot: 15s829ms  Step: 200ms | Tot: 18s640ms 99/132   Step: 200ms | Tot: 22s651ms  Step: 200ms | Tot: 23s456ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "for i,pth in enumerate([os.path.join('resnet34_save_model',i) for i in os.listdir(path='resnet34_save_model') if '.pth' in i]):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg']\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.027504e-03</td>\n",
       "      <td>9.244031e-02</td>\n",
       "      <td>1.784263e-02</td>\n",
       "      <td>5.578169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.931345e-03</td>\n",
       "      <td>3.659658e-01</td>\n",
       "      <td>2.564293e-01</td>\n",
       "      <td>1.571568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.239599e-10</td>\n",
       "      <td>1.970750e-21</td>\n",
       "      <td>3.803356e-08</td>\n",
       "      <td>2.089403e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.993261e-01</td>\n",
       "      <td>9.456407e-01</td>\n",
       "      <td>9.853242e-01</td>\n",
       "      <td>9.989353e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448082e-03</td>\n",
       "      <td>6.435396e-02</td>\n",
       "      <td>3.096765e-02</td>\n",
       "      <td>2.362306e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_iceberg0   is_iceberg1   is_iceberg2   is_iceberg3\n",
       "0  7.027504e-03  9.244031e-02  1.784263e-02  5.578169e-03\n",
       "1  3.931345e-03  3.659658e-01  2.564293e-01  1.571568e-02\n",
       "2  5.239599e-10  1.970750e-21  3.803356e-08  2.089403e-21\n",
       "3  9.993261e-01  9.456407e-01  9.853242e-01  9.989353e-01\n",
       "4  1.448082e-03  6.435396e-02  3.096765e-02  2.362306e-04"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = temp11.mean(1)\n",
    "temp11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, :4].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, :4].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, :4].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg3']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! cp vgg_fcn.ipynb vgg_angle.ipynb\n",
    "# temp11.to_csv('others/vgg19_10fold_40.csv',index=False)\n",
    "temp11.to_csv('training_set_result/vgg19_10fold_noaug_50.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
