{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau,StepLR\n",
    "#torch.multiprocessing.set_start_method(\"spawn\")\n",
    "import vgg_fcn\n",
    "import vgg\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import progress_bar\n",
    "from skimage import transform as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_dir = 'data/processed/'\n",
    "\n",
    "train = pd.read_json(BASE_dir + 'train.json')\n",
    "#test = pd.read_json(BASE_dir + 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "use_cuda= True if torch.cuda.is_available() else False\n",
    "#use_cuda =False\n",
    "#dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor \n",
    "dtype = torch.FloatTensor \n",
    "data=  pd.read_json(BASE_dir + 'train.json')\n",
    "\n",
    "class iceberg_dataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label=  self.data[idx], self.label[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            if self.test is False:\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=1).copy()\n",
    "                if np.random.uniform()>0.5:\n",
    "                    img = np.flip(img,axis=2).copy()\n",
    "                rotate = np.random.randint(4, size=1)\n",
    "                if rotate:\n",
    "                    img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                \n",
    "                scale1 = np.exp(np.random.uniform(np.log(1/1.1), np.log(1.1)))\n",
    "                tran = np.random.uniform(-5, 5)\n",
    "                aug = tf.AffineTransform(translation=tran, scale= (scale1, scale1))\n",
    "                img = tf.warp(img, inverse_map=aug)\n",
    "                pass\n",
    "#             temp = []\n",
    "#             for i in img:\n",
    "#                 temp.append(tf.rescale(i,224/75,mode='constant'))\n",
    "#             img = np.stack(temp)\n",
    "            img = torch.from_numpy(img).type(dtype)\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "class iceberg_angle_dataset(Dataset):\n",
    "    def __init__(self, data,angle,label,size=None, transform=None, test=False): #data: 1604 * 3 *75* 75\n",
    "        self.data =data\n",
    "#         self.angle=torch.cat( (torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1),torch.from_numpy(size).type(torch.FloatTensor).unsqueeze(1)),1)\n",
    "        self.angle=torch.from_numpy(angle).type(torch.FloatTensor).unsqueeze(1)\n",
    "        self.label = torch.from_numpy(label).type(torch.LongTensor)\n",
    "        self.transform= transform\n",
    "        self.test= test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label, angle=  self.data[idx], self.label[idx], self.angle[idx]\n",
    "        if self.transform is not None:\n",
    "            #Random Horizontal Flip and Vertical Flip \n",
    "            #https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n",
    "            \n",
    "            #rotate, scale, shear, translation\n",
    "#             if self.test is False:\n",
    "#                 angle = np.random.uniform(0,360)\n",
    "#                 img = tf.rotate(img,angle=angle,resize=False)\n",
    "#                 scale1 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.2)))\n",
    "#                 scale2 = np.exp(np.random.uniform(np.log(1/1.2), np.log(1.1)))\n",
    "#                 #shear = np.random.uniform(-np.pi/18, np.pi/18)\n",
    "#                 #tran = np.random.uniform(-5, 5)\n",
    "#                 #aug = tf.AffineTransform(shear = shear, translation=tran, scale= (scale1, scale2))\n",
    "#                 aug = tf.AffineTransform(scale= (scale1, scale2))\n",
    "#                 img = tf.warp(img, inverse_map=aug)\n",
    "            \n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "            \n",
    "            if self.test is False:\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=1).copy()\n",
    "#                 if np.random.uniform()>0.5:\n",
    "#                     img = np.flip(img,axis=2).copy()\n",
    "#                 rotate = np.random.randint(4, size=1)\n",
    "#                 if rotate:\n",
    "#                     img = np.rot90(img,k=rotate,axes=(1,2)).copy()\n",
    "                pass\n",
    "        img = torch.from_numpy(img).type(dtype)\n",
    "#         img = self.transform(img)\n",
    "\n",
    "        return img, angle,label    \n",
    "    \n",
    "    \n",
    "def stack(row):\n",
    "    return np.stack(row[['c1','c2','c3']]).reshape(3,75,75)\n",
    "\n",
    "def raw_to_numpy(data):\n",
    "    img = []\n",
    "    data['c1'] = data['band_1'].apply(np.array)\n",
    "    data['c2'] = data['band_2'].apply(np.array)\n",
    "    data['c3'] = (data['c1'] + data['c2'])/2\n",
    "#     data['c3'] = (data['c1'] + data['c2'])/2\n",
    "    for _, row in data.iterrows():\n",
    "        img.append(stack(row))\n",
    "    return np.stack(img)\n",
    "\n",
    "def transform_compute(img):\n",
    "    train_mean = img.mean(axis=(0,2,3))\n",
    "    train_std = img.std(axis=(0,2,3))\n",
    "    return train_mean, train_std\n",
    "\n",
    "def data_aug(X, y):    \n",
    "    X_rot_30 = []\n",
    "    X_rot_60 = [] \n",
    "    X_h = np.flip(X, 3)\n",
    "    X_v = np.flip(X, 2)\n",
    "    for i in X:\n",
    "        X_rot_30.append(tf.rotate(i,angle=90,resize=False))\n",
    "        X_rot_60.append(tf.rotate(i,angle=270,resize=False))\n",
    "        \n",
    "    X_rot_30 = np.stack(X_rot_30)\n",
    "    X_rot_60 = np.stack(X_rot_60)\n",
    "    ch_y = np.concatenate((y,y,y,y,y))\n",
    "    ch_X = np.concatenate((X, X_h, X_v, X_rot_30, X_rot_60))\n",
    "    return ch_X, ch_y\n",
    "\n",
    "train_X = raw_to_numpy(data)#.transpose(0,2,3,1)\n",
    "train_X.shape     #1604 * 3 *75* 75   N*c*H*W\n",
    "train_y = data['is_iceberg'].values # if iceberg then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are 0\n",
      "We are 50\n",
      "We are 100\n",
      "We are 150\n",
      "We are 200\n",
      "We are 250\n",
      "We are 300\n",
      "We are 350\n",
      "We are 400\n",
      "We are 450\n",
      "We are 500\n",
      "We are 550\n",
      "We are 600\n",
      "We are 650\n",
      "We are 700\n",
      "We are 750\n",
      "We are 800\n",
      "We are 850\n",
      "We are 900\n",
      "We are 950\n",
      "We are 1000\n",
      "We are 1050\n",
      "We are 1100\n",
      "We are 1150\n",
      "We are 1200\n",
      "We are 1250\n",
      "We are 1300\n",
      "We are 1350\n",
      "We are 1400\n",
      "We are 1450\n",
      "We are 1500\n",
      "We are 1550\n",
      "We are 1600\n"
     ]
    }
   ],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "result = []\n",
    "for num,i in enumerate(train_X_del):\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(tf.rescale(j,224/75,mode='constant'))\n",
    "    img = np.stack(temp)\n",
    "    result.append(img)\n",
    "    if num%50==0:\n",
    "        print('We are %d'%num)\n",
    "train_X_del = np.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_index=list(range(1300))\n",
    "# val_index= list(range(1300,1604))\n",
    "# train_index=list(range(304,1604)) \n",
    "# val_index= list(range(304))\n",
    "# # train_X[train_index].shape\n",
    "\n",
    "# # data.inc_angle = data.inc_angle.map(lambda x: 0.0 if x == 'na' else x)\n",
    "# # train_index = np.where(data.inc_angle > 0)[0]\n",
    "# # val_index = np.where(data.inc_angle <= 0)[0]\n",
    "\n",
    "# # seed= np.random.RandomState(123)\n",
    "# # spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# # train_index, val_index = next(spliter.split(train_X))\n",
    "# train_mean, train_std = transform_compute(train_X[train_index])\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X[train_index], label=train_y[train_index], transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "## add augmentation \n",
    "# seed= np.random.RandomState(123)\n",
    "# spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "# train_index, val_index = next(spliter.split(train_X))\n",
    "\n",
    "# train_X_af,train_y_af = data_aug(train_X[train_index], train_y[train_index])\n",
    "# train_mean, train_std = transform_compute(train_X_af)\n",
    "# train_transform = T.Compose([\n",
    "#     T.Normalize(train_mean, train_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = iceberg_dataset(data= train_X_af, label=train_y_af, transform=train_transform)\n",
    "# val_dataset = iceberg_dataset(data= train_X[val_index], label=train_y[val_index], transform=train_transform, test=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=3, \n",
    "#                           shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "# train_X_del = train_X[data.inc_angle!='na',:,:,:]\n",
    "# train_y_del = train_y[data.inc_angle!='na']\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "# af_train_X, af_train_y = data_aug(train_X_del[train_index], train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "af_train_X, af_train_y = train_X_del[train_index], train_y_del[train_index]\n",
    "\n",
    "train_dataset = iceberg_dataset(data= af_train_X, label=af_train_y, transform=train_transform)\n",
    "val_dataset = iceberg_dataset(data= train_X_del[val_index], label=train_y_del[val_index], transform=train_transform, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## angle and size\n",
    "\n",
    "#data['inc_angle'][data['inc_angle']=='na']=0\n",
    "data.loc[data['inc_angle']=='na', 'inc_angle'] = 0\n",
    "\n",
    "train_X = train_X\n",
    "train_angle_del = data['inc_angle'].values\n",
    "train_angle = train_angle_del.astype(np.float)\n",
    "#train_size = train['size'].values\n",
    "train_y = train_y\n",
    "\n",
    "train_X_del = train_X\n",
    "train_y_del = train_y\n",
    "\n",
    "seed= np.random.RandomState(123)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "train_index, val_index = next(spliter.split(train_X_del))\n",
    "# # train_index=list(range(284,1471)) \n",
    "# # val_index= list(range(284))\n",
    "\n",
    "train_mean, train_std = transform_compute(train_X_del[train_index])\n",
    "train_transform = T.Compose([\n",
    "    T.Normalize(train_mean, train_std)\n",
    "])\n",
    "#af_train_X,af_train_angle, af_train_y = data_aug(train_X_del[train_index], train_angle_del[train_index],train_y_del[train_index])\n",
    "#af_train_X, af_train_y = data_aug2(train_X_del[train_index], train_y_del[train_index])\n",
    "\n",
    "\n",
    "train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                    label=train_y[train_index],\n",
    "                                    transform=train_transform, test=False)\n",
    "\n",
    "val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                    label=train_y[val_index],\n",
    "                                    transform=train_transform, test=True)\n",
    "\n",
    "# train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],size=train_size[train_index],\n",
    "#                                     label=train_y[train_index],\n",
    "#                                     transform=train_transform)\n",
    "\n",
    "# val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],size= train_size[val_index],\n",
    "#                                     label=train_y[val_index],\n",
    "#                                     transform=train_transform, test=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.from_numpy(train_X).type(torch.FloatTensor)[1].shape\n",
    "train_X[1]\n",
    "use_cuda\n",
    "# for i in train_loader:\n",
    "#     print(i.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "#         scheduler.step()\n",
    "        scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        net.train()\n",
    "        for j,(batch_x, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            y = Variable(batch_y)\n",
    "            out = net(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                acc= best_acc+ 0.01\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total > val_loss:\n",
    "                e_s = 0\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x, val_y) in enumerate(val_load):\n",
    "        #len(val_x.size())==1\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        \n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        if len(out.size())==1: #in case it's one dimensional\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####train with angle and other\n",
    "\n",
    "\n",
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = pre_vgg.vgg16_bn(pretrained=True)\n",
    "for param in vgg16.parameters():\n",
    "    print(param.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for  i in net.features:\n",
    "#     print(i)\n",
    "#     break\n",
    "# for i in i.parameters():\n",
    "#     print(i)\n",
    "len(net.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.487 | Acc: 75.604% (970/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 1.643 | Acc: 50.467% (162/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.314 | Acc: 87.062% (1117/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.774 | Acc: 69.159% (222/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.283 | Acc: 87.373% (1121/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.388 | Acc: 80.685% (259/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s1ms|Loss: 0.276 | Acc: 88.854% (1140/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.378 | Acc: 80.062% (257/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.220 | Acc: 91.037% (1168/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.233 | Acc: 91.277% (293/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.165 | Acc: 93.920% (1205/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.239 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.188 | Acc: 93.453% (1199/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.230 | Acc: 89.720% (288/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.148 | Acc: 94.778% (1216/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.223 | Acc: 90.654% (291/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.142 | Acc: 95.168% (1221/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.222 | Acc: 90.966% (292/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.132 | Acc: 96.025% (1232/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.900% (295/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.133 | Acc: 96.259% (1235/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.241 | Acc: 90.031% (289/321)\n",
      "\n",
      "This is epoch:12\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s1ms|Loss: 0.131 | Acc: 96.103% (1233/1283)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.223 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.227 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.115 | Acc: 96.804% (1242/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.126 | Acc: 96.493% (1238/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 92.212% (296/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.117 | Acc: 96.493% (1238/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.221 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 95.869% (1230/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 96.415% (1237/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.221 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.493% (1238/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.589% (294/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.118 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.113 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.224 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.114 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.122 | Acc: 96.103% (1233/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:25\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.118 | Acc: 96.882% (1243/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.103 | Acc: 97.038% (1245/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.102 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.109 | Acc: 97.506% (1251/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.226 | Acc: 92.212% (296/321)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.112 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.108 | Acc: 97.116% (1246/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:31\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.104 | Acc: 97.272% (1248/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.116% (1246/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.102 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.123 | Acc: 96.337% (1236/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.104 | Acc: 97.350% (1249/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 92.212% (296/321)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:37\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.119 | Acc: 96.571% (1239/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.226 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.108 | Acc: 96.960% (1244/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.101 | Acc: 97.194% (1247/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.107 | Acc: 96.726% (1241/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:41\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.272% (1248/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.218 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.648% (1240/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.900% (295/321)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.804% (1242/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.217 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.107 | Acc: 97.038% (1245/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.100 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.222 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.106 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.230 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.110 | Acc: 96.415% (1237/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.220 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.101 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.224 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.098 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 91.277% (293/321)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.111 | Acc: 96.337% (1236/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.234 | Acc: 91.589% (294/321)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 4s|Loss: 0.105 | Acc: 96.882% (1243/1283)80)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 2ms|Loss: 0.219 | Acc: 90.966% (292/321)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  75/ 81 ====>..]Step: 0ms| Tot: 3s7ms|Loss: 0.102 | Acc: 97.417% (1169/1200)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1417:\n",
      "KeyboardInterrupt\n",
      "Process Process-1419:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f6aa1aed4b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e3449cb2b121>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Process Process-1418:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "result=[]\n",
    "for i in range(1):\n",
    "    vgg19_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "    num = 256\n",
    "    vgg19_bn.classifier = nn.Sequential(\n",
    "                nn.Linear(512+1, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(num, num),\n",
    "                nn.BatchNorm1d(num),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(num, 2)\n",
    "            )\n",
    "\n",
    "    net= vgg19_bn\n",
    "    # net.load_state_dict(torch.load('vgg_fcn_loss.pth'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # #Adam does not perform so good here   \n",
    "    # #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "    # optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "    scheduler = MultiStepLR(optimizer, [5,11,30], gamma=0.1)\n",
    "#     scheduler = MultiStepLR(optimizer, [10,18,26], gamma=0.1)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'max', patience =3,min_lr= 0.00001)\n",
    "    #5e-3 86\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "        net.cuda()\n",
    "    #     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "    #     cudnn.benchmark = True   \n",
    "\n",
    "    a = train(epoch=80,early_stopping =20)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21936874226246297, 91.27725856697819, 39),\n",
       " (0.21782404165773006, 91.58878504672897, 55),\n",
       " (0.22525541061924254, 90.96573208722741, 53)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result 5* 2* 10*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24811193430535147, 90.03115264797508, 29),\n",
       " (0.21092650229314405, 91.58878504672897, 28),\n",
       " (0.22480700989007207, 90.65420560747664, 53)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.218 | Acc: 92.523% (297/321)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 92.523% (297/321)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.222 | Acc: 93.146% (299/321)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.082 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.225 | Acc: 92.835% (298/321)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:6\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.226 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:7\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.086 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.087 | Acc: 96.804% (1242/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.038% (1245/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:11\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.091 | Acc: 96.648% (1240/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.726% (1241/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.089 | Acc: 97.116% (1246/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.428% (1250/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:15\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.084 | Acc: 97.194% (1247/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:16\n",
      "lr change from 0.000001 to 0.000000\n",
      "\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 96.882% (1243/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.038% (1245/1283)48)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.085 | Acc: 97.350% (1249/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.088 | Acc: 97.272% (1248/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  81/ 81 ======>]Step: 0ms| Tot: 3s|Loss: 0.090 | Acc: 96.960% (1244/1283)64)\n",
      "[=========   6/  6 ==>....]Step: 0ms| Tot: 1ms|Loss: 0.227 | Acc: 93.146% (299/321)\n",
      "\n",
      "This is epoch:21\n",
      "[========>  29/ 81 .......]Step: 0ms| Tot: 1s|Loss: 0.075 | Acc: 96.552% (448/464))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8923:\n",
      "Process Process-8924:\n",
      "Process Process-8925:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-77835431faca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     cudnn.benchmark = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e8875581f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, early_stopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n\u001b[1;32m     50\u001b[0m                 % (loss_avg/total, 100.*correct/total, correct, total))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#vgg16 = vgg_fcn.vgg16_bn(pretrained=True)\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 256),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "net.load_state_dict(torch.load('cnn_ang_loss.pth'))\n",
    "for i in vgg16_bn.features:\n",
    "    i.requires_grad = False\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #Adam does not perform so good here   \n",
    "# #(0.1, 0.0001) (50, 80, 110, 170) 52 epoch reaches the maximum.\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0003, nesterov= True)\n",
    "# optimizer = optim.Adam(net.classifier.parameters(), lr=0.00001, weight_decay=0.0003)\n",
    "scheduler = MultiStepLR(optimizer, [5,10,15], gamma=0.1)\n",
    "# scheduler = MultiStepLR(optimizer, [8,18], gamma=0.1)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'max', patience =10,min_lr= 0.0001)\n",
    "#5e-3 86\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "    net.cuda()\n",
    "#     resnet101 = torch.nn.DataParallel(resnet101, device_ids=range(torch.cuda.device_count()))\n",
    "#     cudnn.benchmark = True   \n",
    "\n",
    "train(epoch=250,early_stopping =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 3, 75, 75)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test_set)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k =np.stack(result).mean(axis=0)\n",
    "# #sub.shape\n",
    "# result[1].shape\n",
    "# np.concatenate(prob).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test_set['id']\n",
    "sub['is_iceberg'] =  np.concatenate(prob)\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>is_iceberg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.886197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_iceberg  is_iceberg2\n",
       "is_iceberg     1.000000     0.886197\n",
       "is_iceberg2    0.886197     1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp= pd.read_csv('submission3.csv') #0.0001 wd one\n",
    "sub['is_iceberg2'] = temp['is_iceberg']\n",
    "sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg19_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(4):\n",
    "    ran_num = np.random.randint(60000,70000,size=1)[0]\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        \n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(2):\n",
    "            vgg16_bn = vgg_fcn.vgg19(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [10,20,30], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            with open(\"vgg19_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase1,with aug, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg19_loss.pth vgg19_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "        \n",
    "        for g in range(2):\n",
    "            cmd = 'cp vgg19_loss{}.pth vgg19_models/r1_10vgg_aug{}_{}{}.pth'.format(g,rou,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.512 | Acc: 73.042% (1054/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.026 | Acc: 50.932% (82/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.355 | Acc: 84.200% (1215/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.561 | Acc: 68.323% (110/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.276 | Acc: 88.912% (1283/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.056 | Acc: 50.932% (82/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 91.130% (1315/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.222 | Acc: 91.268% (1317/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.597 | Acc: 50.932% (82/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.248 | Acc: 89.605% (1293/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.032 | Acc: 61.491% (99/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 93.486% (1349/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 92.547% (149/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 94.595% (1365/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 94.872% (1369/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.306 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 95.565% (1379/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.365 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.188% (1388/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 96.881% (1398/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.466% (1392/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.743% (1396/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 93.789% (151/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.367% (1405/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.089% (1401/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.535% (1393/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.505% (1407/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.020% (1400/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 96.466% (1392/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.505% (1407/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 96.951% (1399/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.089% (1401/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 96.951% (1399/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.251 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.852% (1412/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.555 | Acc: 70.062% (1011/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.572 | Acc: 50.932% (82/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.415 | Acc: 81.428% (1175/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.824 | Acc: 60.248% (97/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.321 | Acc: 86.486% (1248/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.062 | Acc: 59.627% (96/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.275 | Acc: 88.912% (1283/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 90.683% (146/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.227 | Acc: 91.268% (1317/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 91.304% (147/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.215 | Acc: 91.545% (1321/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.549 | Acc: 50.932% (82/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.325 | Acc: 84.823% (1224/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.240 | Acc: 90.090% (1300/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 90.298% (1303/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 91.753% (1324/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.343 | Acc: 83.851% (135/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.190 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 91.925% (148/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.179 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.150 | Acc: 67.081% (108/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.200 | Acc: 91.337% (1318/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.294 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 93.416% (1348/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 94.410% (152/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.169 | Acc: 94.456% (1363/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 94.410% (152/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.142 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 93.789% (151/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.161 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.456% (1363/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.151 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.634% (1380/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 93.168% (150/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.205 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.525% (1364/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.144 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 92.547% (149/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 94.872% (1369/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.144 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.143 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.703% (1381/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.192 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.208 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.327% (1390/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.181 | Acc: 93.789% (151/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.134 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 95.357% (1376/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.565% (1379/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 95.911% (1384/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.535% (1393/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.192 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.188% (1388/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.981% (1385/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.506 | Acc: 73.181% (1056/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.435 | Acc: 80.745% (130/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.322 | Acc: 86.071% (1242/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.463 | Acc: 78.882% (127/161)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.297 | Acc: 88.358% (1275/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 86.957% (140/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.248 | Acc: 90.714% (1309/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.247 | Acc: 89.441% (144/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.209 | Acc: 92.100% (1329/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.400 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.321 | Acc: 86.209% (1244/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.281 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.233 | Acc: 91.268% (1317/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 91.925% (148/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.175 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.186 | Acc: 92.585% (1336/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.981% (1385/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.146 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 95.426% (1377/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.202 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.565% (1379/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.128 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.911% (1384/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.208 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.911% (1384/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.188% (1388/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.188% (1388/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 96.535% (1393/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.199 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.134 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.522 | Acc: 71.864% (1037/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.884 | Acc: 50.311% (81/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.424 | Acc: 79.141% (1142/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 19.229 | Acc: 49.689% (80/161)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.601 | Acc: 68.954% (995/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 12.285 | Acc: 49.689% (80/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.509 | Acc: 72.973% (1053/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 6.003 | Acc: 49.689% (80/161)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.433 | Acc: 78.101% (1127/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.357 | Acc: 85.714% (138/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.321 | Acc: 85.586% (1235/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.171 | Acc: 56.522% (91/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.383 | Acc: 82.121% (1185/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.312 | Acc: 85.714% (138/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.310 | Acc: 86.140% (1243/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.374 | Acc: 83.851% (135/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.296 | Acc: 87.249% (1259/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.653 | Acc: 64.596% (104/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.259 | Acc: 89.744% (1295/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.516 | Acc: 70.186% (113/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.252 | Acc: 90.229% (1302/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 91.304% (147/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.248 | Acc: 90.437% (1305/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.046 | Acc: 69.565% (112/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.258 | Acc: 89.536% (1292/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 91.060% (1314/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.829 | Acc: 49.689% (80/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.237 | Acc: 91.130% (1315/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 93.789% (151/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.223 | Acc: 91.615% (1322/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.222 | Acc: 91.545% (1321/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 94.410% (152/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.217 | Acc: 92.030% (1328/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.232 | Acc: 90.852% (1311/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.234 | Acc: 90.159% (1301/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 95.652% (154/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.236 | Acc: 90.298% (1303/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.181 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.212 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 90.506% (1306/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.214 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.225 | Acc: 91.545% (1321/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.289 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.218 | Acc: 91.684% (1323/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 94.410% (152/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.208 | Acc: 93.070% (1343/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.181 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.238 | Acc: 91.615% (1322/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.214 | Acc: 91.823% (1325/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.227 | Acc: 91.476% (1320/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.173 | Acc: 93.789% (151/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 93.347% (1347/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.216 | Acc: 91.823% (1325/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.179 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.207 | Acc: 92.862% (1340/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.199 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.288 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.206 | Acc: 91.823% (1325/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.206 | Acc: 92.169% (1330/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.219 | Acc: 91.753% (1324/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.202 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.210 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.172 | Acc: 95.652% (154/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.213 | Acc: 92.169% (1330/1443)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.174 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.218 | Acc: 91.823% (1325/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.200 | Acc: 92.585% (1336/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.171 | Acc: 95.031% (153/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.201 | Acc: 92.585% (1336/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.171 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.213 | Acc: 92.585% (1336/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.176 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.210 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.197 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 93.789% (151/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.221 | Acc: 91.823% (1325/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.174 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.188 | Acc: 93.070% (1343/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.200 | Acc: 92.585% (1336/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.208 | Acc: 92.516% (1335/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.212 | Acc: 92.030% (1328/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.211 | Acc: 91.476% (1320/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.173 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.195 | Acc: 93.278% (1346/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.172 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.189 | Acc: 93.209% (1345/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.178 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.219 | Acc: 91.268% (1317/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:57\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.203 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:58\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.218 | Acc: 91.545% (1321/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.179 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:59\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.205 | Acc: 92.308% (1332/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:60\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.198 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:61\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.207 | Acc: 92.100% (1329/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.180 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:62\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.207 | Acc: 91.892% (1326/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:63\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.201 | Acc: 94.040% (1357/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:64\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.208 | Acc: 92.238% (1331/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:65\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.193 | Acc: 93.416% (1348/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.173 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:66\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.206 | Acc: 92.308% (1332/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:67\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.198 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:68\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.198 | Acc: 93.416% (1348/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:69\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.203 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:70\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.192 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:71\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.204 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.179 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:72\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 93.139% (1344/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:73\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.197 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.171 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:74\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.207 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.178 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:75\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.198 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.172 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.510 | Acc: 72.211% (1042/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.791 | Acc: 62.112% (100/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.363 | Acc: 83.784% (1209/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.555 | Acc: 50.932% (82/161)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.343 | Acc: 84.962% (1226/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.411 | Acc: 75.776% (122/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.276 | Acc: 87.942% (1269/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.315 | Acc: 85.093% (137/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.248 | Acc: 89.397% (1290/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.928 | Acc: 57.764% (93/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.412 | Acc: 81.566% (1177/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.566 | Acc: 65.839% (106/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.326 | Acc: 86.209% (1244/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.318 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.284 | Acc: 88.912% (1283/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.595 | Acc: 73.913% (119/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.287 | Acc: 88.288% (1274/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.518 | Acc: 77.019% (124/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.270 | Acc: 89.189% (1287/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.008 | Acc: 51.553% (83/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.259 | Acc: 90.021% (1299/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.014 | Acc: 63.975% (103/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.244 | Acc: 90.437% (1305/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.400 | Acc: 83.851% (135/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.250 | Acc: 89.466% (1291/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.690 | Acc: 55.901% (90/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.228 | Acc: 91.199% (1316/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.623 | Acc: 76.398% (123/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.213 | Acc: 91.615% (1322/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 88.199% (142/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.221 | Acc: 91.130% (1315/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 90.062% (145/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.204 | Acc: 91.753% (1324/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 91.892% (1326/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.251 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.199 | Acc: 92.238% (1331/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 88.199% (142/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.204 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.243 | Acc: 88.820% (143/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.654% (1337/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.191 | Acc: 93.486% (1349/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 90.062% (145/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.209 | Acc: 91.545% (1321/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.241 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.206 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.241 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.189 | Acc: 93.209% (1345/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.241 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.188 | Acc: 93.347% (1347/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.238 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.201 | Acc: 91.753% (1324/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.191 | Acc: 93.209% (1345/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.286 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.175 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 94.040% (1357/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.182 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.346 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.407 | Acc: 79.503% (128/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.176 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.256 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.178 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.161 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.872% (1369/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 94.387% (1362/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.177 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.169 | Acc: 94.525% (1364/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.168 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.166 | Acc: 94.525% (1364/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.174 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.166 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.167 | Acc: 94.387% (1362/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.182 | Acc: 93.347% (1347/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:53\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.174 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.238 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.178 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:57\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:58\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.169 | Acc: 93.624% (1351/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:59\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:60\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 92.377% (1333/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:61\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 93.832% (1354/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:62\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:63\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.156 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:64\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:65\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.167 | Acc: 93.971% (1356/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:66\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.162 | Acc: 94.872% (1369/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:67\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:68\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.169 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:69\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 93.139% (1344/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:70\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.161 | Acc: 94.595% (1365/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:71\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:72\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.528 | Acc: 72.696% (1049/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.270 | Acc: 52.174% (84/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.345 | Acc: 85.655% (1236/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.356 | Acc: 81.366% (131/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.287 | Acc: 87.595% (1264/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.333 | Acc: 85.093% (137/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.276 | Acc: 89.051% (1285/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.418 | Acc: 75.155% (121/161)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.242 | Acc: 90.159% (1301/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.423 | Acc: 49.689% (80/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.241 | Acc: 90.437% (1305/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.537 | Acc: 81.366% (131/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.175 | Acc: 93.763% (1353/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.152 | Acc: 95.031% (153/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.911% (1384/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.159 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.158 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.141 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.160 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.154 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.128 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.154 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.604% (1394/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.604% (1394/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.172 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 97.159% (1402/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 97.020% (1400/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.574% (1408/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.158 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.743% (1396/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 97.020% (1400/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.161 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.505% (1407/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.159% (1402/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 96.951% (1399/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 96.604% (1394/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.505 | Acc: 74.012% (1068/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.399 | Acc: 78.882% (127/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.376 | Acc: 81.982% (1183/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.600 | Acc: 74.534% (120/161)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.279 | Acc: 88.288% (1274/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.422 | Acc: 79.503% (128/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.253 | Acc: 89.674% (1294/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.324 | Acc: 84.472% (136/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.225 | Acc: 91.060% (1314/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 91.925% (148/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.199 | Acc: 92.516% (1335/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.426 | Acc: 81.988% (132/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.191 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.142 | Acc: 94.525% (1364/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 95.031% (153/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.842% (1383/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.604% (1394/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.205 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.085 | Acc: 98.060% (1415/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.084 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.782% (1411/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.199 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.089% (1401/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 95.031% (153/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.081 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.084 | Acc: 97.782% (1411/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.436% (1406/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.079 | Acc: 98.060% (1415/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.205 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.436% (1406/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.247 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 98.198% (1417/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 98.406% (1420/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 94.410% (152/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.084 | Acc: 97.852% (1412/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.199 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.683% (1424/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.487 | Acc: 76.230% (1100/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.674 | Acc: 67.081% (108/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.344 | Acc: 85.239% (1230/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.336 | Acc: 85.093% (137/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.282 | Acc: 88.080% (1271/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.601 | Acc: 66.460% (107/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.233 | Acc: 90.229% (1302/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.815 | Acc: 74.534% (120/161)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.257 | Acc: 88.912% (1283/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.504 | Acc: 76.398% (123/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.202 | Acc: 91.753% (1324/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 90.683% (146/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 90.683% (146/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 91.925% (148/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 90.683% (146/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 97.297% (1404/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.436% (1406/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.574% (1408/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.078 | Acc: 98.267% (1418/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.077 | Acc: 97.990% (1414/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.071 | Acc: 98.822% (1426/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.077 | Acc: 98.060% (1415/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 97.990% (1414/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.072 | Acc: 98.614% (1423/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.227 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.068 | Acc: 98.822% (1426/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.062 | Acc: 98.822% (1426/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.066 | Acc: 98.614% (1423/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 92.547% (149/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.062 | Acc: 98.475% (1421/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.406% (1420/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.406% (1420/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.227 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.067 | Acc: 98.960% (1428/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.061 | Acc: 99.099% (1430/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.077 | Acc: 97.990% (1414/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.062 | Acc: 99.030% (1429/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.074 | Acc: 98.129% (1416/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.614% (1423/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.058 | Acc: 99.099% (1430/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.060 | Acc: 98.960% (1428/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.063 | Acc: 98.614% (1423/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.057 | Acc: 99.030% (1429/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.055 | Acc: 99.168% (1431/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.066 | Acc: 98.475% (1421/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.240 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.061 | Acc: 98.891% (1427/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.058 | Acc: 98.891% (1427/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.066 | Acc: 98.545% (1422/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.270 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.064 | Acc: 98.614% (1423/1443)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.058 | Acc: 99.238% (1432/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.522 | Acc: 73.181% (1056/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.526 | Acc: 72.050% (116/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.351 | Acc: 84.893% (1225/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.389 | Acc: 82.609% (133/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.305 | Acc: 87.179% (1258/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.704 | Acc: 70.807% (114/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.285 | Acc: 87.595% (1264/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.114 | Acc: 57.143% (92/161)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.301 | Acc: 86.279% (1245/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 92.547% (149/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 90.922% (1312/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.171 | Acc: 64.596% (104/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.191 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.557 | Acc: 73.292% (118/161)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.283 | Acc: 88.288% (1274/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.293 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.229 | Acc: 90.229% (1302/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.169% (1330/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.480 | Acc: 77.640% (125/161)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.198 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.303 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.190 | Acc: 92.931% (1341/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 93.168% (150/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 93.832% (1354/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.205 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.176 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 92.547% (149/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.163 | Acc: 93.971% (1356/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 93.789% (151/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.160 | Acc: 94.387% (1362/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.160 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.160 | Acc: 94.387% (1362/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.456% (1363/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.202 | Acc: 91.925% (148/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.160 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.197 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.595% (1365/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.162 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.426% (1377/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 93.555% (1350/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.426% (1377/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.208 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 91.304% (147/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.199 | Acc: 93.168% (150/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 94.595% (1365/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 92.547% (149/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.192 | Acc: 93.789% (151/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.484 | Acc: 75.606% (1091/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.456 | Acc: 75.155% (121/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.324 | Acc: 86.348% (1246/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.315 | Acc: 85.714% (138/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.273 | Acc: 88.843% (1282/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.537 | Acc: 73.292% (118/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.215 | Acc: 91.407% (1319/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.312 | Acc: 84.472% (136/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.201 | Acc: 92.030% (1328/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.362 | Acc: 80.745% (130/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.175 | Acc: 93.070% (1343/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.643 | Acc: 77.019% (124/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.141 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.282 | Acc: 87.578% (141/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 90.062% (145/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.466% (1392/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 89.441% (144/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 91.304% (147/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 96.743% (1396/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.085 | Acc: 97.505% (1407/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.076 | Acc: 98.060% (1415/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 91.925% (148/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 98.129% (1416/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.241 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.076 | Acc: 97.990% (1414/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.086 | Acc: 97.644% (1409/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.070 | Acc: 98.406% (1420/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.079 | Acc: 97.921% (1413/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.071 | Acc: 97.713% (1410/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.068 | Acc: 98.475% (1421/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 97.852% (1412/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.067 | Acc: 98.337% (1419/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.072 | Acc: 98.129% (1416/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.129% (1416/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.071 | Acc: 97.921% (1413/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.070 | Acc: 98.267% (1418/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 90.062% (145/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.060 | Acc: 98.545% (1422/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.058 | Acc: 98.891% (1427/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.067 | Acc: 98.129% (1416/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.074 | Acc: 98.337% (1419/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.058 | Acc: 98.337% (1419/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 90.683% (146/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.065 | Acc: 98.545% (1422/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.561 | Acc: 68.815% (993/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.086 | Acc: 45.963% (74/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.394 | Acc: 82.744% (1194/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.529 | Acc: 67.081% (108/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.332 | Acc: 85.724% (1237/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.756 | Acc: 66.460% (107/161)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.263 | Acc: 88.912% (1283/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.411 | Acc: 80.124% (129/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.262 | Acc: 89.605% (1293/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.303 | Acc: 88.199% (142/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.211 | Acc: 92.238% (1331/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.379 | Acc: 83.230% (134/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.184 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.283 | Acc: 86.957% (140/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.187 | Acc: 93.763% (1353/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.278 | Acc: 85.714% (138/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.315 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.141 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 86.335% (139/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.142 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.366 | Acc: 84.472% (136/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.146 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.298 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 95.080% (1372/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 95.842% (1383/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.279 | Acc: 83.851% (135/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.279 | Acc: 83.851% (135/161)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.275 | Acc: 85.714% (138/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.327% (1390/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.273 | Acc: 85.714% (138/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.327% (1390/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.951% (1399/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 85.093% (137/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.743% (1396/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.275 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.270 | Acc: 85.714% (138/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.281 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 97.159% (1402/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.188% (1388/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.284 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.535% (1393/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.273 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.119% (1387/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.273 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.050% (1386/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.278 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.327% (1390/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.327% (1390/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.293 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.604% (1394/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 96.812% (1397/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.271 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.258% (1389/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.271 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.466% (1392/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.674% (1395/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.258% (1389/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.271 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.278 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.089% (1401/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.275 | Acc: 85.093% (137/161)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.396% (1391/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.536 | Acc: 71.726% (1035/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 4.123 | Acc: 45.963% (74/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.404 | Acc: 81.012% (1169/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 8.363 | Acc: 45.963% (74/161)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.400 | Acc: 81.982% (1183/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.460 | Acc: 77.019% (124/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.298 | Acc: 87.803% (1267/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 6.357 | Acc: 45.963% (74/161)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.352 | Acc: 84.754% (1223/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.083 | Acc: 57.143% (92/161)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.284 | Acc: 88.080% (1271/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.644 | Acc: 72.671% (117/161)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.253 | Acc: 89.328% (1289/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.335 | Acc: 83.851% (135/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 91.199% (1316/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.301 | Acc: 85.714% (138/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.213 | Acc: 92.100% (1329/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.320 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.200 | Acc: 93.001% (1342/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.291 | Acc: 84.472% (136/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.190 | Acc: 92.793% (1339/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.432 | Acc: 80.745% (130/161)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.207 | Acc: 92.446% (1334/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.311 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.184 | Acc: 93.347% (1347/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.272 | Acc: 87.578% (141/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.195 | Acc: 92.654% (1337/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.302 | Acc: 85.714% (138/161)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 93.832% (1354/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 87.578% (141/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.186 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.168 | Acc: 94.456% (1363/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.283 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.169 | Acc: 93.555% (1350/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 94.248% (1360/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.170 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 93.971% (1356/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.199% (142/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.257 | Acc: 88.199% (142/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.146 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.265 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.167 | Acc: 94.179% (1359/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.264 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.595% (1365/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 88.820% (143/161)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.149 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.387% (1362/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 93.902% (1355/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.264 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.170 | Acc: 93.832% (1354/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.161 | Acc: 94.317% (1361/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.911% (1384/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.288 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 96.258% (1389/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.273 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 88.820% (143/161)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.267 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.565% (1379/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.842% (1383/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 88.199% (142/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.162 | Acc: 94.872% (1369/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.160 | Acc: 94.109% (1358/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 95.010% (1371/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 89.441% (144/161)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:54\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.185 | Acc: 92.723% (1338/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 95.288% (1375/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:57\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 96.466% (1392/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:58\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 95.981% (1385/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:59\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 95.495% (1378/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:60\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.265 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:61\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.277 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:62\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.270 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:63\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.266 | Acc: 87.578% (141/161)\n",
      "\n",
      "This is epoch:64\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.664% (1366/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.286 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:65\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.634% (1380/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 88.820% (143/161)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:66\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 95.149% (1373/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.264 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:67\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.773% (1382/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.266 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:68\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.941% (1370/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:69\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.218% (1374/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:70\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 89.441% (144/161)\n",
      "\n",
      "This is epoch:71\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 95.981% (1385/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 86.335% (139/161)\n",
      "\n",
      "This is epoch:72\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.842% (1383/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:73\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.802% (1368/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 88.199% (142/161)\n",
      "\n",
      "This is epoch:74\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.456% (1363/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.262 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:75\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.149 | Acc: 94.733% (1367/1443)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.273 | Acc: 86.957% (140/161)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.512 | Acc: 73.892% (1067/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.740 | Acc: 65.000% (104/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.359 | Acc: 84.626% (1222/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.478 | Acc: 75.000% (120/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.299 | Acc: 87.465% (1263/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 3.587 | Acc: 53.750% (86/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.269 | Acc: 88.366% (1276/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.442 | Acc: 75.625% (121/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.241 | Acc: 89.820% (1297/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.491 | Acc: 83.750% (134/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.208 | Acc: 92.036% (1329/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.755 | Acc: 65.625% (105/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.167 | Acc: 93.283% (1347/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.183% (1360/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.146 | Acc: 94.737% (1368/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.291% (1376/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 93.750% (150/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.202 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.875% (1370/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.083 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.538 | Acc: 72.161% (1042/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.025 | Acc: 56.250% (90/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.408 | Acc: 80.125% (1157/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.339 | Acc: 85.625% (137/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.326 | Acc: 86.150% (1244/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.776 | Acc: 60.000% (96/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.270 | Acc: 89.958% (1299/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.235 | Acc: 91.690% (1324/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 92.500% (148/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 91.482% (1321/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 93.698% (1353/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.460% (1364/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 93.125% (149/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 94.460% (1364/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.141 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.179 | Acc: 91.250% (146/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s7ms|Loss: 0.129 | Acc: 95.222% (1375/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.922% (1414/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.191 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.953% (1400/1444)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.853% (1413/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.573 | Acc: 67.659% (977/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.527 | Acc: 73.750% (118/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.494 | Acc: 75.831% (1095/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.862 | Acc: 56.875% (91/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.348 | Acc: 84.834% (1225/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.334 | Acc: 83.750% (134/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.302 | Acc: 88.158% (1273/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.483 | Acc: 80.625% (129/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.274 | Acc: 88.573% (1279/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.678 | Acc: 58.125% (93/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.257 | Acc: 90.443% (1306/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 90.000% (144/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.193 | Acc: 93.144% (1345/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.176 | Acc: 93.352% (1348/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 90.000% (144/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 94.044% (1358/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 90.000% (144/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.598% (1366/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 89.375% (143/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.156 | Acc: 95.152% (1374/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.245 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.172 | Acc: 94.598% (1366/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.156 | Acc: 95.014% (1372/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.247 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.806% (1369/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.298 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.252 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.140 | Acc: 95.291% (1376/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.269 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.260 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.144 | Acc: 95.360% (1377/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.256 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.261 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.143 | Acc: 95.637% (1381/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.259 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.241 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.251 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.507 | Acc: 74.100% (1070/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.598 | Acc: 57.500% (92/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.352 | Acc: 84.765% (1224/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.732 | Acc: 73.750% (118/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.292 | Acc: 87.604% (1265/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 85.625% (137/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.228 | Acc: 91.066% (1315/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.258 | Acc: 90.000% (144/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.219 | Acc: 91.620% (1323/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.462 | Acc: 83.750% (134/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.176 | Acc: 93.490% (1350/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.147 | Acc: 94.668% (1367/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 94.598% (1366/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 91.875% (147/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 92.500% (148/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.305 | Acc: 86.875% (139/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.251 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.202 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.086 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.218 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.085 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.078 | Acc: 98.269% (1419/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 98.061% (1416/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.244 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.076 | Acc: 98.269% (1419/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.081 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 98.615% (1424/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 97.853% (1413/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.078 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.074 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.542 | Acc: 70.429% (1017/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.891 | Acc: 59.375% (95/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.389 | Acc: 82.271% (1188/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.792 | Acc: 58.125% (93/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.304 | Acc: 86.911% (1255/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.159 | Acc: 56.875% (91/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.265 | Acc: 88.850% (1283/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.298 | Acc: 86.875% (139/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.221 | Acc: 91.413% (1320/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.329 | Acc: 85.625% (137/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.197 | Acc: 91.898% (1327/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.434 | Acc: 81.875% (131/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.171 | Acc: 93.629% (1352/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 90.625% (145/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.529% (1365/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.268 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.299 | Acc: 87.500% (140/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.271 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.360% (1377/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.271 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.263 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.280 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.281 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.286 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.287 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.293 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.294 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.288 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.282 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.300 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.289 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.516 | Acc: 72.853% (1052/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.870 | Acc: 67.500% (108/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.423 | Acc: 80.402% (1161/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.942 | Acc: 50.625% (81/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.338 | Acc: 85.388% (1233/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.266 | Acc: 88.125% (141/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.301 | Acc: 87.604% (1265/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.728 | Acc: 67.500% (108/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.240 | Acc: 90.720% (1310/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.469 | Acc: 50.625% (81/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.266 | Acc: 88.227% (1274/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.552 | Acc: 76.250% (122/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.214 | Acc: 91.482% (1321/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.180 | Acc: 94.044% (1358/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.283 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.193 | Acc: 92.659% (1338/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.199 | Acc: 91.875% (147/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.163 | Acc: 94.529% (1365/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 89.375% (143/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.737% (1368/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.193 | Acc: 92.500% (148/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 94.460% (1364/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.200 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.154 | Acc: 94.114% (1359/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 91.875% (147/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.151 | Acc: 95.637% (1381/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.254 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.128 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.131 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.185 | Acc: 91.875% (147/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 94.945% (1371/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.145 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.185 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.178 | Acc: 92.500% (148/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.668% (1367/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.185 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 95.014% (1372/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.637% (1381/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.185 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.139 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.845% (1384/1444)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.143 | Acc: 95.637% (1381/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.182 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.185 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.121 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.184 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.180 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.548 | Acc: 70.706% (1021/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.783 | Acc: 60.000% (96/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.355 | Acc: 84.211% (1216/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.379 | Acc: 81.250% (130/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.292 | Acc: 87.327% (1261/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.287 | Acc: 85.000% (136/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.244 | Acc: 90.374% (1305/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.206 | Acc: 91.205% (1317/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.188 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.217 | Acc: 91.967% (1328/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 90.625% (145/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.165 | Acc: 93.490% (1350/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.157 | Acc: 93.750% (150/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.152 | Acc: 94.321% (1362/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.187 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.321% (1362/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.156 | Acc: 93.750% (150/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.156 | Acc: 91.875% (147/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.144 | Acc: 95.222% (1375/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.157 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 98.061% (1416/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 98.199% (1418/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.161 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.083 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.086 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.086 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 98.199% (1418/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 98.338% (1420/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 98.130% (1417/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.177 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.091 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.073 | Acc: 98.269% (1419/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.081 | Acc: 98.199% (1418/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.527 | Acc: 70.845% (1023/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.387 | Acc: 80.000% (128/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.333 | Acc: 85.388% (1233/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.696 | Acc: 71.250% (114/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.272 | Acc: 88.712% (1281/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.285 | Acc: 90.000% (144/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.230 | Acc: 90.720% (1310/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.940 | Acc: 74.375% (119/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.227 | Acc: 90.512% (1307/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.295 | Acc: 83.750% (134/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.196 | Acc: 92.452% (1335/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 90.625% (145/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.945% (1371/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 88.125% (141/160)\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 95.429% (1378/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 88.750% (142/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.134 | Acc: 95.360% (1377/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 89.375% (143/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.130 | Acc: 95.360% (1377/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 95.776% (1383/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.195 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.249 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 91.875% (147/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.089 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.093 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.084 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.085 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.084 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 98.130% (1417/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.086 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.080 | Acc: 98.269% (1419/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.083 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 98.269% (1419/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.212 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.083 | Acc: 98.061% (1416/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 98.199% (1418/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.536 | Acc: 72.022% (1040/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.464 | Acc: 76.875% (123/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.386 | Acc: 83.033% (1199/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.195 | Acc: 53.125% (85/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.299 | Acc: 86.842% (1254/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.252 | Acc: 86.875% (139/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.261 | Acc: 89.820% (1297/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.838 | Acc: 71.250% (114/160)\n",
      "\n",
      "This is epoch:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.241 | Acc: 90.374% (1305/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.498 | Acc: 79.375% (127/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.218 | Acc: 91.551% (1322/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.459 | Acc: 61.250% (98/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.180 | Acc: 92.936% (1342/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 90.000% (144/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.155 | Acc: 94.668% (1367/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 88.750% (142/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.153 | Acc: 94.875% (1370/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.150 | Acc: 94.391% (1363/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.194 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.346 | Acc: 86.875% (139/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.205 | Acc: 91.875% (147/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.097 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.203 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.211 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 92.500% (148/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.214 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.201 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.207 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.077 | Acc: 98.476% (1422/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.922% (1414/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.204 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.210 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.209 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.215 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.087 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.085 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.213 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.853% (1413/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.229 | Acc: 89.375% (143/160)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 97.992% (1415/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 90.000% (144/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.511 | Acc: 74.030% (1069/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.390 | Acc: 81.875% (131/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.346 | Acc: 83.795% (1210/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.295 | Acc: 86.875% (139/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.259 | Acc: 90.235% (1303/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.275 | Acc: 86.875% (139/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.246 | Acc: 89.543% (1293/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 89.375% (143/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.223 | Acc: 91.136% (1316/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.359 | Acc: 83.125% (133/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.201 | Acc: 92.729% (1339/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.221 | Acc: 90.625% (145/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 94.875% (1370/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.239 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.231 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.255 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.078 | Acc: 97.922% (1414/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.075 | Acc: 97.853% (1413/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.235 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.082 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.232 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.071 | Acc: 98.407% (1421/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.062 | Acc: 98.615% (1424/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.068 | Acc: 98.615% (1424/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.062 | Acc: 98.823% (1427/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.074 | Acc: 98.338% (1420/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.253 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.071 | Acc: 98.130% (1417/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.069 | Acc: 98.338% (1420/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.067 | Acc: 98.546% (1423/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.246 | Acc: 91.250% (146/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.068 | Acc: 98.476% (1422/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.252 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.064 | Acc: 98.823% (1427/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.242 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.067 | Acc: 98.338% (1420/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.250 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.544 | Acc: 70.568% (1019/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.746 | Acc: 56.250% (90/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.396 | Acc: 81.856% (1182/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.420 | Acc: 79.375% (127/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.356 | Acc: 84.072% (1214/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.706 | Acc: 63.750% (102/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.311 | Acc: 86.080% (1243/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.687 | Acc: 73.750% (118/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.267 | Acc: 88.712% (1281/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.295 | Acc: 84.375% (135/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.252 | Acc: 89.335% (1290/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.921 | Acc: 60.625% (97/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.195 | Acc: 92.175% (1331/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 93.750% (150/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 94.737% (1368/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 95.625% (153/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.175 | Acc: 94.044% (1358/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.164 | Acc: 94.391% (1363/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.196 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.159 | Acc: 94.875% (1370/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.177 | Acc: 93.698% (1353/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.163 | Acc: 93.698% (1353/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 95.625% (153/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.083% (1373/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.128 | Acc: 95.776% (1383/1444)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.124 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 94.945% (1371/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.121 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.160 | Acc: 94.375% (151/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.173 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.126 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.133 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.157 | Acc: 95.000% (152/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.168 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.178 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:42\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.173 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.169 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.179 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.176 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.170 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.176 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:52\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.166 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:53\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:54\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.159 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:55\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.171 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:56\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:57\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:58\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.176 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.559 | Acc: 68.629% (991/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.695 | Acc: 65.000% (104/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.445 | Acc: 78.601% (1135/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.106 | Acc: 57.500% (92/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.334 | Acc: 85.111% (1229/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.458 | Acc: 77.500% (124/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.281 | Acc: 87.812% (1268/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.398 | Acc: 61.250% (98/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.242 | Acc: 90.443% (1306/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.454 | Acc: 79.375% (127/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.210 | Acc: 91.205% (1317/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.798 | Acc: 58.750% (94/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.380 | Acc: 82.548% (1192/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.248 | Acc: 86.875% (139/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.282 | Acc: 87.258% (1260/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.237 | Acc: 87.500% (140/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.252 | Acc: 89.612% (1294/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 89.375% (143/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.231 | Acc: 90.859% (1312/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.491 | Acc: 81.250% (130/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.199 | Acc: 92.867% (1341/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 91.875% (147/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.174 | Acc: 93.283% (1347/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.180 | Acc: 93.490% (1350/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.218 | Acc: 91.875% (147/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.668% (1367/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.320 | Acc: 87.500% (140/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.129 | Acc: 96.122% (1388/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 93.750% (150/160)\n",
      "acc: Save it!\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 91.875% (147/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.224 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.218 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.135 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.123 | Acc: 95.706% (1382/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.218 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.219 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.127 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.216 | Acc: 92.500% (148/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.116 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.220 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:37\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.100 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:38\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.233 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:39\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.218 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:40\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.109 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.276 | Acc: 90.625% (145/160)\n",
      "\n",
      "This is epoch:41\n",
      "lr change from 0.000010 to 0.000001\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.225 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:43\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.814% (1398/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.234 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:44\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.089 | Acc: 97.922% (1414/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.217 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:45\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.103 | Acc: 97.645% (1410/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.230 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:46\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.099 | Acc: 97.507% (1408/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.226 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:47\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.236 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:48\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.223 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:49\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 98.338% (1420/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.222 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:50\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.095 | Acc: 98.130% (1417/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.228 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:51\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.227 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.560 | Acc: 70.083% (1012/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.790 | Acc: 42.500% (68/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.421 | Acc: 79.640% (1150/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.306 | Acc: 90.625% (145/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.304 | Acc: 87.050% (1257/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 1.578 | Acc: 58.750% (94/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.284 | Acc: 87.465% (1263/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.678 | Acc: 73.750% (118/160)\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.245 | Acc: 90.166% (1302/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.762 | Acc: 74.375% (119/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.225 | Acc: 90.720% (1310/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.038 | Acc: 57.500% (92/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s5ms|Loss: 0.173 | Acc: 93.837% (1355/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.146 | Acc: 96.250% (154/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.158 | Acc: 94.321% (1362/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.198 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.151 | Acc: 94.737% (1368/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.137 | Acc: 94.375% (151/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.157 | Acc: 94.460% (1364/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.186 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.132 | Acc: 95.845% (1384/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.175 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.118 | Acc: 96.191% (1389/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.128 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.190 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.132 | Acc: 95.625% (153/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.132 | Acc: 95.000% (152/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.230% (1404/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.144 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.368% (1406/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.139 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.140 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.157 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.115 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.150 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.260% (1390/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.144 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.155 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.161% (1403/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.139 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.784% (1412/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.163 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.102 | Acc: 96.676% (1396/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.180 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.152 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.098 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.142 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.089 | Acc: 97.091% (1402/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.147 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.158 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.101 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.145 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.090 | Acc: 97.299% (1405/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.142 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.092 | Acc: 97.715% (1411/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.149 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.094 | Acc: 97.438% (1407/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.147 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.088 | Acc: 97.576% (1409/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.143 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.096 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.149 | Acc: 95.000% (152/160)\n",
      "\n",
      "This is epoch:1\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.527 | Acc: 72.022% (1040/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.385 | Acc: 81.875% (131/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:2\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.392 | Acc: 81.925% (1183/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 2.187 | Acc: 57.500% (92/160)\n",
      "\n",
      "This is epoch:3\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.338 | Acc: 85.803% (1239/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.854 | Acc: 64.375% (103/160)\n",
      "\n",
      "This is epoch:4\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.293 | Acc: 86.565% (1250/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.206 | Acc: 91.250% (146/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:5\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.242 | Acc: 89.612% (1294/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.290 | Acc: 86.875% (139/160)\n",
      "\n",
      "This is epoch:6\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s5ms|Loss: 0.217 | Acc: 90.997% (1314/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.274 | Acc: 85.625% (137/160)\n",
      "\n",
      "This is epoch:7\n",
      "lr change from 0.001000 to 0.000100\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.173 | Acc: 93.698% (1353/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.156 | Acc: 93.125% (149/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:8\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.156 | Acc: 94.321% (1362/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.155 | Acc: 93.750% (150/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:9\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.146 | Acc: 95.152% (1374/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.189 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:10\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.148 | Acc: 93.837% (1355/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.161 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:11\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.136 | Acc: 95.568% (1380/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.183 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:12\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.138 | Acc: 95.291% (1376/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.165 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:13\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.137 | Acc: 95.499% (1379/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.149 | Acc: 94.375% (151/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:14\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.125 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.155 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:15\n",
      "lr change from 0.000100 to 0.000010\n",
      "\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.121 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.148 | Acc: 94.375% (151/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:16\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.953% (1400/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.167 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:17\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 95.914% (1385/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.147 | Acc: 93.750% (150/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:18\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.114 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.164 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:19\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.119 | Acc: 95.983% (1386/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.151 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:20\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.154 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:21\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.122 | Acc: 96.053% (1387/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.143 | Acc: 94.375% (151/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:22\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.147 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:23\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.146 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:24\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.108 | Acc: 96.884% (1399/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.142 | Acc: 94.375% (151/160)\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:25\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 96.399% (1392/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.148 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:26\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.113 | Acc: 96.745% (1397/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.149 | Acc: 93.750% (150/160)\n",
      "\n",
      "This is epoch:27\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.110 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.156 | Acc: 93.125% (149/160)\n",
      "\n",
      "This is epoch:28\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.111 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.162 | Acc: 92.500% (148/160)\n",
      "\n",
      "This is epoch:29\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.107 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.148 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:30\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.120 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.156 | Acc: 91.875% (147/160)\n",
      "\n",
      "This is epoch:31\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.112 | Acc: 96.330% (1391/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.146 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:32\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.104 | Acc: 96.607% (1395/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.144 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:33\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.106 | Acc: 97.022% (1401/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.144 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:34\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.117 | Acc: 96.468% (1393/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.141 | Acc: 95.000% (152/160)\n",
      "acc: Save it!\n",
      "loss: Save it!\n",
      "\n",
      "This is epoch:35\n",
      "[=========  91/ 91 ======>]Step: 0ms| Tot: 3s6ms|Loss: 0.105 | Acc: 96.537% (1394/1444)\n",
      "[=========   3/  3 .......]Step: 0ms| Tot: 0ms|Loss: 0.145 | Acc: 94.375% (151/160)\n",
      "\n",
      "This is epoch:36\n",
      "[=========  58/ 91 .......]Step: 0ms| Tot: 2s2ms|Loss: 0.112 | Acc: 97.091% (901/928)\r"
     ]
    }
   ],
   "source": [
    "def train(epoch,early_stopping = None):\n",
    "    global train_data#,out,y,predicted\n",
    "    acc=0\n",
    "    best_acc =0\n",
    "    best_val_loss= 100\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_data={}\n",
    "    train_data['loss_hist'] = loss_hist\n",
    "    train_data['val_loss_hist'] = val_loss_hist\n",
    "    train_data['train_acc_hist'] = train_acc_hist\n",
    "    train_data['val_acc_hist'] =  val_acc_hist\n",
    "    e_s= 0\n",
    "    last_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('\\nThis is epoch:{}'.format(i+1))\n",
    "        total= 0\n",
    "        correct=0\n",
    "        loss_avg= 0\n",
    "        scheduler.step()\n",
    "#         scheduler.step(acc)\n",
    "        if optimizer.param_groups[0]['lr'] < last_lr:\n",
    "            print('lr change from %f to %f\\n' %(last_lr,optimizer.param_groups[0]['lr']))\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "        net.train()\n",
    "        for j,(batch_x,batch_angle, batch_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_angle=batch_angle.type(torch.FloatTensor)\n",
    "            if use_cuda:\n",
    "                batch_x,batch_angle, batch_y = batch_x.cuda(),batch_angle.cuda(),batch_y.cuda()\n",
    "            x = Variable(batch_x)\n",
    "            angle = Variable(batch_angle)\n",
    "            y = Variable(batch_y)\n",
    "            out = net((x, angle))\n",
    "            loss = criterion(out, y)\n",
    "            loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            progress_bar(j, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "            if j % 5==0:\n",
    "                loss_hist.append(loss_avg/total)\n",
    "            \n",
    "        train_acc_hist.append(100.*correct/total)\n",
    "        e_s+=1\n",
    "        if i %1 == 0:\n",
    "            acc, val_loss = test(val_loader)\n",
    "            val_acc_hist.append(acc)\n",
    "            if acc >best_acc:\n",
    "                best_acc= acc\n",
    "                e_s = 0\n",
    "                print('acc: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg16_acc.pth')\n",
    "            if val_loss <best_val_loss:# and loss_avg/total <=val_loss :\n",
    "                best_val_loss= val_loss\n",
    "                e_s = 0\n",
    "                print('loss: Save it!')\n",
    "                torch.save(net.state_dict(), 'vgg16_loss.pth')\n",
    "            if loss_avg/total >val_loss:\n",
    "                e_s=0\n",
    "\n",
    "#             if best_val_loss >= val_loss:\n",
    "#                 best_val_loss= val_loss\n",
    "#                 torch.save(net.state_dict(), 'resnet34_loss%d.pth'%i)\n",
    "        if early_stopping is not None and e_s >= early_stopping:\n",
    "            return best_val_loss,best_acc,i\n",
    "\n",
    "    return best_val_loss,best_acc,i\n",
    "#         if i%50==0 and save:\n",
    "#             torch.save(net.state_dict(), 'resnet50.pth')\n",
    "        \n",
    "def test(val_load):\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_avg= 0\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(val_load):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        if len(out.size())==1:\n",
    "            out = out.unsqueeze(0)\n",
    "        loss = criterion(out, y)\n",
    "        loss_avg += loss.cpu().data[0] *out.size()[0]\n",
    "        #print(out.size())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        correct += predicted.eq(y.data).cpu().sum()\n",
    "        total += out.size()[0]\n",
    "        progress_bar(k, len(val_load), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (loss_avg/total, 100.*correct/total, correct, total))\n",
    "    train_data['val_loss_hist'].append(loss_avg/total) #also keep track of loss of val set\n",
    "    acc =  (correct*100.0)/total\n",
    "    return acc,loss_avg/total\n",
    "\n",
    "#Try different transformation\n",
    "\n",
    "for rou in range(1):\n",
    "    ran_num = np.random.randint(60000,70000, size=1)[0]\n",
    "    seed= np.random.RandomState(ran_num)\n",
    "    spliter = KFold(n_splits=10,shuffle =True,random_state = seed)\n",
    "    for k,(train_index, val_index) in enumerate(spliter.split(train_X_del)):\n",
    "        train_dataset = iceberg_angle_dataset(data= train_X[train_index], angle=train_angle[train_index],\n",
    "                                            label=train_y[train_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        val_dataset = iceberg_angle_dataset(data= train_X[val_index], angle=train_angle[val_index],\n",
    "                                            label=train_y[val_index],\n",
    "                                            transform=train_transform, test=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 16, num_workers=3, \n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "        \n",
    "        candidate = []\n",
    "        for rep in range(3):\n",
    "            vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "            num = 256\n",
    "            vgg16_bn.classifier = nn.Sequential(\n",
    "                        nn.Linear(512+1, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, num),\n",
    "                        nn.BatchNorm1d(num),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(num, 2)\n",
    "                    )\n",
    "            net= vgg16_bn\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001, nesterov= True)\n",
    "            scheduler = MultiStepLR(optimizer, [6,14,40], gamma=0.1)\n",
    "            #5e-3 86\n",
    "            if use_cuda:\n",
    "                criterion.cuda()\n",
    "                net.cuda()\n",
    "            result = train(epoch=75,early_stopping =20)\n",
    "            candidate.append(result[0])\n",
    "            with open(\"vgg_models/log.txt\", \"a\") as myfile:\n",
    "                msg = '10folds, Phase3, At fold {}, seed {},round {} we find one with acc: {}, loss: {}\\n'.format(\n",
    "                                                            k,ran_num,rep+1, result[1], result[0])\n",
    "                myfile.write(msg)\n",
    "            cmd = 'cp vgg16_loss.pth vgg16_loss{}.pth'.format(rep)\n",
    "            os.system(cmd)\n",
    "            del vgg16_bn\n",
    "            if rep ==1 and np.sum(np.array(candidate)>0.20)>=1:\n",
    "                continue\n",
    "            elif rep==1:\n",
    "                break\n",
    "            \n",
    "        for g in range(rep+1):\n",
    "            cmd = 'cp vgg16_loss{}.pth vgg_models/r3_10vgg{}_{}{}.pth'.format(g,rou+4,k,g)\n",
    "            os.system(cmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo poweroff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vgg_models/r1_5vgg0_00.pth', 'vgg_models/r1_5vgg0_11.pth', 'vgg_models/r1_5vgg0_20.pth', 'vgg_models/r1_5vgg0_31.pth', 'vgg_models/r1_5vgg0_40.pth']\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6mss\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s6ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s7ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "waiting_list=  \n",
    "\n",
    "\n",
    "#waiting_list = [i for i in os.listdir('vgg_models/') if 'r1' in i]\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "vgg16_bn = vgg_fcn.vgg16_bn(pretrained=True)#copy.deepcopy(vgg16)\n",
    "# vgg16_bn.avg= nn.Conv2d(512, 512, kernel_size=2,\n",
    "#                                bias=False)\n",
    "\n",
    "# vgg16_bn.classifier = nn.Sequential(\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p=0.6),\n",
    "#             nn.Linear(512, 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Conv2d(512,512, kernel_size= 3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Conv2d(512, 2, kernel_size=3, padding=1,\n",
    "                               bias=False),\n",
    "            nn.AvgPool2d(3)\n",
    "        )\n",
    "\n",
    "net= vgg16_bn\n",
    "\n",
    "print(waiting_list)\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s3ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n",
      "[========= 132/132 ======>]Step: 0ms| Tot: 5s2ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "# temp11= pd.read_csv('plain_cnn_15_models.csv')\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "train_transform=None\n",
    "test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle=test.inc_angle.values.astype(np.float), transform=train_transform,test=True)\n",
    "\n",
    "# data = test\n",
    "# data.loc[data.inc_angle=='na', 'inc_angle']=0\n",
    "# train_angle_del = data['inc_angle'].values\n",
    "# train_angle_del = train_angle_del.astype(np.float)\n",
    "# test_dataset = iceberg_angle_dataset(data= test_X, label=fake_label,angle= train_angle_del, transform=train_transform,test=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "vgg16_bn = vgg_fcn.vgg16(pretrained=True)#copy.deepcopy(vgg16)\n",
    "num = 256\n",
    "vgg16_bn.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, num),\n",
    "            nn.BatchNorm1d(num),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(num, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "waiting_list = ['r3_10vgg0_01.pth','r3_10vgg0_50.pth',\n",
    "                'r3_10vgg0_12.pth','r3_10vgg0_60.pth',\n",
    "                'r3_10vgg0_20.pth','r3_10vgg0_71.pth',\n",
    "                'r3_10vgg0_30.pth','r3_10vgg0_82.pth',\n",
    "                'r3_10vgg0_40.pth','r3_10vgg0_90.pth',\n",
    "                'r3_10vgg1_02.pth','r3_10vgg1_50.pth',\n",
    "                'r3_10vgg1_10.pth','r3_10vgg1_60.pth',\n",
    "                'r3_10vgg1_21.pth','r3_10vgg1_71.pth',\n",
    "                'r3_10vgg1_31.pth','r3_10vgg1_80.pth',\n",
    "                'r3_10vgg1_40.pth','r3_10vgg1_92.pth',\n",
    "                'r3_10vgg2_00.pth','r3_10vgg2_51.pth',\n",
    "                'r3_10vgg2_10.pth','r3_10vgg2_60.pth',\n",
    "                'r3_10vgg2_21.pth','r3_10vgg2_70.pth',\n",
    "                'r3_10vgg2_30.pth','r3_10vgg2_81.pth',\n",
    "                'r3_10vgg2_41.pth','r3_10vgg2_91.pth',\n",
    "                'r3_10vgg3_00.pth','r3_10vgg3_51.pth',\n",
    "                'r3_10vgg3_10.pth','r3_10vgg3_62.pth',\n",
    "                'r3_10vgg3_21.pth','r3_10vgg3_71.pth',\n",
    "                'r3_10vgg3_31.pth','r3_10vgg3_82.pth',\n",
    "                'r3_10vgg3_42.pth','r3_10vgg3_92.pth',\n",
    "                'r3_10vgg4_01.pth','r3_10vgg4_51.pth',\n",
    "                'r3_10vgg4_12.pth','r3_10vgg4_61.pth',\n",
    "                'r3_10vgg4_22.pth','r3_10vgg4_71.pth',\n",
    "                'r3_10vgg4_30.pth','r3_10vgg4_81.pth',\n",
    "                'r3_10vgg4_41.pth','r3_10vgg4_90.pth'\n",
    "\n",
    "]\n",
    "\n",
    "waiting_list= [os.path.join('vgg_models', i) for i in waiting_list] \n",
    "net= vgg16_bn\n",
    "\n",
    "for i,pth in enumerate(waiting_list):\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x,val_angle, val_y) in enumerate(test_loader):\n",
    "        val_angle=val_angle.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            val_x, val_angle,val_y = val_x.cuda(),val_angle.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        angle=Variable(val_angle)\n",
    "        y = Variable(val_y)\n",
    "        out = net((x,angle))\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % (i)\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg6</th>\n",
       "      <th>is_iceberg7</th>\n",
       "      <th>is_iceberg8</th>\n",
       "      <th>is_iceberg9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_iceberg40</th>\n",
       "      <th>is_iceberg41</th>\n",
       "      <th>is_iceberg42</th>\n",
       "      <th>is_iceberg43</th>\n",
       "      <th>is_iceberg44</th>\n",
       "      <th>is_iceberg45</th>\n",
       "      <th>is_iceberg46</th>\n",
       "      <th>is_iceberg47</th>\n",
       "      <th>is_iceberg48</th>\n",
       "      <th>is_iceberg49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770224</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.812716</td>\n",
       "      <td>0.761829</td>\n",
       "      <td>0.751923</td>\n",
       "      <td>0.771430</td>\n",
       "      <td>0.707299</td>\n",
       "      <td>0.733186</td>\n",
       "      <td>0.849820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595581</td>\n",
       "      <td>0.727062</td>\n",
       "      <td>0.739767</td>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.640529</td>\n",
       "      <td>0.816284</td>\n",
       "      <td>0.732457</td>\n",
       "      <td>0.741159</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.769398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.770224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807722</td>\n",
       "      <td>0.811476</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.854919</td>\n",
       "      <td>0.861634</td>\n",
       "      <td>0.824799</td>\n",
       "      <td>0.853850</td>\n",
       "      <td>0.855107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786296</td>\n",
       "      <td>0.838019</td>\n",
       "      <td>0.844386</td>\n",
       "      <td>0.793433</td>\n",
       "      <td>0.697064</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.838689</td>\n",
       "      <td>0.825832</td>\n",
       "      <td>0.724584</td>\n",
       "      <td>0.787544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.807722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783185</td>\n",
       "      <td>0.672175</td>\n",
       "      <td>0.767648</td>\n",
       "      <td>0.895445</td>\n",
       "      <td>0.920282</td>\n",
       "      <td>0.789848</td>\n",
       "      <td>0.795803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.853956</td>\n",
       "      <td>0.892806</td>\n",
       "      <td>0.842076</td>\n",
       "      <td>0.792093</td>\n",
       "      <td>0.873014</td>\n",
       "      <td>0.755374</td>\n",
       "      <td>0.818414</td>\n",
       "      <td>0.788662</td>\n",
       "      <td>0.782689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.812716</td>\n",
       "      <td>0.811476</td>\n",
       "      <td>0.783185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854853</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.820160</td>\n",
       "      <td>0.746989</td>\n",
       "      <td>0.868094</td>\n",
       "      <td>0.835872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651419</td>\n",
       "      <td>0.765239</td>\n",
       "      <td>0.740954</td>\n",
       "      <td>0.837066</td>\n",
       "      <td>0.571896</td>\n",
       "      <td>0.773814</td>\n",
       "      <td>0.872571</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.692183</td>\n",
       "      <td>0.878742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.761829</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.672175</td>\n",
       "      <td>0.854853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901550</td>\n",
       "      <td>0.725504</td>\n",
       "      <td>0.655364</td>\n",
       "      <td>0.871098</td>\n",
       "      <td>0.796065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558676</td>\n",
       "      <td>0.713357</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>0.779774</td>\n",
       "      <td>0.441549</td>\n",
       "      <td>0.687553</td>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.763686</td>\n",
       "      <td>0.594812</td>\n",
       "      <td>0.862225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.751923</td>\n",
       "      <td>0.854919</td>\n",
       "      <td>0.767648</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.901550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819581</td>\n",
       "      <td>0.762304</td>\n",
       "      <td>0.904431</td>\n",
       "      <td>0.819042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682966</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.749079</td>\n",
       "      <td>0.802206</td>\n",
       "      <td>0.548617</td>\n",
       "      <td>0.744157</td>\n",
       "      <td>0.922309</td>\n",
       "      <td>0.827901</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.875096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg6</th>\n",
       "      <td>0.771430</td>\n",
       "      <td>0.861634</td>\n",
       "      <td>0.895445</td>\n",
       "      <td>0.820160</td>\n",
       "      <td>0.725504</td>\n",
       "      <td>0.819581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906023</td>\n",
       "      <td>0.839866</td>\n",
       "      <td>0.827177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819770</td>\n",
       "      <td>0.882035</td>\n",
       "      <td>0.888098</td>\n",
       "      <td>0.800782</td>\n",
       "      <td>0.737620</td>\n",
       "      <td>0.851712</td>\n",
       "      <td>0.791929</td>\n",
       "      <td>0.815928</td>\n",
       "      <td>0.772829</td>\n",
       "      <td>0.811338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg7</th>\n",
       "      <td>0.707299</td>\n",
       "      <td>0.824799</td>\n",
       "      <td>0.920282</td>\n",
       "      <td>0.746989</td>\n",
       "      <td>0.655364</td>\n",
       "      <td>0.762304</td>\n",
       "      <td>0.906023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775508</td>\n",
       "      <td>0.781362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886983</td>\n",
       "      <td>0.877812</td>\n",
       "      <td>0.921650</td>\n",
       "      <td>0.803384</td>\n",
       "      <td>0.807225</td>\n",
       "      <td>0.841130</td>\n",
       "      <td>0.745567</td>\n",
       "      <td>0.803406</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0.766636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg8</th>\n",
       "      <td>0.733186</td>\n",
       "      <td>0.853850</td>\n",
       "      <td>0.789848</td>\n",
       "      <td>0.868094</td>\n",
       "      <td>0.871098</td>\n",
       "      <td>0.904431</td>\n",
       "      <td>0.839866</td>\n",
       "      <td>0.775508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709420</td>\n",
       "      <td>0.842603</td>\n",
       "      <td>0.771504</td>\n",
       "      <td>0.791568</td>\n",
       "      <td>0.560244</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.926564</td>\n",
       "      <td>0.808728</td>\n",
       "      <td>0.629212</td>\n",
       "      <td>0.851471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg9</th>\n",
       "      <td>0.849820</td>\n",
       "      <td>0.855107</td>\n",
       "      <td>0.795803</td>\n",
       "      <td>0.835872</td>\n",
       "      <td>0.796065</td>\n",
       "      <td>0.819042</td>\n",
       "      <td>0.827177</td>\n",
       "      <td>0.781362</td>\n",
       "      <td>0.820538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721193</td>\n",
       "      <td>0.824319</td>\n",
       "      <td>0.787452</td>\n",
       "      <td>0.809392</td>\n",
       "      <td>0.700110</td>\n",
       "      <td>0.816099</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>0.811481</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.821853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg10</th>\n",
       "      <td>0.784080</td>\n",
       "      <td>0.824308</td>\n",
       "      <td>0.911953</td>\n",
       "      <td>0.813061</td>\n",
       "      <td>0.711283</td>\n",
       "      <td>0.798542</td>\n",
       "      <td>0.903903</td>\n",
       "      <td>0.900968</td>\n",
       "      <td>0.803478</td>\n",
       "      <td>0.826071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800318</td>\n",
       "      <td>0.860232</td>\n",
       "      <td>0.882989</td>\n",
       "      <td>0.840762</td>\n",
       "      <td>0.754296</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.779059</td>\n",
       "      <td>0.811007</td>\n",
       "      <td>0.792667</td>\n",
       "      <td>0.819784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg11</th>\n",
       "      <td>0.795289</td>\n",
       "      <td>0.858240</td>\n",
       "      <td>0.884824</td>\n",
       "      <td>0.803229</td>\n",
       "      <td>0.720384</td>\n",
       "      <td>0.804941</td>\n",
       "      <td>0.881563</td>\n",
       "      <td>0.876620</td>\n",
       "      <td>0.794848</td>\n",
       "      <td>0.838434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809062</td>\n",
       "      <td>0.842180</td>\n",
       "      <td>0.905705</td>\n",
       "      <td>0.802211</td>\n",
       "      <td>0.767874</td>\n",
       "      <td>0.862941</td>\n",
       "      <td>0.777541</td>\n",
       "      <td>0.839040</td>\n",
       "      <td>0.796452</td>\n",
       "      <td>0.777816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg12</th>\n",
       "      <td>0.720621</td>\n",
       "      <td>0.787275</td>\n",
       "      <td>0.840585</td>\n",
       "      <td>0.836708</td>\n",
       "      <td>0.772722</td>\n",
       "      <td>0.839856</td>\n",
       "      <td>0.843995</td>\n",
       "      <td>0.835125</td>\n",
       "      <td>0.838059</td>\n",
       "      <td>0.804809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770710</td>\n",
       "      <td>0.842825</td>\n",
       "      <td>0.788455</td>\n",
       "      <td>0.832198</td>\n",
       "      <td>0.659062</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.844225</td>\n",
       "      <td>0.818025</td>\n",
       "      <td>0.707617</td>\n",
       "      <td>0.886423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg13</th>\n",
       "      <td>0.810199</td>\n",
       "      <td>0.872938</td>\n",
       "      <td>0.863438</td>\n",
       "      <td>0.875431</td>\n",
       "      <td>0.832877</td>\n",
       "      <td>0.888351</td>\n",
       "      <td>0.882262</td>\n",
       "      <td>0.837610</td>\n",
       "      <td>0.872536</td>\n",
       "      <td>0.857105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743886</td>\n",
       "      <td>0.833323</td>\n",
       "      <td>0.837923</td>\n",
       "      <td>0.861489</td>\n",
       "      <td>0.647365</td>\n",
       "      <td>0.836509</td>\n",
       "      <td>0.865610</td>\n",
       "      <td>0.824239</td>\n",
       "      <td>0.745863</td>\n",
       "      <td>0.867494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg14</th>\n",
       "      <td>0.698230</td>\n",
       "      <td>0.845107</td>\n",
       "      <td>0.844882</td>\n",
       "      <td>0.802195</td>\n",
       "      <td>0.741242</td>\n",
       "      <td>0.825850</td>\n",
       "      <td>0.856901</td>\n",
       "      <td>0.828385</td>\n",
       "      <td>0.895478</td>\n",
       "      <td>0.800760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798101</td>\n",
       "      <td>0.870973</td>\n",
       "      <td>0.809283</td>\n",
       "      <td>0.777105</td>\n",
       "      <td>0.651234</td>\n",
       "      <td>0.784154</td>\n",
       "      <td>0.866822</td>\n",
       "      <td>0.826653</td>\n",
       "      <td>0.648304</td>\n",
       "      <td>0.779728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg15</th>\n",
       "      <td>0.741699</td>\n",
       "      <td>0.861851</td>\n",
       "      <td>0.763364</td>\n",
       "      <td>0.866889</td>\n",
       "      <td>0.894971</td>\n",
       "      <td>0.925519</td>\n",
       "      <td>0.827613</td>\n",
       "      <td>0.760706</td>\n",
       "      <td>0.930914</td>\n",
       "      <td>0.818929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689848</td>\n",
       "      <td>0.811980</td>\n",
       "      <td>0.748931</td>\n",
       "      <td>0.790480</td>\n",
       "      <td>0.534577</td>\n",
       "      <td>0.742209</td>\n",
       "      <td>0.940440</td>\n",
       "      <td>0.829413</td>\n",
       "      <td>0.614827</td>\n",
       "      <td>0.865512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg16</th>\n",
       "      <td>0.678974</td>\n",
       "      <td>0.805644</td>\n",
       "      <td>0.866538</td>\n",
       "      <td>0.710251</td>\n",
       "      <td>0.617672</td>\n",
       "      <td>0.725501</td>\n",
       "      <td>0.889423</td>\n",
       "      <td>0.892696</td>\n",
       "      <td>0.733423</td>\n",
       "      <td>0.759254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843514</td>\n",
       "      <td>0.833940</td>\n",
       "      <td>0.870438</td>\n",
       "      <td>0.760311</td>\n",
       "      <td>0.781933</td>\n",
       "      <td>0.803833</td>\n",
       "      <td>0.685734</td>\n",
       "      <td>0.750672</td>\n",
       "      <td>0.778434</td>\n",
       "      <td>0.744502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg17</th>\n",
       "      <td>0.782807</td>\n",
       "      <td>0.864550</td>\n",
       "      <td>0.830328</td>\n",
       "      <td>0.846135</td>\n",
       "      <td>0.817049</td>\n",
       "      <td>0.868934</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.829383</td>\n",
       "      <td>0.868041</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767218</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.810077</td>\n",
       "      <td>0.862770</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>0.839649</td>\n",
       "      <td>0.868158</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.755056</td>\n",
       "      <td>0.866510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg18</th>\n",
       "      <td>0.787730</td>\n",
       "      <td>0.849181</td>\n",
       "      <td>0.901977</td>\n",
       "      <td>0.785391</td>\n",
       "      <td>0.707005</td>\n",
       "      <td>0.794368</td>\n",
       "      <td>0.911134</td>\n",
       "      <td>0.894044</td>\n",
       "      <td>0.803979</td>\n",
       "      <td>0.816891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823860</td>\n",
       "      <td>0.877732</td>\n",
       "      <td>0.898949</td>\n",
       "      <td>0.820653</td>\n",
       "      <td>0.784234</td>\n",
       "      <td>0.901708</td>\n",
       "      <td>0.771894</td>\n",
       "      <td>0.845336</td>\n",
       "      <td>0.814642</td>\n",
       "      <td>0.777735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg19</th>\n",
       "      <td>0.718527</td>\n",
       "      <td>0.864856</td>\n",
       "      <td>0.893335</td>\n",
       "      <td>0.799589</td>\n",
       "      <td>0.725569</td>\n",
       "      <td>0.827410</td>\n",
       "      <td>0.896053</td>\n",
       "      <td>0.901706</td>\n",
       "      <td>0.869456</td>\n",
       "      <td>0.836490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854424</td>\n",
       "      <td>0.923908</td>\n",
       "      <td>0.882505</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.738555</td>\n",
       "      <td>0.837651</td>\n",
       "      <td>0.842445</td>\n",
       "      <td>0.846942</td>\n",
       "      <td>0.716542</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg20</th>\n",
       "      <td>0.726065</td>\n",
       "      <td>0.739855</td>\n",
       "      <td>0.730931</td>\n",
       "      <td>0.633451</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.613479</td>\n",
       "      <td>0.727434</td>\n",
       "      <td>0.741256</td>\n",
       "      <td>0.609592</td>\n",
       "      <td>0.783766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738543</td>\n",
       "      <td>0.731272</td>\n",
       "      <td>0.744225</td>\n",
       "      <td>0.680756</td>\n",
       "      <td>0.793542</td>\n",
       "      <td>0.770538</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.819108</td>\n",
       "      <td>0.619464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg21</th>\n",
       "      <td>0.758144</td>\n",
       "      <td>0.825605</td>\n",
       "      <td>0.787020</td>\n",
       "      <td>0.699156</td>\n",
       "      <td>0.647373</td>\n",
       "      <td>0.703398</td>\n",
       "      <td>0.797701</td>\n",
       "      <td>0.778331</td>\n",
       "      <td>0.726920</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774936</td>\n",
       "      <td>0.783839</td>\n",
       "      <td>0.796019</td>\n",
       "      <td>0.730846</td>\n",
       "      <td>0.766135</td>\n",
       "      <td>0.784825</td>\n",
       "      <td>0.683850</td>\n",
       "      <td>0.775103</td>\n",
       "      <td>0.782106</td>\n",
       "      <td>0.662805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg22</th>\n",
       "      <td>0.751737</td>\n",
       "      <td>0.879223</td>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.845902</td>\n",
       "      <td>0.812725</td>\n",
       "      <td>0.877349</td>\n",
       "      <td>0.856921</td>\n",
       "      <td>0.803879</td>\n",
       "      <td>0.913776</td>\n",
       "      <td>0.831408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752150</td>\n",
       "      <td>0.859862</td>\n",
       "      <td>0.794438</td>\n",
       "      <td>0.802346</td>\n",
       "      <td>0.618978</td>\n",
       "      <td>0.786501</td>\n",
       "      <td>0.903962</td>\n",
       "      <td>0.824573</td>\n",
       "      <td>0.667610</td>\n",
       "      <td>0.826002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg23</th>\n",
       "      <td>0.751428</td>\n",
       "      <td>0.830390</td>\n",
       "      <td>0.877688</td>\n",
       "      <td>0.750809</td>\n",
       "      <td>0.629580</td>\n",
       "      <td>0.730641</td>\n",
       "      <td>0.880444</td>\n",
       "      <td>0.874697</td>\n",
       "      <td>0.751133</td>\n",
       "      <td>0.818634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825047</td>\n",
       "      <td>0.816419</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>0.766843</td>\n",
       "      <td>0.827465</td>\n",
       "      <td>0.823776</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.822758</td>\n",
       "      <td>0.732928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg24</th>\n",
       "      <td>0.579870</td>\n",
       "      <td>0.714972</td>\n",
       "      <td>0.838228</td>\n",
       "      <td>0.610666</td>\n",
       "      <td>0.474873</td>\n",
       "      <td>0.603121</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0.844492</td>\n",
       "      <td>0.627706</td>\n",
       "      <td>0.684138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849384</td>\n",
       "      <td>0.777924</td>\n",
       "      <td>0.802417</td>\n",
       "      <td>0.689449</td>\n",
       "      <td>0.845660</td>\n",
       "      <td>0.754637</td>\n",
       "      <td>0.607578</td>\n",
       "      <td>0.707537</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.614872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg25</th>\n",
       "      <td>0.797617</td>\n",
       "      <td>0.763540</td>\n",
       "      <td>0.798193</td>\n",
       "      <td>0.720576</td>\n",
       "      <td>0.640593</td>\n",
       "      <td>0.684604</td>\n",
       "      <td>0.786578</td>\n",
       "      <td>0.784352</td>\n",
       "      <td>0.686468</td>\n",
       "      <td>0.799272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697828</td>\n",
       "      <td>0.789317</td>\n",
       "      <td>0.767211</td>\n",
       "      <td>0.793793</td>\n",
       "      <td>0.745524</td>\n",
       "      <td>0.851942</td>\n",
       "      <td>0.680410</td>\n",
       "      <td>0.718473</td>\n",
       "      <td>0.820696</td>\n",
       "      <td>0.707766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg26</th>\n",
       "      <td>0.774553</td>\n",
       "      <td>0.877734</td>\n",
       "      <td>0.876937</td>\n",
       "      <td>0.867823</td>\n",
       "      <td>0.816373</td>\n",
       "      <td>0.890950</td>\n",
       "      <td>0.912820</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.906076</td>\n",
       "      <td>0.863276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803441</td>\n",
       "      <td>0.877361</td>\n",
       "      <td>0.867357</td>\n",
       "      <td>0.825212</td>\n",
       "      <td>0.695625</td>\n",
       "      <td>0.827090</td>\n",
       "      <td>0.870759</td>\n",
       "      <td>0.842056</td>\n",
       "      <td>0.746464</td>\n",
       "      <td>0.890521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg27</th>\n",
       "      <td>0.640509</td>\n",
       "      <td>0.779976</td>\n",
       "      <td>0.887178</td>\n",
       "      <td>0.668846</td>\n",
       "      <td>0.550454</td>\n",
       "      <td>0.679372</td>\n",
       "      <td>0.851402</td>\n",
       "      <td>0.919958</td>\n",
       "      <td>0.706761</td>\n",
       "      <td>0.735535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889827</td>\n",
       "      <td>0.833695</td>\n",
       "      <td>0.894299</td>\n",
       "      <td>0.725362</td>\n",
       "      <td>0.840688</td>\n",
       "      <td>0.796218</td>\n",
       "      <td>0.670152</td>\n",
       "      <td>0.766155</td>\n",
       "      <td>0.755773</td>\n",
       "      <td>0.677182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg28</th>\n",
       "      <td>0.796556</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>0.860566</td>\n",
       "      <td>0.839364</td>\n",
       "      <td>0.789369</td>\n",
       "      <td>0.855970</td>\n",
       "      <td>0.877622</td>\n",
       "      <td>0.864526</td>\n",
       "      <td>0.868561</td>\n",
       "      <td>0.868550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802729</td>\n",
       "      <td>0.867293</td>\n",
       "      <td>0.855644</td>\n",
       "      <td>0.849269</td>\n",
       "      <td>0.712721</td>\n",
       "      <td>0.848079</td>\n",
       "      <td>0.841564</td>\n",
       "      <td>0.845314</td>\n",
       "      <td>0.778940</td>\n",
       "      <td>0.841742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg29</th>\n",
       "      <td>0.771169</td>\n",
       "      <td>0.897497</td>\n",
       "      <td>0.839561</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.794389</td>\n",
       "      <td>0.880300</td>\n",
       "      <td>0.882036</td>\n",
       "      <td>0.861183</td>\n",
       "      <td>0.882374</td>\n",
       "      <td>0.863817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802276</td>\n",
       "      <td>0.877170</td>\n",
       "      <td>0.861250</td>\n",
       "      <td>0.801056</td>\n",
       "      <td>0.691606</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0.857328</td>\n",
       "      <td>0.846798</td>\n",
       "      <td>0.733984</td>\n",
       "      <td>0.826017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg30</th>\n",
       "      <td>0.784463</td>\n",
       "      <td>0.824949</td>\n",
       "      <td>0.865821</td>\n",
       "      <td>0.789830</td>\n",
       "      <td>0.749132</td>\n",
       "      <td>0.803366</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.871640</td>\n",
       "      <td>0.790288</td>\n",
       "      <td>0.827434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789872</td>\n",
       "      <td>0.859318</td>\n",
       "      <td>0.862215</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.742272</td>\n",
       "      <td>0.882811</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.841188</td>\n",
       "      <td>0.800998</td>\n",
       "      <td>0.815256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg31</th>\n",
       "      <td>0.759729</td>\n",
       "      <td>0.702955</td>\n",
       "      <td>0.760868</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.600191</td>\n",
       "      <td>0.745525</td>\n",
       "      <td>0.756493</td>\n",
       "      <td>0.581374</td>\n",
       "      <td>0.740946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715082</td>\n",
       "      <td>0.720962</td>\n",
       "      <td>0.779283</td>\n",
       "      <td>0.700899</td>\n",
       "      <td>0.846161</td>\n",
       "      <td>0.813245</td>\n",
       "      <td>0.558968</td>\n",
       "      <td>0.697392</td>\n",
       "      <td>0.897261</td>\n",
       "      <td>0.637650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg32</th>\n",
       "      <td>0.709544</td>\n",
       "      <td>0.778566</td>\n",
       "      <td>0.730815</td>\n",
       "      <td>0.884512</td>\n",
       "      <td>0.885114</td>\n",
       "      <td>0.888262</td>\n",
       "      <td>0.772960</td>\n",
       "      <td>0.715281</td>\n",
       "      <td>0.915901</td>\n",
       "      <td>0.777615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641606</td>\n",
       "      <td>0.762678</td>\n",
       "      <td>0.681441</td>\n",
       "      <td>0.799337</td>\n",
       "      <td>0.481813</td>\n",
       "      <td>0.695533</td>\n",
       "      <td>0.926524</td>\n",
       "      <td>0.756421</td>\n",
       "      <td>0.578760</td>\n",
       "      <td>0.866761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg33</th>\n",
       "      <td>0.788735</td>\n",
       "      <td>0.886892</td>\n",
       "      <td>0.829387</td>\n",
       "      <td>0.803637</td>\n",
       "      <td>0.761073</td>\n",
       "      <td>0.825351</td>\n",
       "      <td>0.890855</td>\n",
       "      <td>0.849283</td>\n",
       "      <td>0.835138</td>\n",
       "      <td>0.861810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776133</td>\n",
       "      <td>0.858855</td>\n",
       "      <td>0.847262</td>\n",
       "      <td>0.800902</td>\n",
       "      <td>0.702492</td>\n",
       "      <td>0.832472</td>\n",
       "      <td>0.812178</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.754339</td>\n",
       "      <td>0.801604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg34</th>\n",
       "      <td>0.777609</td>\n",
       "      <td>0.859591</td>\n",
       "      <td>0.804547</td>\n",
       "      <td>0.791468</td>\n",
       "      <td>0.759491</td>\n",
       "      <td>0.818619</td>\n",
       "      <td>0.844979</td>\n",
       "      <td>0.799296</td>\n",
       "      <td>0.800207</td>\n",
       "      <td>0.872067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754982</td>\n",
       "      <td>0.797540</td>\n",
       "      <td>0.836076</td>\n",
       "      <td>0.744844</td>\n",
       "      <td>0.690239</td>\n",
       "      <td>0.778997</td>\n",
       "      <td>0.793903</td>\n",
       "      <td>0.820831</td>\n",
       "      <td>0.746077</td>\n",
       "      <td>0.784237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg35</th>\n",
       "      <td>0.778720</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>0.854948</td>\n",
       "      <td>0.725635</td>\n",
       "      <td>0.631223</td>\n",
       "      <td>0.709507</td>\n",
       "      <td>0.826536</td>\n",
       "      <td>0.859674</td>\n",
       "      <td>0.700997</td>\n",
       "      <td>0.804543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818678</td>\n",
       "      <td>0.819022</td>\n",
       "      <td>0.872169</td>\n",
       "      <td>0.798304</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.858489</td>\n",
       "      <td>0.680599</td>\n",
       "      <td>0.800975</td>\n",
       "      <td>0.848978</td>\n",
       "      <td>0.709637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg36</th>\n",
       "      <td>0.712832</td>\n",
       "      <td>0.724610</td>\n",
       "      <td>0.696642</td>\n",
       "      <td>0.857058</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>0.874608</td>\n",
       "      <td>0.754756</td>\n",
       "      <td>0.676078</td>\n",
       "      <td>0.858072</td>\n",
       "      <td>0.733005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560415</td>\n",
       "      <td>0.694266</td>\n",
       "      <td>0.650822</td>\n",
       "      <td>0.765401</td>\n",
       "      <td>0.428331</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.875675</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.563202</td>\n",
       "      <td>0.874184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg37</th>\n",
       "      <td>0.827496</td>\n",
       "      <td>0.828285</td>\n",
       "      <td>0.836743</td>\n",
       "      <td>0.881371</td>\n",
       "      <td>0.857207</td>\n",
       "      <td>0.883692</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.818405</td>\n",
       "      <td>0.851265</td>\n",
       "      <td>0.832779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.798705</td>\n",
       "      <td>0.815061</td>\n",
       "      <td>0.867633</td>\n",
       "      <td>0.617578</td>\n",
       "      <td>0.820505</td>\n",
       "      <td>0.857297</td>\n",
       "      <td>0.792067</td>\n",
       "      <td>0.735430</td>\n",
       "      <td>0.882126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg38</th>\n",
       "      <td>0.808133</td>\n",
       "      <td>0.807070</td>\n",
       "      <td>0.830983</td>\n",
       "      <td>0.885041</td>\n",
       "      <td>0.846116</td>\n",
       "      <td>0.882999</td>\n",
       "      <td>0.856266</td>\n",
       "      <td>0.805862</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.839557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688255</td>\n",
       "      <td>0.844888</td>\n",
       "      <td>0.784575</td>\n",
       "      <td>0.848784</td>\n",
       "      <td>0.618703</td>\n",
       "      <td>0.840059</td>\n",
       "      <td>0.882290</td>\n",
       "      <td>0.827234</td>\n",
       "      <td>0.735263</td>\n",
       "      <td>0.895625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg39</th>\n",
       "      <td>0.645571</td>\n",
       "      <td>0.674179</td>\n",
       "      <td>0.763903</td>\n",
       "      <td>0.555721</td>\n",
       "      <td>0.424030</td>\n",
       "      <td>0.526340</td>\n",
       "      <td>0.715230</td>\n",
       "      <td>0.758950</td>\n",
       "      <td>0.525305</td>\n",
       "      <td>0.686470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776618</td>\n",
       "      <td>0.696466</td>\n",
       "      <td>0.761866</td>\n",
       "      <td>0.632136</td>\n",
       "      <td>0.914288</td>\n",
       "      <td>0.759673</td>\n",
       "      <td>0.492106</td>\n",
       "      <td>0.676281</td>\n",
       "      <td>0.832889</td>\n",
       "      <td>0.554879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg40</th>\n",
       "      <td>0.595581</td>\n",
       "      <td>0.786296</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.651419</td>\n",
       "      <td>0.558676</td>\n",
       "      <td>0.682966</td>\n",
       "      <td>0.819770</td>\n",
       "      <td>0.886983</td>\n",
       "      <td>0.709420</td>\n",
       "      <td>0.721193</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819632</td>\n",
       "      <td>0.855540</td>\n",
       "      <td>0.708335</td>\n",
       "      <td>0.821491</td>\n",
       "      <td>0.743514</td>\n",
       "      <td>0.684938</td>\n",
       "      <td>0.765045</td>\n",
       "      <td>0.699657</td>\n",
       "      <td>0.673071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg41</th>\n",
       "      <td>0.727062</td>\n",
       "      <td>0.838019</td>\n",
       "      <td>0.853956</td>\n",
       "      <td>0.765239</td>\n",
       "      <td>0.713357</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.882035</td>\n",
       "      <td>0.877812</td>\n",
       "      <td>0.842603</td>\n",
       "      <td>0.824319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851388</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.733973</td>\n",
       "      <td>0.848252</td>\n",
       "      <td>0.803683</td>\n",
       "      <td>0.802333</td>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.776179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg42</th>\n",
       "      <td>0.739767</td>\n",
       "      <td>0.844386</td>\n",
       "      <td>0.892806</td>\n",
       "      <td>0.740954</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>0.749079</td>\n",
       "      <td>0.888098</td>\n",
       "      <td>0.921650</td>\n",
       "      <td>0.771504</td>\n",
       "      <td>0.787452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855540</td>\n",
       "      <td>0.851388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757608</td>\n",
       "      <td>0.791321</td>\n",
       "      <td>0.826708</td>\n",
       "      <td>0.731219</td>\n",
       "      <td>0.807620</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0.745615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg43</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.793433</td>\n",
       "      <td>0.842076</td>\n",
       "      <td>0.837066</td>\n",
       "      <td>0.779774</td>\n",
       "      <td>0.802206</td>\n",
       "      <td>0.800782</td>\n",
       "      <td>0.803384</td>\n",
       "      <td>0.791568</td>\n",
       "      <td>0.809392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708335</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.757608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661225</td>\n",
       "      <td>0.841122</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.772842</td>\n",
       "      <td>0.753669</td>\n",
       "      <td>0.846022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg44</th>\n",
       "      <td>0.640529</td>\n",
       "      <td>0.697064</td>\n",
       "      <td>0.792093</td>\n",
       "      <td>0.571896</td>\n",
       "      <td>0.441549</td>\n",
       "      <td>0.548617</td>\n",
       "      <td>0.737620</td>\n",
       "      <td>0.807225</td>\n",
       "      <td>0.560244</td>\n",
       "      <td>0.700110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821491</td>\n",
       "      <td>0.733973</td>\n",
       "      <td>0.791321</td>\n",
       "      <td>0.661225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772786</td>\n",
       "      <td>0.525131</td>\n",
       "      <td>0.699193</td>\n",
       "      <td>0.827265</td>\n",
       "      <td>0.569875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg45</th>\n",
       "      <td>0.816284</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.873014</td>\n",
       "      <td>0.773814</td>\n",
       "      <td>0.687553</td>\n",
       "      <td>0.744157</td>\n",
       "      <td>0.851712</td>\n",
       "      <td>0.841130</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.816099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743514</td>\n",
       "      <td>0.848252</td>\n",
       "      <td>0.826708</td>\n",
       "      <td>0.841122</td>\n",
       "      <td>0.772786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733053</td>\n",
       "      <td>0.802835</td>\n",
       "      <td>0.848760</td>\n",
       "      <td>0.770287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg46</th>\n",
       "      <td>0.732457</td>\n",
       "      <td>0.838689</td>\n",
       "      <td>0.755374</td>\n",
       "      <td>0.872571</td>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.922309</td>\n",
       "      <td>0.791929</td>\n",
       "      <td>0.745567</td>\n",
       "      <td>0.926564</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684938</td>\n",
       "      <td>0.803683</td>\n",
       "      <td>0.731219</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.525131</td>\n",
       "      <td>0.733053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>0.608595</td>\n",
       "      <td>0.880879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg47</th>\n",
       "      <td>0.741159</td>\n",
       "      <td>0.825832</td>\n",
       "      <td>0.818414</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.763686</td>\n",
       "      <td>0.827901</td>\n",
       "      <td>0.815928</td>\n",
       "      <td>0.803406</td>\n",
       "      <td>0.808728</td>\n",
       "      <td>0.811481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765045</td>\n",
       "      <td>0.802333</td>\n",
       "      <td>0.807620</td>\n",
       "      <td>0.772842</td>\n",
       "      <td>0.699193</td>\n",
       "      <td>0.802835</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727873</td>\n",
       "      <td>0.784302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg48</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.724584</td>\n",
       "      <td>0.788662</td>\n",
       "      <td>0.692183</td>\n",
       "      <td>0.594812</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.772829</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0.629212</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699657</td>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0.753669</td>\n",
       "      <td>0.827265</td>\n",
       "      <td>0.848760</td>\n",
       "      <td>0.608595</td>\n",
       "      <td>0.727873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg49</th>\n",
       "      <td>0.769398</td>\n",
       "      <td>0.787544</td>\n",
       "      <td>0.782689</td>\n",
       "      <td>0.878742</td>\n",
       "      <td>0.862225</td>\n",
       "      <td>0.875096</td>\n",
       "      <td>0.811338</td>\n",
       "      <td>0.766636</td>\n",
       "      <td>0.851471</td>\n",
       "      <td>0.821853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673071</td>\n",
       "      <td>0.776179</td>\n",
       "      <td>0.745615</td>\n",
       "      <td>0.846022</td>\n",
       "      <td>0.569875</td>\n",
       "      <td>0.770287</td>\n",
       "      <td>0.880879</td>\n",
       "      <td>0.784302</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  is_iceberg4  \\\n",
       "is_iceberg0      1.000000     0.770224     0.748447     0.812716     0.761829   \n",
       "is_iceberg1      0.770224     1.000000     0.807722     0.811476     0.781088   \n",
       "is_iceberg2      0.748447     0.807722     1.000000     0.783185     0.672175   \n",
       "is_iceberg3      0.812716     0.811476     0.783185     1.000000     0.854853   \n",
       "is_iceberg4      0.761829     0.781088     0.672175     0.854853     1.000000   \n",
       "is_iceberg5      0.751923     0.854919     0.767648     0.887363     0.901550   \n",
       "is_iceberg6      0.771430     0.861634     0.895445     0.820160     0.725504   \n",
       "is_iceberg7      0.707299     0.824799     0.920282     0.746989     0.655364   \n",
       "is_iceberg8      0.733186     0.853850     0.789848     0.868094     0.871098   \n",
       "is_iceberg9      0.849820     0.855107     0.795803     0.835872     0.796065   \n",
       "is_iceberg10     0.784080     0.824308     0.911953     0.813061     0.711283   \n",
       "is_iceberg11     0.795289     0.858240     0.884824     0.803229     0.720384   \n",
       "is_iceberg12     0.720621     0.787275     0.840585     0.836708     0.772722   \n",
       "is_iceberg13     0.810199     0.872938     0.863438     0.875431     0.832877   \n",
       "is_iceberg14     0.698230     0.845107     0.844882     0.802195     0.741242   \n",
       "is_iceberg15     0.741699     0.861851     0.763364     0.866889     0.894971   \n",
       "is_iceberg16     0.678974     0.805644     0.866538     0.710251     0.617672   \n",
       "is_iceberg17     0.782807     0.864550     0.830328     0.846135     0.817049   \n",
       "is_iceberg18     0.787730     0.849181     0.901977     0.785391     0.707005   \n",
       "is_iceberg19     0.718527     0.864856     0.893335     0.799589     0.725569   \n",
       "is_iceberg20     0.726065     0.739855     0.730931     0.633451     0.555836   \n",
       "is_iceberg21     0.758144     0.825605     0.787020     0.699156     0.647373   \n",
       "is_iceberg22     0.751737     0.879223     0.820007     0.845902     0.812725   \n",
       "is_iceberg23     0.751428     0.830390     0.877688     0.750809     0.629580   \n",
       "is_iceberg24     0.579870     0.714972     0.838228     0.610666     0.474873   \n",
       "is_iceberg25     0.797617     0.763540     0.798193     0.720576     0.640593   \n",
       "is_iceberg26     0.774553     0.877734     0.876937     0.867823     0.816373   \n",
       "is_iceberg27     0.640509     0.779976     0.887178     0.668846     0.550454   \n",
       "is_iceberg28     0.796556     0.877602     0.860566     0.839364     0.789369   \n",
       "is_iceberg29     0.771169     0.897497     0.839561     0.835113     0.794389   \n",
       "is_iceberg30     0.784463     0.824949     0.865821     0.789830     0.749132   \n",
       "is_iceberg31     0.759729     0.702955     0.760868     0.647532     0.542944   \n",
       "is_iceberg32     0.709544     0.778566     0.730815     0.884512     0.885114   \n",
       "is_iceberg33     0.788735     0.886892     0.829387     0.803637     0.761073   \n",
       "is_iceberg34     0.777609     0.859591     0.804547     0.791468     0.759491   \n",
       "is_iceberg35     0.778720     0.818945     0.854948     0.725635     0.631223   \n",
       "is_iceberg36     0.712832     0.724610     0.696642     0.857058     0.877061   \n",
       "is_iceberg37     0.827496     0.828285     0.836743     0.881371     0.857207   \n",
       "is_iceberg38     0.808133     0.807070     0.830983     0.885041     0.846116   \n",
       "is_iceberg39     0.645571     0.674179     0.763903     0.555721     0.424030   \n",
       "is_iceberg40     0.595581     0.786296     0.846167     0.651419     0.558676   \n",
       "is_iceberg41     0.727062     0.838019     0.853956     0.765239     0.713357   \n",
       "is_iceberg42     0.739767     0.844386     0.892806     0.740954     0.649981   \n",
       "is_iceberg43     0.796670     0.793433     0.842076     0.837066     0.779774   \n",
       "is_iceberg44     0.640529     0.697064     0.792093     0.571896     0.441549   \n",
       "is_iceberg45     0.816284     0.785637     0.873014     0.773814     0.687553   \n",
       "is_iceberg46     0.732457     0.838689     0.755374     0.872571     0.903607   \n",
       "is_iceberg47     0.741159     0.825832     0.818414     0.774974     0.763686   \n",
       "is_iceberg48     0.823333     0.724584     0.788662     0.692183     0.594812   \n",
       "is_iceberg49     0.769398     0.787544     0.782689     0.878742     0.862225   \n",
       "\n",
       "              is_iceberg5  is_iceberg6  is_iceberg7  is_iceberg8  is_iceberg9  \\\n",
       "is_iceberg0      0.751923     0.771430     0.707299     0.733186     0.849820   \n",
       "is_iceberg1      0.854919     0.861634     0.824799     0.853850     0.855107   \n",
       "is_iceberg2      0.767648     0.895445     0.920282     0.789848     0.795803   \n",
       "is_iceberg3      0.887363     0.820160     0.746989     0.868094     0.835872   \n",
       "is_iceberg4      0.901550     0.725504     0.655364     0.871098     0.796065   \n",
       "is_iceberg5      1.000000     0.819581     0.762304     0.904431     0.819042   \n",
       "is_iceberg6      0.819581     1.000000     0.906023     0.839866     0.827177   \n",
       "is_iceberg7      0.762304     0.906023     1.000000     0.775508     0.781362   \n",
       "is_iceberg8      0.904431     0.839866     0.775508     1.000000     0.820538   \n",
       "is_iceberg9      0.819042     0.827177     0.781362     0.820538     1.000000   \n",
       "is_iceberg10     0.798542     0.903903     0.900968     0.803478     0.826071   \n",
       "is_iceberg11     0.804941     0.881563     0.876620     0.794848     0.838434   \n",
       "is_iceberg12     0.839856     0.843995     0.835125     0.838059     0.804809   \n",
       "is_iceberg13     0.888351     0.882262     0.837610     0.872536     0.857105   \n",
       "is_iceberg14     0.825850     0.856901     0.828385     0.895478     0.800760   \n",
       "is_iceberg15     0.925519     0.827613     0.760706     0.930914     0.818929   \n",
       "is_iceberg16     0.725501     0.889423     0.892696     0.733423     0.759254   \n",
       "is_iceberg17     0.868934     0.856600     0.829383     0.868041     0.853800   \n",
       "is_iceberg18     0.794368     0.911134     0.894044     0.803979     0.816891   \n",
       "is_iceberg19     0.827410     0.896053     0.901706     0.869456     0.836490   \n",
       "is_iceberg20     0.613479     0.727434     0.741256     0.609592     0.783766   \n",
       "is_iceberg21     0.703398     0.797701     0.778331     0.726920     0.831207   \n",
       "is_iceberg22     0.877349     0.856921     0.803879     0.913776     0.831408   \n",
       "is_iceberg23     0.730641     0.880444     0.874697     0.751133     0.818634   \n",
       "is_iceberg24     0.603121     0.776256     0.844492     0.627706     0.684138   \n",
       "is_iceberg25     0.684604     0.786578     0.784352     0.686468     0.799272   \n",
       "is_iceberg26     0.890950     0.912820     0.877264     0.906076     0.863276   \n",
       "is_iceberg27     0.679372     0.851402     0.919958     0.706761     0.735535   \n",
       "is_iceberg28     0.855970     0.877622     0.864526     0.868561     0.868550   \n",
       "is_iceberg29     0.880300     0.882036     0.861183     0.882374     0.863817   \n",
       "is_iceberg30     0.803366     0.854294     0.871640     0.790288     0.827434   \n",
       "is_iceberg31     0.600191     0.745525     0.756493     0.581374     0.740946   \n",
       "is_iceberg32     0.888262     0.772960     0.715281     0.915901     0.777615   \n",
       "is_iceberg33     0.825351     0.890855     0.849283     0.835138     0.861810   \n",
       "is_iceberg34     0.818619     0.844979     0.799296     0.800207     0.872067   \n",
       "is_iceberg35     0.709507     0.826536     0.859674     0.700997     0.804543   \n",
       "is_iceberg36     0.874608     0.754756     0.676078     0.858072     0.733005   \n",
       "is_iceberg37     0.883692     0.857868     0.818405     0.851265     0.832779   \n",
       "is_iceberg38     0.882999     0.856266     0.805862     0.884103     0.839557   \n",
       "is_iceberg39     0.526340     0.715230     0.758950     0.525305     0.686470   \n",
       "is_iceberg40     0.682966     0.819770     0.886983     0.709420     0.721193   \n",
       "is_iceberg41     0.793555     0.882035     0.877812     0.842603     0.824319   \n",
       "is_iceberg42     0.749079     0.888098     0.921650     0.771504     0.787452   \n",
       "is_iceberg43     0.802206     0.800782     0.803384     0.791568     0.809392   \n",
       "is_iceberg44     0.548617     0.737620     0.807225     0.560244     0.700110   \n",
       "is_iceberg45     0.744157     0.851712     0.841130     0.759359     0.816099   \n",
       "is_iceberg46     0.922309     0.791929     0.745567     0.926564     0.820056   \n",
       "is_iceberg47     0.827901     0.815928     0.803406     0.808728     0.811481   \n",
       "is_iceberg48     0.651310     0.772829     0.776256     0.629212     0.794286   \n",
       "is_iceberg49     0.875096     0.811338     0.766636     0.851471     0.821853   \n",
       "\n",
       "                  ...       is_iceberg40  is_iceberg41  is_iceberg42  \\\n",
       "is_iceberg0       ...           0.595581      0.727062      0.739767   \n",
       "is_iceberg1       ...           0.786296      0.838019      0.844386   \n",
       "is_iceberg2       ...           0.846167      0.853956      0.892806   \n",
       "is_iceberg3       ...           0.651419      0.765239      0.740954   \n",
       "is_iceberg4       ...           0.558676      0.713357      0.649981   \n",
       "is_iceberg5       ...           0.682966      0.793555      0.749079   \n",
       "is_iceberg6       ...           0.819770      0.882035      0.888098   \n",
       "is_iceberg7       ...           0.886983      0.877812      0.921650   \n",
       "is_iceberg8       ...           0.709420      0.842603      0.771504   \n",
       "is_iceberg9       ...           0.721193      0.824319      0.787452   \n",
       "is_iceberg10      ...           0.800318      0.860232      0.882989   \n",
       "is_iceberg11      ...           0.809062      0.842180      0.905705   \n",
       "is_iceberg12      ...           0.770710      0.842825      0.788455   \n",
       "is_iceberg13      ...           0.743886      0.833323      0.837923   \n",
       "is_iceberg14      ...           0.798101      0.870973      0.809283   \n",
       "is_iceberg15      ...           0.689848      0.811980      0.748931   \n",
       "is_iceberg16      ...           0.843514      0.833940      0.870438   \n",
       "is_iceberg17      ...           0.767218      0.877005      0.810077   \n",
       "is_iceberg18      ...           0.823860      0.877732      0.898949   \n",
       "is_iceberg19      ...           0.854424      0.923908      0.882505   \n",
       "is_iceberg20      ...           0.738543      0.731272      0.744225   \n",
       "is_iceberg21      ...           0.774936      0.783839      0.796019   \n",
       "is_iceberg22      ...           0.752150      0.859862      0.794438   \n",
       "is_iceberg23      ...           0.825047      0.816419      0.869850   \n",
       "is_iceberg24      ...           0.849384      0.777924      0.802417   \n",
       "is_iceberg25      ...           0.697828      0.789317      0.767211   \n",
       "is_iceberg26      ...           0.803441      0.877361      0.867357   \n",
       "is_iceberg27      ...           0.889827      0.833695      0.894299   \n",
       "is_iceberg28      ...           0.802729      0.867293      0.855644   \n",
       "is_iceberg29      ...           0.802276      0.877170      0.861250   \n",
       "is_iceberg30      ...           0.789872      0.859318      0.862215   \n",
       "is_iceberg31      ...           0.715082      0.720962      0.779283   \n",
       "is_iceberg32      ...           0.641606      0.762678      0.681441   \n",
       "is_iceberg33      ...           0.776133      0.858855      0.847262   \n",
       "is_iceberg34      ...           0.754982      0.797540      0.836076   \n",
       "is_iceberg35      ...           0.818678      0.819022      0.872169   \n",
       "is_iceberg36      ...           0.560415      0.694266      0.650822   \n",
       "is_iceberg37      ...           0.694102      0.798705      0.815061   \n",
       "is_iceberg38      ...           0.688255      0.844888      0.784575   \n",
       "is_iceberg39      ...           0.776618      0.696466      0.761866   \n",
       "is_iceberg40      ...           1.000000      0.819632      0.855540   \n",
       "is_iceberg41      ...           0.819632      1.000000      0.851388   \n",
       "is_iceberg42      ...           0.855540      0.851388      1.000000   \n",
       "is_iceberg43      ...           0.708335      0.797718      0.757608   \n",
       "is_iceberg44      ...           0.821491      0.733973      0.791321   \n",
       "is_iceberg45      ...           0.743514      0.848252      0.826708   \n",
       "is_iceberg46      ...           0.684938      0.803683      0.731219   \n",
       "is_iceberg47      ...           0.765045      0.802333      0.807620   \n",
       "is_iceberg48      ...           0.699657      0.743434      0.779798   \n",
       "is_iceberg49      ...           0.673071      0.776179      0.745615   \n",
       "\n",
       "              is_iceberg43  is_iceberg44  is_iceberg45  is_iceberg46  \\\n",
       "is_iceberg0       0.796670      0.640529      0.816284      0.732457   \n",
       "is_iceberg1       0.793433      0.697064      0.785637      0.838689   \n",
       "is_iceberg2       0.842076      0.792093      0.873014      0.755374   \n",
       "is_iceberg3       0.837066      0.571896      0.773814      0.872571   \n",
       "is_iceberg4       0.779774      0.441549      0.687553      0.903607   \n",
       "is_iceberg5       0.802206      0.548617      0.744157      0.922309   \n",
       "is_iceberg6       0.800782      0.737620      0.851712      0.791929   \n",
       "is_iceberg7       0.803384      0.807225      0.841130      0.745567   \n",
       "is_iceberg8       0.791568      0.560244      0.759359      0.926564   \n",
       "is_iceberg9       0.809392      0.700110      0.816099      0.820056   \n",
       "is_iceberg10      0.840762      0.754296      0.865623      0.779059   \n",
       "is_iceberg11      0.802211      0.767874      0.862941      0.777541   \n",
       "is_iceberg12      0.832198      0.659062      0.811224      0.844225   \n",
       "is_iceberg13      0.861489      0.647365      0.836509      0.865610   \n",
       "is_iceberg14      0.777105      0.651234      0.784154      0.866822   \n",
       "is_iceberg15      0.790480      0.534577      0.742209      0.940440   \n",
       "is_iceberg16      0.760311      0.781933      0.803833      0.685734   \n",
       "is_iceberg17      0.862770      0.678607      0.839649      0.868158   \n",
       "is_iceberg18      0.820653      0.784234      0.901708      0.771894   \n",
       "is_iceberg19      0.807619      0.738555      0.837651      0.842445   \n",
       "is_iceberg20      0.680756      0.793542      0.770538      0.595703   \n",
       "is_iceberg21      0.730846      0.766135      0.784825      0.683850   \n",
       "is_iceberg22      0.802346      0.618978      0.786501      0.903962   \n",
       "is_iceberg23      0.766843      0.827465      0.823776      0.696500   \n",
       "is_iceberg24      0.689449      0.845660      0.754637      0.607578   \n",
       "is_iceberg25      0.793793      0.745524      0.851942      0.680410   \n",
       "is_iceberg26      0.825212      0.695625      0.827090      0.870759   \n",
       "is_iceberg27      0.725362      0.840688      0.796218      0.670152   \n",
       "is_iceberg28      0.849269      0.712721      0.848079      0.841564   \n",
       "is_iceberg29      0.801056      0.691606      0.816011      0.857328   \n",
       "is_iceberg30      0.851800      0.742272      0.882811      0.783333   \n",
       "is_iceberg31      0.700899      0.846161      0.813245      0.558968   \n",
       "is_iceberg32      0.799337      0.481813      0.695533      0.926524   \n",
       "is_iceberg33      0.800902      0.702492      0.832472      0.812178   \n",
       "is_iceberg34      0.744844      0.690239      0.778997      0.793903   \n",
       "is_iceberg35      0.798304      0.844452      0.858489      0.680599   \n",
       "is_iceberg36      0.765401      0.428331      0.674610      0.875675   \n",
       "is_iceberg37      0.867633      0.617578      0.820505      0.857297   \n",
       "is_iceberg38      0.848784      0.618703      0.840059      0.882290   \n",
       "is_iceberg39      0.632136      0.914288      0.759673      0.492106   \n",
       "is_iceberg40      0.708335      0.821491      0.743514      0.684938   \n",
       "is_iceberg41      0.797718      0.733973      0.848252      0.803683   \n",
       "is_iceberg42      0.757608      0.791321      0.826708      0.731219   \n",
       "is_iceberg43      1.000000      0.661225      0.841122      0.814867   \n",
       "is_iceberg44      0.661225      1.000000      0.772786      0.525131   \n",
       "is_iceberg45      0.841122      0.772786      1.000000      0.733053   \n",
       "is_iceberg46      0.814867      0.525131      0.733053      1.000000   \n",
       "is_iceberg47      0.772842      0.699193      0.802835      0.816464   \n",
       "is_iceberg48      0.753669      0.827265      0.848760      0.608595   \n",
       "is_iceberg49      0.846022      0.569875      0.770287      0.880879   \n",
       "\n",
       "              is_iceberg47  is_iceberg48  is_iceberg49  \n",
       "is_iceberg0       0.741159      0.823333      0.769398  \n",
       "is_iceberg1       0.825832      0.724584      0.787544  \n",
       "is_iceberg2       0.818414      0.788662      0.782689  \n",
       "is_iceberg3       0.774974      0.692183      0.878742  \n",
       "is_iceberg4       0.763686      0.594812      0.862225  \n",
       "is_iceberg5       0.827901      0.651310      0.875096  \n",
       "is_iceberg6       0.815928      0.772829      0.811338  \n",
       "is_iceberg7       0.803406      0.776256      0.766636  \n",
       "is_iceberg8       0.808728      0.629212      0.851471  \n",
       "is_iceberg9       0.811481      0.794286      0.821853  \n",
       "is_iceberg10      0.811007      0.792667      0.819784  \n",
       "is_iceberg11      0.839040      0.796452      0.777816  \n",
       "is_iceberg12      0.818025      0.707617      0.886423  \n",
       "is_iceberg13      0.824239      0.745863      0.867494  \n",
       "is_iceberg14      0.826653      0.648304      0.779728  \n",
       "is_iceberg15      0.829413      0.614827      0.865512  \n",
       "is_iceberg16      0.750672      0.778434      0.744502  \n",
       "is_iceberg17      0.829897      0.755056      0.866510  \n",
       "is_iceberg18      0.845336      0.814642      0.777735  \n",
       "is_iceberg19      0.846942      0.716542      0.802909  \n",
       "is_iceberg20      0.730435      0.819108      0.619464  \n",
       "is_iceberg21      0.775103      0.782106      0.662805  \n",
       "is_iceberg22      0.824573      0.667610      0.826002  \n",
       "is_iceberg23      0.766744      0.822758      0.732928  \n",
       "is_iceberg24      0.707537      0.714500      0.614872  \n",
       "is_iceberg25      0.718473      0.820696      0.707766  \n",
       "is_iceberg26      0.842056      0.746464      0.890521  \n",
       "is_iceberg27      0.766155      0.755773      0.677182  \n",
       "is_iceberg28      0.845314      0.778940      0.841742  \n",
       "is_iceberg29      0.846798      0.733984      0.826017  \n",
       "is_iceberg30      0.841188      0.800998      0.815256  \n",
       "is_iceberg31      0.697392      0.897261      0.637650  \n",
       "is_iceberg32      0.756421      0.578760      0.866761  \n",
       "is_iceberg33      0.844103      0.754339      0.801604  \n",
       "is_iceberg34      0.820831      0.746077      0.784237  \n",
       "is_iceberg35      0.800975      0.848978      0.709637  \n",
       "is_iceberg36      0.729593      0.563202      0.874184  \n",
       "is_iceberg37      0.792067      0.735430      0.882126  \n",
       "is_iceberg38      0.827234      0.735263      0.895625  \n",
       "is_iceberg39      0.676281      0.832889      0.554879  \n",
       "is_iceberg40      0.765045      0.699657      0.673071  \n",
       "is_iceberg41      0.802333      0.743434      0.776179  \n",
       "is_iceberg42      0.807620      0.779798      0.745615  \n",
       "is_iceberg43      0.772842      0.753669      0.846022  \n",
       "is_iceberg44      0.699193      0.827265      0.569875  \n",
       "is_iceberg45      0.802835      0.848760      0.770287  \n",
       "is_iceberg46      0.816464      0.608595      0.880879  \n",
       "is_iceberg47      1.000000      0.727873      0.784302  \n",
       "is_iceberg48      0.727873      1.000000      0.690808  \n",
       "is_iceberg49      0.784302      0.690808      1.000000  \n",
       "\n",
       "[50 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()\n",
    "# [i for i in os.listdir('vgg_models') if 'r3' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s494ms\n",
      "[=================== 132/132 ================>]  Step: 160ms | Tot: 27s661ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s644ms\n",
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s598ms\n",
      "[=================== 132/132 ================>]  Step: 161ms | Tot: 27s668ms\n"
     ]
    }
   ],
   "source": [
    "#result_hist\n",
    "\n",
    "temp11 = pd.DataFrame()\n",
    "\n",
    "for i in range(5):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load('resnet34_acc%d.pth'%i))\n",
    "    net.cuda()\n",
    "\n",
    "    test = pd.read_json(BASE_dir + 'test.json')\n",
    "    test_X = raw_to_numpy(test)\n",
    "    test_X.shape \n",
    "    fake_label = np.zeros(len(test_X))\n",
    "\n",
    "    test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' %i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11.median(axis=1)\n",
    "sub.shape\n",
    "sub.to_csv('submission23.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, 0:6].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, 0:6].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, 0:6].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg5']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg5']\n",
    "sub.shape\n",
    "sub.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 162ms | Tot: 27s704ms\n"
     ]
    }
   ],
   "source": [
    "net = resnet.resnet34(num_classes=2)\n",
    "net.load_state_dict(torch.load('save_resnet34_acc117.pth'))\n",
    "net.cuda()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "prob = [] \n",
    "net.eval()\n",
    "for k, (val_x, val_y) in enumerate(test_loader):\n",
    "    if use_cuda:\n",
    "        val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    x = Variable(val_x)\n",
    "    y = Variable(val_y)\n",
    "    out = net(x)\n",
    "    #prevent overflow\n",
    "    temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "    ans= temp[:,1]/(temp.sum(axis=1))\n",
    "    prob.append(ans)\n",
    "    #print(out.size())\n",
    "    progress_bar(k, len(test_loader))\n",
    "msg = 'is_iceberg%d' %5\n",
    "temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "      <th>is_iceberg4</th>\n",
       "      <th>is_iceberg5</th>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg1</th>\n",
       "      <td>0.852644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.816630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg2</th>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.785453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg3</th>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.771766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.559032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg4</th>\n",
       "      <td>0.883101</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.774018</td>\n",
       "      <td>0.685649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>0.827514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg5</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_max</th>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.940914</td>\n",
       "      <td>0.685683</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.655435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_min</th>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>0.738630</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.920097</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.583326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.895989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_median</th>\n",
       "      <td>0.942663</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.792055</td>\n",
       "      <td>0.875356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.897011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg_base</th>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.784324</td>\n",
       "      <td>0.556919</td>\n",
       "      <td>0.826391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653849</td>\n",
       "      <td>0.895245</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.816630</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.559032</td>\n",
       "      <td>0.827514</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.655435</td>\n",
       "      <td>0.895989</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_iceberg0  is_iceberg1  is_iceberg2  is_iceberg3  \\\n",
       "is_iceberg0           1.000000     0.852644     0.822586     0.648968   \n",
       "is_iceberg1           0.852644     1.000000     0.905401     0.754710   \n",
       "is_iceberg2           0.822586     0.905401     1.000000     0.771766   \n",
       "is_iceberg3           0.648968     0.754710     0.771766     1.000000   \n",
       "is_iceberg4           0.883101     0.833295     0.774018     0.685649   \n",
       "is_iceberg5           0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg_max        0.682861     0.821258     0.847868     0.940914   \n",
       "is_iceberg_min        0.922862     0.777728     0.738630     0.592617   \n",
       "is_iceberg_median     0.942663     0.956190     0.918857     0.749656   \n",
       "is_iceberg_base       0.905277     0.815734     0.784324     0.556919   \n",
       "is_iceberg            0.905900     0.816630     0.785453     0.559032   \n",
       "\n",
       "                   is_iceberg4  is_iceberg5  is_iceberg_max  is_iceberg_min  \\\n",
       "is_iceberg0           0.883101     0.905277        0.682861        0.922862   \n",
       "is_iceberg1           0.833295     0.815734        0.821258        0.777728   \n",
       "is_iceberg2           0.774018     0.784324        0.847868        0.738630   \n",
       "is_iceberg3           0.685649     0.556919        0.940914        0.592617   \n",
       "is_iceberg4           1.000000     0.826391        0.685683        0.920097   \n",
       "is_iceberg5           0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg_max        0.685683     0.653849        1.000000        0.583326   \n",
       "is_iceberg_min        0.920097     0.895245        0.583326        1.000000   \n",
       "is_iceberg_median     0.909537     0.896220        0.792055        0.875356   \n",
       "is_iceberg_base       0.826391     1.000000        0.653849        0.895245   \n",
       "is_iceberg            0.827514     0.999683        0.655435        0.895989   \n",
       "\n",
       "                   is_iceberg_median  is_iceberg_base  is_iceberg  \n",
       "is_iceberg0                 0.942663         0.905277    0.905900  \n",
       "is_iceberg1                 0.956190         0.815734    0.816630  \n",
       "is_iceberg2                 0.918857         0.784324    0.785453  \n",
       "is_iceberg3                 0.749656         0.556919    0.559032  \n",
       "is_iceberg4                 0.909537         0.826391    0.827514  \n",
       "is_iceberg5                 0.896220         1.000000    0.999683  \n",
       "is_iceberg_max              0.792055         0.653849    0.655435  \n",
       "is_iceberg_min              0.875356         0.895245    0.895989  \n",
       "is_iceberg_median           1.000000         0.896220    0.897011  \n",
       "is_iceberg_base             0.896220         1.000000    0.999683  \n",
       "is_iceberg                  0.897011         0.999683    1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp11.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  5,  6,  7,  8,  9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22,\n",
      "       23, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42,\n",
      "       44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62,\n",
      "       63, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 84, 85,\n",
      "       86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99]), array([ 0,  3,  4, 11, 14, 17, 24, 29, 40, 43, 48, 59, 64, 66, 70, 79, 82,\n",
      "       83, 93, 97]))\n"
     ]
    }
   ],
   "source": [
    "seed= np.random.RandomState(67)\n",
    "spliter = KFold(n_splits=5,shuffle =True,random_state = seed)\n",
    "for i in spliter.split(list(range(100))):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s222ms Step: 200ms | Tot: 2s492ms  Step: 199ms | Tot: 2s692ms  Step: 200ms | Tot: 7s692ms  Step: 199ms | Tot: 13s642ms  Step: 200ms | Tot: 14s841ms  Step: 200ms | Tot: 24s62ms\n",
      "[=================== 132/132 ================>]  Step: 152ms | Tot: 26s199ms Step: 199ms | Tot: 1s425ms  Step: 200ms | Tot: 1s626ms  Step: 200ms | Tot: 8s236ms  Step: 200ms | Tot: 12s639ms  Step: 200ms | Tot: 14s844ms  Step: 199ms | Tot: 16s449ms  Step: 199ms | Tot: 17s647ms  Step: 199ms | Tot: 20s857ms  Step: 200ms | Tot: 23s61ms  Step: 199ms | Tot: 25s648ms  Step: 201ms | Tot: 25s850ms\n",
      "[=================== 132/132 ================>]  Step: 151ms | Tot: 26s141ms Step: 199ms | Tot: 6s583ms  Step: 199ms | Tot: 6s982ms 41/132   Step: 200ms | Tot: 9s576ms  Step: 200ms | Tot: 11s185ms\n",
      "[=================== 132/132 ================>]  Step: 155ms | Tot: 26s219ms Step: 200ms | Tot: 3s394ms  Step: 200ms | Tot: 4s393ms 52/132   Step: 200ms | Tot: 10s614ms  Step: 200ms | Tot: 10s814ms  Step: 200ms | Tot: 12s219ms  Step: 200ms | Tot: 14s624ms  Step: 200ms | Tot: 15s829ms  Step: 200ms | Tot: 18s640ms 99/132   Step: 200ms | Tot: 22s651ms  Step: 200ms | Tot: 23s456ms\n"
     ]
    }
   ],
   "source": [
    "temp11 = pd.DataFrame()\n",
    "\n",
    "test = pd.read_json(BASE_dir + 'test.json')\n",
    "test_X = raw_to_numpy(test)\n",
    "test_X.shape \n",
    "fake_label = np.zeros(len(test_X))\n",
    "\n",
    "test_dataset = iceberg_dataset(data= test_X, label=fake_label, transform=train_transform,test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=3)\n",
    "\n",
    "\n",
    "for i,pth in enumerate([os.path.join('resnet34_save_model',i) for i in os.listdir(path='resnet34_save_model') if '.pth' in i]):\n",
    "    net = resnet.resnet34(num_classes=2)\n",
    "    net.load_state_dict(torch.load(pth))\n",
    "    net.cuda()\n",
    "    prob = [] \n",
    "    net.eval()\n",
    "    for k, (val_x, val_y) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "        x = Variable(val_x)\n",
    "        y = Variable(val_y)\n",
    "        out = net(x)\n",
    "        #prevent overflow\n",
    "        temp = np.exp(out.cpu().data.numpy()-np.max(out.cpu().data.numpy(),axis=1)[:,np.newaxis])\n",
    "        ans= temp[:,1]/(temp.sum(axis=1))\n",
    "        prob.append(ans)\n",
    "        #print(out.size())\n",
    "        progress_bar(k, len(test_loader))\n",
    "    msg = 'is_iceberg%d' % i\n",
    "    temp11[msg]= np.concatenate(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['is_iceberg'] = temp11['is_iceberg']\n",
    "sub.shape\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg0</th>\n",
       "      <th>is_iceberg1</th>\n",
       "      <th>is_iceberg2</th>\n",
       "      <th>is_iceberg3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.027504e-03</td>\n",
       "      <td>9.244031e-02</td>\n",
       "      <td>1.784263e-02</td>\n",
       "      <td>5.578169e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.931345e-03</td>\n",
       "      <td>3.659658e-01</td>\n",
       "      <td>2.564293e-01</td>\n",
       "      <td>1.571568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.239599e-10</td>\n",
       "      <td>1.970750e-21</td>\n",
       "      <td>3.803356e-08</td>\n",
       "      <td>2.089403e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.993261e-01</td>\n",
       "      <td>9.456407e-01</td>\n",
       "      <td>9.853242e-01</td>\n",
       "      <td>9.989353e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448082e-03</td>\n",
       "      <td>6.435396e-02</td>\n",
       "      <td>3.096765e-02</td>\n",
       "      <td>2.362306e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_iceberg0   is_iceberg1   is_iceberg2   is_iceberg3\n",
       "0  7.027504e-03  9.244031e-02  1.784263e-02  5.578169e-03\n",
       "1  3.931345e-03  3.659658e-01  2.564293e-01  1.571568e-02\n",
       "2  5.239599e-10  1.970750e-21  3.803356e-08  2.089403e-21\n",
       "3  9.993261e-01  9.456407e-01  9.853242e-01  9.989353e-01\n",
       "4  1.448082e-03  6.435396e-02  3.096765e-02  2.362306e-04"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = temp11.mean(1)\n",
    "temp11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp11['is_iceberg_max'] = temp11.iloc[:, :4].max(axis=1)\n",
    "temp11['is_iceberg_min'] = temp11.iloc[:, :4].min(axis=1)\n",
    "temp11['is_iceberg_median'] = temp11.iloc[:, :4].median(axis=1)\n",
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2\n",
    "\n",
    "temp11['is_iceberg_base'] = temp11['is_iceberg3']\n",
    "temp11['is_iceberg'] = np.where(np.all(temp11.iloc[:,0:6] > cutoff_lo, axis=1), \n",
    "                                    temp11['is_iceberg_max'], \n",
    "                                    np.where(np.all(temp11.iloc[:,0:6] < cutoff_hi, axis=1),\n",
    "                                             temp11['is_iceberg_min'], \n",
    "                                             temp11['is_iceberg_base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cp vgg_fcn.ipynb vgg_angle.ipynb\n",
    "temp11.to_csv('others/vgg16_10fold_50.csv',index=False)\n",
    "# temp11.to_csv('training_set_result/vgg16_10fold_noaug_50.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
